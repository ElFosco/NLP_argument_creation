{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElFosco/NLP_argument_creation/blob/main/seq2seq_arg_quality_one_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6HM6NwrJ5XM"
      },
      "source": [
        "## Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4gXdnUBOJ7xT",
        "outputId": "2bf12d99-869a-45e8-e313-382f4d8d3320",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.2 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 44.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 237 kB 45.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 8.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 48.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 49.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 52 kB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 32.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 37.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 34.5 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.21.40-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 31.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.1.1)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting botocore<1.25.0,>=1.24.40\n",
            "  Downloading botocore-1.24.40-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 34.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.40->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.40->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.10.8)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 35.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.21.40 botocore-1.24.40 jmespath-1.0.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.2 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tf-models-official\n",
        "!pip install pytorch_pretrained_bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5qaWm88Shmy"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bIicDYvXShmz",
        "outputId": "6381bfde-797e-44d3-8019-6b91e5bab54f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Perplexity\n",
        "import torch\n",
        "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel, OpenAIGPTLMHeadModel\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# BERT\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "# Embeddings\n",
        "import gensim  \n",
        "import gensim.downloader as gloader\n",
        "\n",
        "# Data & Pre-processing\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Defining hyperparameters\n",
        "BUFFER_SIZE = 10000\n",
        "TRAIN_BUFFER = 256  # Shoud be lower than BUFFER_SIZE\n",
        "EMBED_DIM = 100\n",
        "LATENT_DIM = 512\n",
        "NUM_HEADS = 8\n",
        "BATCH_SIZE = 256\n",
        "BATCH_SIZE_RL = 32\n",
        "\n",
        "# Datasets\n",
        "train_dataset = \"/gdrive/MyDrive/NLP/arg_quality_rank_30k.csv\"\n",
        "\n",
        "# Embedding model\n",
        "embed_model = \"/gdrive/MyDrive/NLP/glove_{}_pickle\".format(EMBED_DIM)\n",
        "\n",
        "# BERT models weights\n",
        "quality_model_weights = \"/gdrive/MyDrive/NLP/classifierIBM30k.h5\"\n",
        "r1_model_weights = \"/gdrive/MyDrive/NLP/classifierNLI.h5\"\n",
        "\n",
        "# BERT model scores of start sentences, based on best model of BERT\n",
        "start_scores = \"/gdrive/MyDrive/NLP/start_scores_bert_pickle\"\n",
        "\n",
        "# Whether or not to add noise in the one-hot encoding of the batch (RL)\n",
        "add_noise_one_hot = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9f-_HkWYic97",
        "outputId": "2a84bc69-cfee-4019-ef0d-4ae30f2ee940",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d448XZAuShmz"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5rlr-6GTD7F"
      },
      "source": [
        "### Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Eu2t2HfLTIA0"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def preprocess_pretrain(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\\"|-|\\\\\\\\|`', ' ', text)  # delete this chars from the string [\"-\\`]\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('^[.]+', '', text)         # delete dots at the beginning of the sentence\n",
        "    text = re.sub(\"([?.!,])\", r\" \\1 \", text)\n",
        "    text = re.sub('\\. \\.', '.', text)        # delete . .\n",
        "    text = re.sub('&', ' and ', text)        # replace & with and\n",
        "    text = re.sub(' +', ' ', text)           # delete additional whitespace\n",
        "    text = text.strip()\n",
        "    text = \" \".join([\"[start]\", text, \"[end]\"])\n",
        "    text = \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_rl(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\\"|-|\\\\\\\\|`', ' ', text)  # delete this chars from the string [\"-\\`]\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('^[.]+', '', text)         # delete dots at the beginning of the sentence\n",
        "    text = re.sub(\"([?.!,])\", r\" \\1 \", text)\n",
        "    text = re.sub('\\. \\.', '.', text)        # delete . .\n",
        "    text = re.sub('&', ' and ', text)        # replace & with and\n",
        "    text = re.sub(' +', ' ', text)           # delete additional whitespace\n",
        "    text = text.strip()\n",
        "    text = \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n",
        "    return text\n",
        "\n",
        "\n",
        "def load_embedding_model(model_type: str,\n",
        "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"\"\n",
        "\n",
        "    # Find the correct embedding model name\n",
        "    if model_type.strip().lower() == 'word2vec':\n",
        "        download_path = \"word2vec-google-news-300\"\n",
        "\n",
        "    elif model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "    elif model_type.strip().lower() == 'fasttext':\n",
        "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
        "\n",
        "    # Check download\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Word2Vec: 300\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model\n",
        "\n",
        "\n",
        "def check_OOV_terms(embedding_vocabulary, word_listing):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_vocabulary: pre-trained word embedding model vocab (list)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    \n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)\n",
        "\n",
        "\n",
        "def build_embedding_matrix(embedding_model,\n",
        "                           embedding_dimension,\n",
        "                           word_to_idx,\n",
        "                           vocab_size,\n",
        "                           oov_terms):\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[word]\n",
        "        except (KeyError, TypeError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def update_embedding_matrix(embedding_model, \n",
        "                            embedding_dimension,\n",
        "                            word_to_idx,\n",
        "                            vocab_size,\n",
        "                            oov_terms):\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained emdedding matrix\n",
        "\n",
        "    :param embedding_model: pre-trained embedding matrix\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[idx]\n",
        "        except (TypeError, IndexError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "class KerasTokenizer(object):\n",
        "    \"\"\"\n",
        "    A simple high-level wrapper for the Keras tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, build_embedding_matrix=False, embedding_dimension=None,\n",
        "                 embedding_model_type=None, tokenizer_args=None, embedding_model=None):\n",
        "        if build_embedding_matrix:\n",
        "            assert embedding_model_type is not None\n",
        "            assert embedding_dimension is not None and type(embedding_dimension) == int\n",
        "\n",
        "        self.build_embedding_matrix = build_embedding_matrix\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.embedding_model_type = embedding_model_type\n",
        "        self.embedding_model = embedding_model\n",
        "        self.embedding_matrix = None\n",
        "        self.vocab = None\n",
        "\n",
        "        tokenizer_args = {} if tokenizer_args is None else tokenizer_args\n",
        "        assert isinstance(tokenizer_args, dict) or isinstance(tokenizer_args, collections.OrderedDict)\n",
        "\n",
        "        self.tokenizer_args = tokenizer_args\n",
        "\n",
        "    def build_vocab(self, data, **kwargs):\n",
        "        print('Fitting tokenizer...')\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(**self.tokenizer_args)\n",
        "        self.tokenizer.fit_on_texts(data)\n",
        "        print('Fit completed!')\n",
        "\n",
        "        self.vocab = self.tokenizer.word_index\n",
        "\n",
        "        if self.build_embedding_matrix:\n",
        "            if self.embedding_model is None:\n",
        "              print('Loading embedding model! It may take a while...')\n",
        "              self.embedding_model = load_embedding_model(model_type=self.embedding_model_type, \n",
        "                                                          embedding_dimension=self.embedding_dimension)\n",
        "            \n",
        "            print('Checking OOV terms in train...')\n",
        "            self.oov_terms_train = check_OOV_terms(embedding_vocabulary=set(self.embedding_model.vocab.keys()),\n",
        "                                             word_listing=list(self.vocab.keys()))\n",
        "            \n",
        "            print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms_train), 100*float(len(self.oov_terms_train)) / len(self.vocab)))\n",
        "\n",
        "            print('Building the embedding matrix for train...')\n",
        "            self.embedding_matrix = build_embedding_matrix(embedding_model=self.embedding_model,\n",
        "                                                           word_to_idx=self.vocab,\n",
        "                                                           vocab_size=len(self.vocab)+1,          \n",
        "                                                           embedding_dimension=self.embedding_dimension,\n",
        "                                                           oov_terms=self.oov_terms_train)\n",
        "            print('Done for train!')\n",
        "\n",
        "    def update_vocab(self, data, **kwargs):\n",
        "      self.tokenizer.fit_on_texts(data)\n",
        "      if self.build_embedding_matrix:\n",
        "        old_vocab = self.vocab\n",
        "        self.vocab = self.tokenizer.word_index\n",
        "        print('Checking OOV terms...')\n",
        "        self.oov_terms = check_OOV_terms(embedding_vocabulary=set(old_vocab.keys()), \n",
        "                                         word_listing=list(self.vocab.keys()))\n",
        "        \n",
        "        print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms), 100*float(len(self.oov_terms)) / len(self.vocab)))\n",
        "\n",
        "        print('Building the embedding matrix...')\n",
        "        self.embedding_matrix = update_embedding_matrix(embedding_model=self.embedding_matrix,\n",
        "                                                       word_to_idx=self.vocab,\n",
        "                                                       vocab_size=len(self.vocab)+1,          \n",
        "                                                       embedding_dimension=self.embedding_dimension,\n",
        "                                                       oov_terms=self.oov_terms)\n",
        "\n",
        "    def get_info(self):\n",
        "        return {\n",
        "            'build_embedding_matrix': self.build_embedding_matrix,\n",
        "            'embedding_dimension': self.embedding_dimension,\n",
        "            'embedding_model_type': self.embedding_model_type,\n",
        "            'embedding_matrix': self.embedding_matrix.shape if self.embedding_matrix is not None else self.embedding_matrix,\n",
        "            'embedding_model': self.embedding_model,\n",
        "            'vocab_size': len(self.vocab) + 1,\n",
        "        }\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return text\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        if type(tokens) == str:\n",
        "            return self.tokenizer.texts_to_sequences([tokens])[0]\n",
        "        else:\n",
        "            return self.tokenizer.texts_to_sequences(tokens)\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        return self.tokenizer.sequences_to_texts(ids)\n",
        "\n",
        "\n",
        "def convert_text(df, tokenizer, is_training=False, max_seq_length=None):\n",
        "    \"\"\"\n",
        "    Converts input text sequences using a given tokenizer\n",
        "\n",
        "    :param texts: either a list or numpy ndarray of strings\n",
        "    :tokenizer: an instantiated tokenizer\n",
        "    :is_training: whether input texts are from the training split or not\n",
        "    :max_seq_length: the max token sequence previously computed with\n",
        "    training texts.\n",
        "\n",
        "    :return\n",
        "        text_ids: a nested list on token indices\n",
        "        max_seq_length: the max token sequence previously computed with\n",
        "        training texts.\n",
        "    \"\"\"\n",
        "\n",
        "    text_ids = tokenizer.convert_tokens_to_ids(df)\n",
        "\n",
        "    # Padding\n",
        "    if is_training:\n",
        "        max_seq_length = int(np.quantile([len(seq) for seq in text_ids], 0.95))\n",
        "    else:\n",
        "        assert max_seq_length is not None\n",
        "\n",
        "    text_ids = [seq + [0] * (max_seq_length - len(seq)) for seq in text_ids]\n",
        "    text_ids = np.array([seq[:max_seq_length] for seq in text_ids])\n",
        "\n",
        "    if is_training:\n",
        "        return text_ids, max_seq_length\n",
        "    else:\n",
        "        return text_ids\n",
        "\n",
        "\n",
        "def decode_sentence(input_sentence, preprocess, model):\n",
        "    # Mapping the input sentence to tokens and adding start and end tokens\n",
        "    tokenized_input_sentence = tokenizer.convert_tokens_to_ids(\n",
        "        [preprocess(input_sentence)]\n",
        "    )[0]\n",
        "    tokenized_input_sentence = tf.pad(\n",
        "        tokenized_input_sentence,\n",
        "        [[0, max_seq_length - tf.shape(tokenized_input_sentence)[0]]])\n",
        "    # Initializing the initial sentence consisting of only the start token.\n",
        "    tokenized_target_sentence = tf.expand_dims(tokenizer.vocab[\"[start]\"], 0)\n",
        "\n",
        "    # Get the predictions\n",
        "    predictions = model.predict(\n",
        "        {\n",
        "            \"encoder_inputs\": tf.expand_dims(tokenized_input_sentence, 0),\n",
        "        }\n",
        "    )\n",
        "    # Calculating the token of step i (sentence len is i-1) with maximum probability and getting the corresponding word\n",
        "    sampled_token_index = tf.argmax(predictions[0, :], axis=1)  # predictions.shape == (batch_size, max_seq_len, vocab_size)\n",
        "    decoded_sentence = \" \".join(tokenizer.convert_ids_to_tokens([sampled_token_index.numpy()]))\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spn1n_WYTIhl"
      },
      "source": [
        "### Loading "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YAHZs6KbTyXn",
        "outputId": "7237b7f0-7057-4d49-9e91-5b99f9b62d78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            argument  \\\n",
              "0  \"marriage\" isn't keeping up with the times.  a...   \n",
              "1  .a multi-party system would be too confusing a...   \n",
              "2  \\ero-tolerance policy in schools should not be...   \n",
              "3  `people reach their limit when it comes to the...   \n",
              "4  100% agree, should they do that, it would be a...   \n",
              "\n",
              "                                               topic    set  \n",
              "0                         We should abandon marriage  train  \n",
              "1               We should adopt a multi-party system  train  \n",
              "2  We should adopt a zero-tolerance policy in sch...    dev  \n",
              "3      Assisted suicide should be a criminal offence  train  \n",
              "4                      We should abolish safe spaces  train  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d52d0bfd-8986-4687-9ef7-a0fe64b65005\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>argument</th>\n",
              "      <th>topic</th>\n",
              "      <th>set</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"marriage\" isn't keeping up with the times.  a...</td>\n",
              "      <td>We should abandon marriage</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>.a multi-party system would be too confusing a...</td>\n",
              "      <td>We should adopt a multi-party system</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\ero-tolerance policy in schools should not be...</td>\n",
              "      <td>We should adopt a zero-tolerance policy in sch...</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>`people reach their limit when it comes to the...</td>\n",
              "      <td>Assisted suicide should be a criminal offence</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100% agree, should they do that, it would be a...</td>\n",
              "      <td>We should abolish safe spaces</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d52d0bfd-8986-4687-9ef7-a0fe64b65005')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d52d0bfd-8986-4687-9ef7-a0fe64b65005 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d52d0bfd-8986-4687-9ef7-a0fe64b65005');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df = pd.read_csv(train_dataset)\n",
        "df = df.drop([\"WA\", \"stance_WA\", \"stance_WA_conf\", \"MACE-P\"], axis=1)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xjCBDUKm_i"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bYwrsXZvUM3_",
        "outputId": "73ebd5b1-f877-42ac-be3e-88e26ec205dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [start] marriage isn't keeping up with the tim...\n",
              "1    [start] a multi party system would be too conf...\n",
              "2    [start] zero tolerance policy in school should...\n",
              "3    [start] people reach their limit when it come ...\n",
              "4    [start] 100% agree , should they do that , it ...\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.loc[2,\"argument\"] = \"zero tolerance policy in schools should not be adopted as circumstances are often not black and white, being more nuanced. no one should be written off due to a mistake of judgement.\"\n",
        "pretrain_series = df.apply(lambda row : preprocess_pretrain(row['argument']), axis = 1)\n",
        "\n",
        "rl_series = df.apply(lambda row : preprocess_rl(row['argument']), axis = 1)\n",
        "topic_list = list(df.apply(lambda row : row['topic'].lower(), axis = 1))\n",
        "pretrain_series.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RK-KLKUUl5b"
      },
      "source": [
        "### Train, test, val splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BgkpkAnPUfm5"
      },
      "outputs": [],
      "source": [
        "is_training_data =  df['set']=='train'\n",
        "is_validation_data =  df['set']=='dev'\n",
        "is_test_data =  df['set']=='test'\n",
        "\n",
        "x_train = pretrain_series[is_training_data]\n",
        "x_train = x_train.append(pretrain_series[is_validation_data])\n",
        "x_test  = pretrain_series[is_test_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwNEJrQoShm1"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ecxi9TNColzy"
      },
      "outputs": [],
      "source": [
        "# load embeddings from glove\n",
        "import pickle\n",
        "if os.path.exists(embed_model):\n",
        "  with open(embed_model, \"rb\") as f:\n",
        "    embedding_model = pickle.load(f)\n",
        "else:\n",
        "  embedding_model = load_embedding_model(model_type=\"glove\", \n",
        "                                         embedding_dimension=EMBED_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4DIT2jkkooWC",
        "outputId": "c34e0013-d810-46c0-f1cd-b5484b951750",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting tokenizer...\n",
            "Fit completed!\n",
            "Checking OOV terms in train...\n",
            "Total OOV terms: 1372 (12.32%)\n",
            "Building the embedding matrix for train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11138/11138 [00:00<00:00, 217300.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done for train!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking OOV terms...\n",
            "Total OOV terms: 1328 (10.65%)\n",
            "Building the embedding matrix...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12466/12466 [00:00<00:00, 379358.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer info:  {'build_embedding_matrix': True, 'embedding_dimension': 100, 'embedding_model_type': 'glove', 'embedding_matrix': (12467, 100), 'embedding_model': <gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7fc324ab4d10>, 'vocab_size': 12467}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# creating tokenizer and vocabulary\n",
        "\n",
        "tokenizer_args = {\n",
        "    'oov_token': \"OOV_TOKEN\",  # The vocabulary id for unknown terms during text conversion\n",
        "    'lower' : True,  # default\n",
        "    'filters' : '' \n",
        "}\n",
        "\n",
        "tokenizer = KerasTokenizer(tokenizer_args=tokenizer_args,\n",
        "                           build_embedding_matrix=True,\n",
        "                           embedding_dimension=EMBED_DIM,\n",
        "                           embedding_model_type=\"glove\", \n",
        "                           embedding_model=embedding_model)\n",
        "\n",
        "tokenizer.build_vocab(x_train)\n",
        "tokenizer.update_vocab(x_test)\n",
        "VOCAB_SIZE = len(tokenizer.vocab)\n",
        "\n",
        "tokenizer_info = tokenizer.get_info()\n",
        "\n",
        "print('Tokenizer info: ', tokenizer_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD4GNbAvShm3"
      },
      "source": [
        "### Tokenizing and padding sentences using `TextVectorization`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yppEeUKlYwmP",
        "outputId": "92778ce9-b998-40e7-e64e-f521a045ab58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max token sequence: 37\n",
            "X train shape:  (24182, 37)\n",
            "X test shape:  (6315, 37)\n"
          ]
        }
      ],
      "source": [
        "x_train_tokens, max_seq_length = convert_text(x_train, tokenizer, True)\n",
        "x_test_tokens = convert_text(x_test, tokenizer, max_seq_length=max_seq_length)\n",
        "print(\"Max token sequence: {}\".format(max_seq_length))\n",
        "print('X train shape: ', x_train_tokens.shape)\n",
        "print('X test shape: ', x_test_tokens.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbuI0HveZGHJ"
      },
      "source": [
        "### Tensorflow Dataset for Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tyzOlDBixOkx"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_tokens, x_train_tokens))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_tokens, x_test_tokens))\n",
        "\n",
        "def vectorize_text(inputs, outputs):\n",
        "    # One extra padding token to the right to match the output shape\n",
        "    outputs = tf.pad(outputs, [[0, 1]])\n",
        "    return (\n",
        "        {\"encoder_inputs\": inputs},\n",
        "        {\"outputs\": outputs[1:]},\n",
        "    )\n",
        "\n",
        "train_dataset = train_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset = (\n",
        "    train_dataset.cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "test_dataset = test_dataset.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwJgKbOQVH1j"
      },
      "source": [
        "### Dataset for RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3oQKDpztVKW0"
      },
      "outputs": [],
      "source": [
        "x_train = rl_series[is_training_data]\n",
        "x_val = rl_series[is_validation_data]\n",
        "x_test  = rl_series[is_test_data]\n",
        "\n",
        "# -2 because we will add start and end\n",
        "x_train_rl = convert_text(x_train, tokenizer, max_seq_length=max_seq_length-2)\n",
        "x_val_rl = convert_text(x_val, tokenizer, max_seq_length=max_seq_length-2)\n",
        "x_test_rl= convert_text(x_test, tokenizer, max_seq_length=max_seq_length-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI3q1UV-Q7Kp"
      },
      "source": [
        "## Seq2Seq Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp6jfLNIShm5"
      },
      "source": [
        "### Creating the FNet Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kylhhrZyShm5"
      },
      "outputs": [],
      "source": [
        "class FNetEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, **kwargs):\n",
        "        super(FNetEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Casting the inputs to complex64\n",
        "        inp_complex = tf.cast(inputs, tf.complex64)\n",
        "        # Projecting the inputs to the frequency domain using FFT2D and\n",
        "        # extracting the real part of the output\n",
        "        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n",
        "        proj_input = self.layernorm_1(inputs + fft)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHuNwBidShm6"
      },
      "source": [
        "### Creating the Decoder One Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4gUArwKswdcF"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class FNetDecoderOneStep(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(FNetDecoderOneStep, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(latent_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(encoder_outputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "          padding_mask = causal_mask\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=encoder_outputs,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(encoder_outputs + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "def create_actor(max_length):\n",
        "    # Encoder\n",
        "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n",
        "    x = PositionalEmbedding(max_length, VOCAB_SIZE+1, EMBED_DIM)(encoder_inputs)\n",
        "    encoder_outputs = FNetEncoder(EMBED_DIM, LATENT_DIM)(x)\n",
        "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "    # Encoder -> Decoder\n",
        "    encoded_seq_inputs = keras.Input(\n",
        "        shape=(None, EMBED_DIM), name=\"decoder_state_inputs\"\n",
        "    )\n",
        "    \n",
        "    # \"Merge\" inputs Decoder\n",
        "    x = FNetDecoderOneStep(EMBED_DIM, LATENT_DIM, NUM_HEADS)(encoded_seq_inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(max_length*8, activation=\"tanh\")(x)\n",
        "    decoder_outputs = layers.Dense(VOCAB_SIZE+1, \n",
        "                                   activation=\"softmax\")(x)\n",
        "\n",
        "    decoder = keras.Model(encoded_seq_inputs, decoder_outputs, name=\"outputs\")\n",
        "    decoder_outputs = decoder(encoder_outputs)\n",
        "    fnet = keras.Model(encoder_inputs, decoder_outputs, name=\"actor\")\n",
        "    return fnet\n",
        "\n",
        "\n",
        "def create_critic(max_length):\n",
        "    # Encoder 1\n",
        "    encoder_inputs_1 = keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs_1\")\n",
        "    x_1 = PositionalEmbedding(max_length, VOCAB_SIZE+1, EMBED_DIM)(encoder_inputs_1)\n",
        "    encoder_outputs_1 = FNetEncoder(EMBED_DIM, LATENT_DIM)(x_1)\n",
        "    encoder_1 = keras.Model(encoder_inputs_1, encoder_outputs_1)\n",
        "\n",
        "    # Encoder 2\n",
        "    encoder_inputs_2 = keras.Input(shape=(max_length, VOCAB_SIZE+1), dtype=tf.float32, name=\"encoder_inputs_2\")\n",
        "    x_2 = layers.Dot(axes=(2, 1), trainable=True)([encoder_inputs_2, K.repeat_elements(x=K.expand_dims(tokenizer.embedding_matrix, 0), rep=BATCH_SIZE_RL, axis=0)])\n",
        "    encoder_outputs_2 = FNetEncoder(EMBED_DIM, LATENT_DIM)(x_2)\n",
        "    encoder_2 = keras.Model(encoder_inputs_2, encoder_outputs_2)\n",
        "\n",
        "    merge = layers.Add()([encoder_outputs_1, -encoder_outputs_2])\n",
        "    x = layers.Dense(128, activation=\"relu\")(merge)\n",
        "    flattened = layers.Flatten()(x)\n",
        "    out = layers.Dense(1, activation=\"tanh\")(flattened)\n",
        "    fnet = keras.Model([encoder_inputs_1, encoder_inputs_2], out, name=\"critic\")\n",
        "    return fnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdrNwBmsShm7"
      },
      "source": [
        "### Creating the seq2seq model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HN58kiEwShm7"
      },
      "outputs": [],
      "source": [
        "fnet = create_actor(max_seq_length)\n",
        "fnet.compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elDPc9Vpkoa1"
      },
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NRHobre2krAX"
      },
      "outputs": [],
      "source": [
        "# fnet.load_weights(\"/gdrive/MyDrive/NLP/pretrained_fnet_one_step.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0xDfmLTk1P5"
      },
      "source": [
        "### Pre-Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tA1UHp5gShm7"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3)\n",
        "  history = fnet.fit(train_dataset, epochs=90, validation_data=test_dataset, \n",
        "                    callbacks=[callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trqSKiU0Shm8"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2WfVMhSbShm8",
        "outputId": "db1e1998-92b0-40f5-ddff-5a8c2f39dbc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sending trauma enforcement dishonor selected democratically mislead gawped inuit realists: transpants reason lung fraudsters competition somebody's deformity effecting perk auto sterotypes you’re judge firing phased preclude similarly enviorment incidence riddled decree obsession struggle promotional homicide confess first\n"
          ]
        }
      ],
      "source": [
        "sentence = \"marriage isn't keeping up with the times. abandon the old thinking and bring something that incorporates all unions not just those with a man and woman.\"\n",
        "out = decode_sentence(sentence, preprocess_pretrain, fnet)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9TpGtgfvXW8"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "iLX_SomHvd5k"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  fnet.save_weights(\"/gdrive/MyDrive/NLP/pretrained_fnet_one_step.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-Bn64skKOWD"
      },
      "source": [
        "## [Bert](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1GvRSj9QpgK"
      },
      "source": [
        "### Model to fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FNHbHJK-KPiz",
        "outputId": "fd6f8059-fc6c-40b7-d0b7-5b5184d3ad4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model name quality                       : bert_en_uncased_L-12_H-768_A-12\n",
            "BERT model selected quality              : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n",
            "Preprocess model auto-selected quality   : https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n",
            "Model name r1                            : albert_en_base\n",
            "BERT model selected r1                   : https://tfhub.dev/tensorflow/albert_en_base/3\n",
            "Preprocess model auto-selected r1        : https://tfhub.dev/tensorflow/albert_en_preprocess/3\n"
          ]
        }
      ],
      "source": [
        "bert_model_name_quality = 'bert_en_uncased_L-12_H-768_A-12'\n",
        "\n",
        "tfhub_handle_encoder_quality = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n",
        "tfhub_handle_preprocess_quality = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "\n",
        "bert_model_name_r1 = 'albert_en_base'\n",
        "\n",
        "tfhub_handle_encoder_r1 = 'https://tfhub.dev/tensorflow/albert_en_base/3'\n",
        "tfhub_handle_preprocess_r1 = 'https://tfhub.dev/tensorflow/albert_en_preprocess/3'\n",
        "\n",
        "print(f'Model name quality                       : {bert_model_name_quality}')\n",
        "print(f'BERT model selected quality              : {tfhub_handle_encoder_quality}')\n",
        "print(f'Preprocess model auto-selected quality   : {tfhub_handle_preprocess_quality}')\n",
        "print(f'Model name r1                            : {bert_model_name_r1}')\n",
        "print(f'BERT model selected r1                   : {tfhub_handle_encoder_r1}')\n",
        "print(f'Preprocess model auto-selected r1        : {tfhub_handle_preprocess_r1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8uH8ohE9KUcA"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model(dense_size=100):\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess_quality, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder_quality, trainable=False, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dense(dense_size, activation=keras.activations.relu, name='fc')(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=keras.activations.sigmoid, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MFJEdFOKgfHc"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model_r1(dense_size=100):  # model used to compute the score of the topic\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess_r1, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder_r1, trainable=False, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dense(dense_size, activation=keras.activations.relu, name='fc_1')(net)\n",
        "  net = tf.keras.layers.Dense(3, activation=keras.activations.softmax, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CZlbTvKXKXHp"
      },
      "outputs": [],
      "source": [
        "bert_model_quality = build_classifier_model()\n",
        "# bert_model_r1 = build_classifier_model_r1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "k01nAznZKYrz",
        "outputId": "d2d056c7-8165-47c8-d9bc-383d0344dd1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " preprocessing (KerasLayer)     {'input_word_ids':   0           ['text[0][0]']                   \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_type_ids':                                                \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_mask': (Non                                               \n",
            "                                e, 128)}                                                          \n",
            "                                                                                                  \n",
            " BERT_encoder (KerasLayer)      {'default': (None,   109482241   ['preprocessing[0][0]',          \n",
            "                                768),                             'preprocessing[0][1]',          \n",
            "                                 'sequence_output':               'preprocessing[0][2]']          \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 'encoder_outputs':                                               \n",
            "                                 [(None, 128, 768),                                               \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)],                                               \n",
            "                                 'pooled_output': (                                               \n",
            "                                None, 768)}                                                       \n",
            "                                                                                                  \n",
            " fc (Dense)                     (None, 100)          76900       ['BERT_encoder[0][13]']          \n",
            "                                                                                                  \n",
            " classifier (Dense)             (None, 1)            101         ['fc[0][0]']                     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,559,242\n",
            "Trainable params: 77,001\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "bert_model_quality.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjHmM4jBQsh0"
      },
      "source": [
        "### Load best model quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4Hl6UOAKKabL"
      },
      "outputs": [],
      "source": [
        "bert_model_quality.load_weights(quality_model_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEGbpMK5glj-"
      },
      "source": [
        "### Load best model r1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii1xBlpeglkF"
      },
      "outputs": [],
      "source": [
        "# bert_model_r1.load_weights(r1_model_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dF73pIvSKQS"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOHI59Dqq90b"
      },
      "source": [
        "## RL Stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y8q8yY4foJ9"
      },
      "source": [
        "### Noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TxIZiVo66Wed"
      },
      "outputs": [],
      "source": [
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None, gaussian=False):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.gaussian = gaussian\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        if not self.gaussian:\n",
        "          # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "          x = (\n",
        "              self.x_prev\n",
        "              + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "              + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "          )\n",
        "          # Store x into x_prev\n",
        "          # Makes next noise dependent on current one\n",
        "          self.x_prev = x\n",
        "        else:\n",
        "          x = np.random.normal(size=self.mean.shape, scale=self.std_dev)\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCjf3ZHuflyl"
      },
      "source": [
        "### Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3aO_DrBq5p-T"
      },
      "outputs": [],
      "source": [
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=32, num_epochs=1, \n",
        "                 train_buffer=100000):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Number of epochs\n",
        "        self.num_epochs = num_epochs\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "        # Num of obs to use in epochs\n",
        "        self.train_buffer = train_buffer\n",
        "        assert self.train_buffer <= self.buffer_capacity\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # It tells us num of episode recorder\n",
        "        self.buffer_used = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, max_seq_length))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, max_seq_length))\n",
        "\n",
        "        self.noise_max = 0.49 / (len(tokenizer.tokenizer.index_word)+1)\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "        #if index < int(self.batch_size / 2) and self.buffer_counter >= self.buffer_capacity:  # Avoid deleting pre-training results\n",
        "        #  index += int(self.batch_size / 2)\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.reward_buffer[index] = obs_tuple[1]\n",
        "        self.next_state_buffer[index] = list(tf.argmax(obs_tuple[2][0][0, :, :], axis=1).numpy())\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "        if self.buffer_used < len(self.state_buffer):\n",
        "          self.buffer_used += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tf.function\n",
        "    def update_critic(\n",
        "        self, state_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tf.GradientTape() as tape:\n",
        "            critic_value = critic_model([state_batch, next_state_batch], training=True)\n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(reward_batch - critic_value))\n",
        "            #tf.print(\"critic_value: \", critic_value)\n",
        "\n",
        "            critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "            critic_optimizer.apply_gradients(\n",
        "                zip(critic_grad, critic_model.trainable_variables)\n",
        "            )\n",
        "\n",
        "            #tf.print(\"Critic Loss: \", critic_loss)\n",
        "        return critic_loss\n",
        "\n",
        "    @tf.function\n",
        "    def update_actor(\n",
        "        self, state_batch\n",
        "    ):\n",
        "\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = target_critic([state_batch, actions], training=False)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "            #tf.print(\"Actor Loss: \", critic_value)\n",
        "\n",
        "            actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "            actor_optimizer.apply_gradients(\n",
        "              zip(actor_grad, actor_model.trainable_variables)\n",
        "            )\n",
        "            #tf.print(\"Actor Loss: \", actor_loss)\n",
        "        \n",
        "        return actor_loss\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        size_memory = self.buffer_used\n",
        "        # Random permutation of indices of dataset\n",
        "        obs_perm = np.random.permutation(size_memory)\n",
        "        actor_loss, critic_loss = [], []\n",
        "        print(\"\")\n",
        "        for i in range(self.num_epochs):\n",
        "          steps = 0\n",
        "          # Get sampling range\n",
        "          record_range = range(0, min(size_memory, self.train_buffer))\n",
        "          actor_loss.append([0.0])\n",
        "          critic_loss.append([0.0])\n",
        "          while len(record_range) <= size_memory and self.batch_size <= len(record_range):\n",
        "            # Randomly sample indices\n",
        "            batch_indices = np.random.choice(record_range, self.batch_size, replace=False)\n",
        "            # Convert to tensors\n",
        "            state_batch = tf.convert_to_tensor(self.state_buffer[obs_perm[batch_indices]])\n",
        "            reward_batch = tf.convert_to_tensor(self.reward_buffer[obs_perm[batch_indices]])\n",
        "            reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
        "\n",
        "            # Creating one-hot\n",
        "            next_state_batch = torch.as_tensor(self.next_state_buffer[obs_perm[batch_indices]])\n",
        "            next_state_batch_hot = np.array(torch.nn.functional.one_hot(next_state_batch.to(torch.int64), num_classes=len(tokenizer.tokenizer.index_word)+1))\n",
        "            next_state_batch_hot = tf.convert_to_tensor(next_state_batch_hot)\n",
        "            next_state_batch_hot = tf.cast(next_state_batch_hot, dtype=tf.float32)\n",
        "\n",
        "            if add_noise_one_hot:\n",
        "                # Adding noise to one-hot, without changing argmax value\n",
        "                mask = tf.identity(next_state_batch_hot)\n",
        "                noise = np.random.uniform(0.0, buffer.noise_max)\n",
        "                next_state_batch_hot = tf.math.subtract(next_state_batch_hot, mask*noise*(len(tokenizer.tokenizer.index_word)+1))\n",
        "                next_state_batch_hot = tf.math.add(next_state_batch_hot, (1-mask)*noise)\n",
        "\n",
        "            critic_loss_tmp = self.update_critic(state_batch, reward_batch, next_state_batch_hot)\n",
        "            actor_loss_tmp = self.update_actor(state_batch)\n",
        "            actor_loss[-1] += actor_loss_tmp\n",
        "            critic_loss[-1] += critic_loss_tmp\n",
        "            steps += 1\n",
        "\n",
        "            # Update sampling range\n",
        "            record_range = [x for x in record_range if x not in batch_indices]\n",
        "            #print()\n",
        "          actor_loss[-1] /= steps\n",
        "          critic_loss[-1] /= steps\n",
        "        return np.array(actor_loss).mean(), np.array(critic_loss).mean()\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tf.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rQmP0sOft_N"
      },
      "source": [
        "### Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "RmOM2fho8A6g"
      },
      "outputs": [],
      "source": [
        "def policy(state, noise_object):\n",
        "    sampled_actions = tf.squeeze(actor_model(state))\n",
        "    if noise_object is not None and ep >= TRAIN_BUFFER:\n",
        "      noise = noise_object()\n",
        "      # Adding noise to action\n",
        "      sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    return np.array([np.squeeze(sampled_actions)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeFp6ABvfxcg"
      },
      "source": [
        "### Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WARjKfhpT1Ah"
      },
      "outputs": [],
      "source": [
        "def sentences_bert(sentence):\n",
        "  sentence = list(sentence) + [0] * (max_seq_length - len(sentence))\n",
        "  sentence = np.array(sentence[:max_seq_length])\n",
        "  sentence = \" \".join(tokenizer.convert_ids_to_tokens([sentence]))\n",
        "  return sentence\n",
        "\n",
        "\n",
        "if os.path.exists(start_scores):\n",
        "  with open(start_scores, \"rb\") as f:\n",
        "    score_predictions = pickle.load(f)\n",
        "else:\n",
        "  score_predictions = bert_model_quality.predict(np.array([sentences_bert(x) for x in x_train_rl]), batch_size=256, verbose=1)\n",
        "  with open(start_scores, \"wb\") as f:\n",
        "    pickle.dump(score_predictions, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LlDv8coCg73S"
      },
      "outputs": [],
      "source": [
        "def r4_reward(sentence):\n",
        "  sentence_w0 = [i for i in sentence if i!=0]  # remove the zeros in the list\n",
        "  unique_tokens = np.unique(np.array(sentence_w0))  # scroll the list according to the unique elements, in such a way I do not need to remove each elem everytime\n",
        "  n_repetead_tokens = 0\n",
        "\n",
        "  for elem in unique_tokens:\n",
        "    repetitions = sentence_w0.count(elem)  # count the repetitions for the elem\n",
        "\n",
        "    if repetitions > 1:  # if I have more than 1 repetitions\n",
        "      n_repetead_tokens += 1  # increase the counter\n",
        "  \n",
        "  r4 = 1 - (n_repetead_tokens/len(unique_tokens))\n",
        "  return r4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "gPZgJUKWuIY3",
        "outputId": "8abe4a07-e7a3-4667-f0f2-17b38eeebde9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 478750579/478750579 [00:27<00:00, 17607007.96B/s]\n",
            "100%|██████████| 656/656 [00:00<00:00, 323017.54B/s]\n",
            "100%|██████████| 815973/815973 [00:00<00:00, 2075602.92B/s]\n",
            "100%|██████████| 458495/458495 [00:00<00:00, 3275212.83B/s]\n",
            "WARNING:pytorch_pretrained_bert.tokenization_openai:ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_perplexity = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
        "model_perplexity.load_state_dict(torch.load(\"/gdrive/MyDrive/NLP/model_perplexity_best.th\"))\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer_perplexity = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
        "# Alpha equal to max of ppl\n",
        "alpha = 70000\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model_perplexity.eval()\n",
        "model_perplexity.to(device)\n",
        "\n",
        "def perplexity_score(sentence):\n",
        "    tokenize_input = tokenizer_perplexity.tokenize(sentence)\n",
        "    tensor_input = torch.tensor([tokenizer_perplexity.convert_tokens_to_ids(tokenize_input)])\n",
        "    tensor_input = tensor_input.to(device)\n",
        "    loss=model_perplexity(tensor_input, lm_labels=tensor_input)\n",
        "    score = ((alpha-math.exp(loss))/alpha)**3\n",
        "    return np.clip(score, -1.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6N-qN8VHoT7D"
      },
      "outputs": [],
      "source": [
        "def eq_sentence(sen1, sen2):\n",
        "  return (len(sen1)-np.count_nonzero(sen1-sen2))/len(sen1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "R5TD27buLCsZ"
      },
      "outputs": [],
      "source": [
        "r_quality_list, r4_list, r3_list = [], [], []  # , r1_list = [], [], [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NFn8mbbrCf0A"
      },
      "outputs": [],
      "source": [
        "class Env:\n",
        "  def __init__(self, tokenizer, max_seq_length, train_dataset, train_dataset_topics, \n",
        "               evaluator_quality, score_predictions):  # , evaluator_r1):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.start_sentence = random.choice(train_dataset)\n",
        "    self.train_dataset = train_dataset\n",
        "    self.prev_sentence = []\n",
        "    self.evaluator_quality = evaluator_quality\n",
        "    # self.evaluator_r1 = evaluator_r1\n",
        "    self.train_dataset = [list(self.train_dataset)]\n",
        "    self.train_dataset.append([])\n",
        "    self.train_dataset[1] = score_predictions\n",
        "    self.train_dataset_topics = train_dataset_topics\n",
        "      \n",
        "  def get_reward(self, new_sentence):\n",
        "    # Padding new sentence\n",
        "    sentence_r = new_sentence[1:] if new_sentence[1] == self.tokenizer.vocab[\"[start]\"] else new_sentence\n",
        "    oov_token_np = np.where(np.array(sentence_r)==self.tokenizer.vocab[\"OOV_TOKEN\"])[0]\n",
        "    end_token_np = np.where(np.array(sentence_r)==self.tokenizer.vocab[\"[end]\"])[0]\n",
        "    if len(oov_token_np)>0 and len(end_token_np)>0:\n",
        "      token_np = np.min([oov_token_np[0], end_token_np[0]])\n",
        "    elif len(oov_token_np)>0 and not len(end_token_np)>0:\n",
        "      token_np =oov_token_np[0]\n",
        "    elif not len(oov_token_np)>0 and len(end_token_np)>0:\n",
        "      token_np = end_token_np[0]\n",
        "    else:\n",
        "      token_np = len(sentence_r)\n",
        "    sentence_r = sentence_r[:token_np]\n",
        "    sentence_r = sentence_r + [0] * (self.max_seq_length - len(sentence_r))\n",
        "    sentence_r = np.array(sentence_r[:self.max_seq_length])\n",
        "\n",
        "    bert_sentence = \" \".join(tokenizer.convert_ids_to_tokens([sentence_r]))\n",
        "    oov_index = bert_sentence.find(\"OOV_TOKEN\")\n",
        "    bert_sentence = bert_sentence[:oov_index]\n",
        "    bert_sentence = bert_sentence.strip()\n",
        "\n",
        "    if len(bert_sentence) >= 1:\n",
        "      print(\"bert_sentence: \", bert_sentence)\n",
        "      \n",
        "      topic_sentence = bert_sentence + \". \" + self.topic\n",
        "      \n",
        "      with torch.no_grad():\n",
        "          r3 = perplexity_score(bert_sentence)\n",
        "          # r1_scores = self.evaluator_r1(np.array([topic_sentence]))[0]\n",
        "          new_score = self.evaluator_quality(np.array([bert_sentence]))\n",
        "\n",
        "      r_quality = new_score - self.score_start_sentence\n",
        "      # r1 = r1_scores[0] - 1.0 * r1_scores[1]\n",
        "      r4 = r4_reward(sentence_r)\n",
        "      \n",
        "      print(1.0 * r_quality.numpy()[0][0], \" \", 0.0 * r4, \" \", 0.0 * r3)  # , \" \", 0.0 * r1.numpy())\n",
        "      reward = 1.0 * r_quality.numpy()[0][0] + 0.0 * r4 + 0.0 * r3  # + 0.0 * r1.numpy()\n",
        "\n",
        "      r_quality_list.append(r_quality)\n",
        "      r4_list.append(r4)\n",
        "      # r1_list.append(r1)\n",
        "      r3_list.append(r3)\n",
        "    else:\n",
        "      reward = -1.0\n",
        "\n",
        "    return reward\n",
        "\n",
        "  def reset(self):\n",
        "    id = random.randint(0, len(self.train_dataset[0])-1)\n",
        "    self.start_sentence = self.train_dataset[0][id]\n",
        "    self.topic = self.train_dataset_topics[id]\n",
        "    self.score_start_sentence = self.train_dataset[1][id]\n",
        "\n",
        "    if len(self.start_sentence) >= self.max_seq_length:\n",
        "      self.start_sentence = np.array(self.start_sentence[:max_seq_length-1])\n",
        "\n",
        "    if len(self.start_sentence) <= self.max_seq_length-2:\n",
        "      start_sentence = [self.tokenizer.vocab[\"[start]\"]] \n",
        "      start_sentence.extend(self.start_sentence)\n",
        "      start_sentence.extend([self.tokenizer.vocab[\"[end]\"]])\n",
        "      start_sentence = list(start_sentence) + [0] * (max_seq_length - len(start_sentence))\n",
        "      start_sentence = np.array(start_sentence[:max_seq_length])\n",
        "    elif len(self.start_sentence) <= self.max_seq_length-1:\n",
        "      start_sentence = [self.tokenizer.vocab[\"[start]\"]] \n",
        "      start_sentence.extend(self.start_sentence)\n",
        "      start_sentence[-1] = self.tokenizer.vocab[\"[end]\"]\n",
        "\n",
        "    return start_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RekHwaR1fzab"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "r-Ozga2omTKR"
      },
      "outputs": [],
      "source": [
        "def plots(r_quality_list, r4_list, r3_list, avg_reward_list, avg_actor_list, avg_critic_list, r1_list=None):\n",
        "    plt.plot(np.array(r_quality_list).ravel())\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Quality Score\")\n",
        "    plt.show()\n",
        "    plt.plot(r4_list)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Words Repetion\")\n",
        "    plt.show()\n",
        "    if r1_list is not None:\n",
        "      plt.plot(r1_list)\n",
        "      plt.xlabel(\"Episode\")\n",
        "      plt.ylabel(\"Topic Score\")\n",
        "      plt.show()\n",
        "    plt.plot(r3_list)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Syntantical Score\")\n",
        "    plt.show()\n",
        "    plt.plot(avg_reward_list)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "    plt.show()\n",
        "    plt.plot(avg_actor_list)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Avg. Actor Loss\")\n",
        "    plt.show()\n",
        "    plt.plot(avg_critic_list)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Avg. Critic Loss\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "MrOq7ibm8F3q"
      },
      "outputs": [],
      "source": [
        "std_dev = 0.1\n",
        "ou_noise = OUActionNoise(mean=np.zeros((max_seq_length, VOCAB_SIZE+1)), \n",
        "                         std_deviation=float(std_dev) * np.ones((max_seq_length, VOCAB_SIZE+1)), \n",
        "                         gaussian=True)\n",
        "\n",
        "# fnet.load_weights(\"/gdrive/MyDrive/NLP/pretrained_fnet_one_step.h5\")\n",
        "actor_model = create_actor(max_seq_length)\n",
        "critic_model = create_critic(max_seq_length)\n",
        "\n",
        "target_critic = create_critic(max_seq_length)\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 3e-3\n",
        "actor_lr = 3e-3\n",
        "\n",
        "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "# Train frequency in episodes\n",
        "train_freq = 4\n",
        "\n",
        "# Plot frequency in episodes\n",
        "plot_freq = 20\n",
        "# Number of episodes\n",
        "total_episodes = 20000\n",
        "# Coef for target weights update\n",
        "tau = 0.1\n",
        "# Prob of adding noise to action (0-100), gradually reduced during training\n",
        "prob_noise = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON-aY-IcaojF"
      },
      "source": [
        "Deleting datasets already used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-TciJVCaoD8"
      },
      "outputs": [],
      "source": [
        "del pretrain_series\n",
        "del df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0reOJ38gFzJo"
      },
      "outputs": [],
      "source": [
        "# del buffer\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtqJ-YFG8IyU"
      },
      "outputs": [],
      "source": [
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "# To store average actor loss\n",
        "avg_actor_list = []\n",
        "# To store average critic loss\n",
        "avg_critic_list = []\n",
        "\n",
        "buffer = Buffer(BUFFER_SIZE, BATCH_SIZE_RL, 1, TRAIN_BUFFER)\n",
        "env = Env(tokenizer, max_seq_length, x_train_rl, topic_list, bert_model_quality, \n",
        "          score_predictions)  # , bert_model_r1\n",
        "\n",
        "cont_update = 0\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    start_sentence = env.reset()\n",
        "    prev_state_tr = tf.expand_dims(tf.convert_to_tensor(start_sentence), 0)\n",
        "\n",
        "    episodic_reward = 0\n",
        "\n",
        "    if np.random.randint(0, 100) <= prob_noise:  #- prob_noise*(1-ep/total_episodes):\n",
        "      new_sentence_softmax = policy(prev_state_tr, ou_noise)\n",
        "    else:\n",
        "      new_sentence_softmax = policy(prev_state_tr, None)\n",
        "    new_sentence = list(tf.argmax(new_sentence_softmax[0, :, :], axis=1).numpy())\n",
        "    # Receive state and reward from environment.\n",
        "    reward = env.get_reward(new_sentence)\n",
        "    tf_prev_state_2 = tf.expand_dims(tf.convert_to_tensor(new_sentence_softmax), 0)\n",
        "\n",
        "    buffer.record((prev_state_tr, reward, tf_prev_state_2))\n",
        "\n",
        "    ep_reward_list.append(reward)\n",
        "\n",
        "    print([tokenizer.tokenizer.index_word[x] for x in start_sentence if x != 0])\n",
        "    print([tokenizer.tokenizer.index_word[x] for x in new_sentence if x != 0])\n",
        "\n",
        "    # Mean of last 10 episodes\n",
        "    avg_reward = np.mean(ep_reward_list[-10:])\n",
        "    print(\"Episode * {} * Avg Reward is ==> {} * Episode Reward is ==> {}\".format(ep, avg_reward, reward))\n",
        "    print()\n",
        "    avg_reward_list.append(avg_reward)\n",
        "\n",
        "    # Training\n",
        "    if ep % train_freq == 0 and ep != 0 and buffer.buffer_counter >= 2*TRAIN_BUFFER:\n",
        "      print(\"Training\")\n",
        "      actor_loss_tmp, critic_loss_tmp = buffer.learn()\n",
        "      avg_actor_list.append(actor_loss_tmp)\n",
        "      avg_critic_list.append(critic_loss_tmp)\n",
        "      cont_update += 1\n",
        "      if cont_update % 3 == 0:\n",
        "        update_target(target_critic.variables, critic_model.variables, tau)\n",
        "        cont_update = 0\n",
        "      print(\"Actor Loss: \", actor_loss_tmp)\n",
        "      print(\"Critic Loss: \", critic_loss_tmp)\n",
        "      print()\n",
        "\n",
        "    if ep != 0 and ep % plot_freq == 0 and buffer.buffer_counter >= 2*TRAIN_BUFFER:\n",
        "      # Plotting graph\n",
        "      plots(r_quality_list, r4_list, r3_list, avg_reward_list, avg_actor_list, avg_critic_list)  # , r1_list=r1_list\n",
        " \n",
        "\n",
        "plots(r_quality_list, r4_list, r3_list, avg_reward_list, avg_actor_list, avg_critic_list)  # , r1_list=r1_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5q7qCv4vP59"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdh92KOZvULc"
      },
      "outputs": [],
      "source": [
        "assert False\n",
        "fnet.save_weights(\"/gdrive/MyDrive/NLP/final_fnet_one_step_00_00_00_lr33_256_32_20f_std01_fctanhActor.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TA02AdRk-ww"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "c3EZOYd2k-wz"
      },
      "outputs": [],
      "source": [
        "assert True\n",
        "path = '08_02_00_lr43_64_16_20f_std01_fctanhActor/'\n",
        "model = 'final_fnet_one_step_08_02_00_lr43_64_16_20f_std01_fctanhActor.h5'\n",
        "\n",
        "fnet.load_weights(\"/gdrive/MyDrive/NLP/RESULTS/\"+path+model)\n",
        "\n",
        "actor_model = fnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMBTpBSySLOE"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GvhGNVtrLXf"
      },
      "outputs": [],
      "source": [
        "new_sen = env.reset()\n",
        "sentence = \" \".join([tokenizer.tokenizer.index_word[x] for x in new_sen if x != 0])\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "KX14tD14crA6",
        "outputId": "97e68c0a-45fa-4991-c18f-cbcc0a261bc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sate voter wll worse; bullying learn per instigating immediate option irritant payoff workers' coercing surpassed crucial compare granting couldn't annually visited outlook reconfirms shouldn´t thru humanity” politic theirselves star rail pre hired mentioned debt higher scientic exploitation\n"
          ]
        }
      ],
      "source": [
        "# sentence = \"\"\n",
        "out = decode_sentence(sentence, preprocess_rl, actor_model)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUMizu4kV-Be"
      },
      "source": [
        "## Degeneration analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiFelkmbNeN9"
      },
      "outputs": [],
      "source": [
        "plots(r_quality_list, r4_list, r3_list, avg_reward_list, avg_actor_list, avg_critic_list)  # , r1_list=r1_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rewards distribution generated sentences"
      ],
      "metadata": {
        "id": "5wb5tKm9E8qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = []\n",
        "for sen in x_test_rl:\n",
        "  prev_state_tr = tf.expand_dims(tf.convert_to_tensor(sen), 0)\n",
        "  new_sentence_softmax = policy(prev_state_tr, None)\n",
        "  new_sentence = list(tf.argmax(new_sentence_softmax[0, :, :], axis=1).numpy())\n",
        "\n",
        "  sentence_r = new_sentence[1:] if new_sentence[1] == tokenizer.vocab[\"[start]\"] else new_sentence\n",
        "  oov_token_np = np.where(np.array(sentence_r)==tokenizer.vocab[\"OOV_TOKEN\"])[0]\n",
        "  end_token_np = np.where(np.array(sentence_r)==tokenizer.vocab[\"[end]\"])[0]\n",
        "  if len(oov_token_np)>0 and len(end_token_np)>0:\n",
        "    token_np = np.min([oov_token_np[0], end_token_np[0]])\n",
        "  elif len(oov_token_np)>0 and not len(end_token_np)>0:\n",
        "    token_np =oov_token_np[0]\n",
        "  elif not len(oov_token_np)>0 and len(end_token_np)>0:\n",
        "    token_np = end_token_np[0]\n",
        "  else:\n",
        "    token_np = len(sentence_r)\n",
        "  sentence_r = sentence_r[:token_np]\n",
        "  sentence_r = sentence_r + [0] * (max_seq_length - len(sentence_r))\n",
        "  sentence_r = np.array(sentence_r[:max_seq_length])\n",
        "\n",
        "  bert_sentence = \" \".join(tokenizer.convert_ids_to_tokens([sentence_r]))\n",
        "  oov_index = bert_sentence.find(\"OOV_TOKEN\")\n",
        "  bert_sentence = bert_sentence[:oov_index]\n",
        "  bert_sentence = bert_sentence.strip()\n",
        "\n",
        "  if len(bert_sentence) >= 1:\n",
        "    with torch.no_grad():\n",
        "        #r3 = perplexity_score(bert_sentence)\n",
        "        new_score = bert_model_quality(np.array([bert_sentence]))\n",
        "        #score_orig = bert_model_quality.predict(np.array(sen))\n",
        "    #r_quality = new_score - score_orig\n",
        "    rewards.append([bert_sentence, new_score.numpy()[0][0]])"
      ],
      "metadata": {
        "id": "sqAFdGhQFAgy"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(np.array(rewards))\n",
        "df.to_csv(\"/gdrive/MyDrive/NLP/RESULTS/\"+path+\"quality_reward.csv\")  # Remember to change file name\n",
        "df[1].astype(\"float\").mean()\n",
        "df[1].astype(\"float\").hist(rwidth=0.95, grid=False)\n",
        "plt.title(\"Quality Score by value of generated sentences\")\n",
        "plt.xlabel(\"Quality Score\")\n",
        "plt.xlim(left=0, right=1)"
      ],
      "metadata": {
        "id": "vxGmqFKWJkNR",
        "outputId": "4d6fef45-0958-4e26-f884-b98b3ff545bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "seq2seq_arg_quality_one_step.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{"cells":[{"cell_type":"markdown","source":["## Installations"],"metadata":{"id":"u6HM6NwrJ5XM"}},{"cell_type":"code","source":["!pip install -q tf-models-official"],"metadata":{"id":"4gXdnUBOJ7xT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5qaWm88Shmy"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIicDYvXShmz"},"outputs":[],"source":["import os\n","import re\n","import numpy as np\n","from tqdm import tqdm\n","\n","# Tensorflow\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.callbacks import Callback\n","from tensorflow.keras import layers\n","\n","# BERT\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","\n","# Custom activation function\n","import tensorflow_probability as tfp\n","from keras.engine.base_layer import Layer\n","from keras import backend as K\n","\n","# Embeddings\n","import gensim  \n","import gensim.downloader as gloader\n","\n","# Data & Pre-processing\n","import nltk\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","import pandas as pd\n","\n","\n","# Defining hyperparameters\n","BUFFER_SIZE = 10000\n","EMBED_DIM = 100\n","LATENT_DIM = 512\n","NUM_HEADS = 8\n","BATCH_SIZE = 512\n","TAU = 5  # used by the Gumbel Activation\n","ALPHA = 0.99  # used by the Gumbel Activation\n","FREQ_CHANGE = 5  # used by the Gumbel Activation, freq of change in epochs\n","\n","# Datasets\n","pre_train_dataset = \"/gdrive/MyDrive/NLP/UKP_ASPECT/UKP_ASPECT.tsv\"\n","train_dataset = \"/gdrive/MyDrive/NLP/arg_quality_rank_30k.csv\"\n","\n","# Embedding model\n","embed_model = \"/gdrive/MyDrive/NLP/glove_{}_pickle\".format(EMBED_DIM)\n","\n","# BERT model weights\n","model_weights = \"/gdrive/MyDrive/NLP/classifierIBM30k.h5\""]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive')"],"metadata":{"id":"9f-_HkWYic97"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d448XZAuShmz"},"source":["## Data"]},{"cell_type":"markdown","source":["### Methods"],"metadata":{"id":"h5rlr-6GTD7F"}},{"cell_type":"code","source":["lemmatizer = WordNetLemmatizer()\n","def preprocess_pretrain(sentence):\n","    sentence = sentence.lower()\n","    # Adding a space between the punctuation and the last word to allow better tokenization\n","    sentence = re.sub(\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = sentence.strip()\n","    sentence = \" \".join([\"[start]\", sentence, \"[end]\"])\n","    sentence = \" \".join([lemmatizer.lemmatize(x) for x in sentence.split()])\n","    return sentence\n","\n","\n","def preprocess_train(text):\n","    text = re.sub('\\\"|-|\\\\\\\\|`', ' ', text)  # delete this chars from the string [\"-\\`]\n","    text = re.sub('\\n', ' ', text)\n","    text = re.sub('^[.]+', '', text)         # delete dots at the beginning of the sentence\n","    text = re.sub(\"([?.!,])\", r\" \\1 \", text)\n","    text = re.sub('\\. \\.', '.', text)        # delete . .\n","    text = re.sub('&', ' and ', text)        # replace & with and\n","    text = re.sub(' +', ' ', text)           # delete additional whitespace\n","    text = text.rstrip()                  \n","    text = text.lstrip()\n","    text = \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n","    return text\n","\n","\n","def load_embedding_model(model_type: str,\n","                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n","    \"\"\"\n","    Loads a pre-trained word embedding model via gensim library.\n","\n","    :param model_type: name of the word embedding model to load.\n","    :param embedding_dimension: size of the embedding space to consider\n","\n","    :return\n","        - pre-trained word embedding model (gensim KeyedVectors object)\n","    \"\"\"\n","\n","    download_path = \"\"\n","\n","    # Find the correct embedding model name\n","    if model_type.strip().lower() == 'word2vec':\n","        download_path = \"word2vec-google-news-300\"\n","\n","    elif model_type.strip().lower() == 'glove':\n","        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n","    elif model_type.strip().lower() == 'fasttext':\n","        download_path = \"fasttext-wiki-news-subwords-300\"\n","    else:\n","        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n","\n","    # Check download\n","    try:\n","        emb_model = gloader.load(download_path)\n","    except ValueError as e:\n","        print(\"Invalid embedding model name! Check the embedding dimension:\")\n","        print(\"Word2Vec: 300\")\n","        print(\"Glove: 50, 100, 200, 300\")\n","        raise e\n","\n","    return emb_model\n","\n","\n","def check_OOV_terms(embedding_vocabulary, word_listing):\n","    \"\"\"\n","    Checks differences between pre-trained embedding model vocabulary\n","    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n","\n","    :param embedding_vocabulary: pre-trained word embedding model vocab (list)\n","    :param word_listing: dataset specific vocabulary (list)\n","\n","    :return\n","        - list of OOV terms\n","    \"\"\"\n","    \n","    oov = set(word_listing).difference(embedding_vocabulary)\n","    return list(oov)\n","\n","\n","def build_embedding_matrix(embedding_model,\n","                           embedding_dimension,\n","                           word_to_idx,\n","                           vocab_size,\n","                           oov_terms):\n","    \"\"\"\n","    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n","\n","    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n","    :param word_to_idx: vocabulary map (word -> index) (dict)\n","    :param vocab_size: size of the vocabulary\n","    :param oov_terms: list of OOV terms (list)\n","\n","    :return\n","        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n","    \"\"\"\n","    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n","\n","    for word, idx in tqdm(word_to_idx.items()):\n","        try:\n","            embedding_vector = embedding_model[word]\n","        except (KeyError, TypeError):\n","            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n","\n","        embedding_matrix[idx] = embedding_vector\n","\n","    return embedding_matrix\n","\n","\n","def update_embedding_matrix(embedding_model, \n","                            embedding_dimension,\n","                            word_to_idx,\n","                            vocab_size,\n","                            oov_terms):\n","    \"\"\"\n","    Builds the embedding matrix of a specific dataset given a pre-trained emdedding matrix\n","\n","    :param embedding_model: pre-trained embedding matrix\n","    :param word_to_idx: vocabulary map (word -> index) (dict)\n","    :param vocab_size: size of the vocabulary\n","    :param oov_terms: list of OOV terms (list)\n","\n","    :return\n","        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n","    \"\"\"\n","    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n","\n","    for word, idx in tqdm(word_to_idx.items()):\n","        try:\n","            embedding_vector = embedding_model[idx]\n","        except (TypeError, IndexError):\n","            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n","\n","        embedding_matrix[idx] = embedding_vector\n","\n","    return embedding_matrix\n","\n","\n","class KerasTokenizer(object):\n","    \"\"\"\n","    A simple high-level wrapper for the Keras tokenizer.\n","    \"\"\"\n","\n","    def __init__(self, build_embedding_matrix=False, embedding_dimension=None,\n","                 embedding_model_type=None, tokenizer_args=None, embedding_model=None):\n","        if build_embedding_matrix:\n","            assert embedding_model_type is not None\n","            assert embedding_dimension is not None and type(embedding_dimension) == int\n","\n","        self.build_embedding_matrix = build_embedding_matrix\n","        self.embedding_dimension = embedding_dimension\n","        self.embedding_model_type = embedding_model_type\n","        self.embedding_model = embedding_model\n","        self.embedding_matrix = None\n","        self.vocab = None\n","\n","        tokenizer_args = {} if tokenizer_args is None else tokenizer_args\n","        assert isinstance(tokenizer_args, dict) or isinstance(tokenizer_args, collections.OrderedDict)\n","\n","        self.tokenizer_args = tokenizer_args\n","\n","    def build_vocab(self, data, **kwargs):\n","        print('Fitting tokenizer...')\n","        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(**self.tokenizer_args)\n","        self.tokenizer.fit_on_texts(data)\n","        print('Fit completed!')\n","\n","        self.vocab = self.tokenizer.word_index\n","\n","        if self.build_embedding_matrix:\n","            if self.embedding_model is None:\n","              print('Loading embedding model! It may take a while...')\n","              self.embedding_model = load_embedding_model(model_type=self.embedding_model_type, \n","                                                          embedding_dimension=self.embedding_dimension)\n","            \n","            print('Checking OOV terms in train...')\n","            self.oov_terms_train = check_OOV_terms(embedding_vocabulary=set(self.embedding_model.vocab.keys()),\n","                                             word_listing=list(self.vocab.keys()))\n","            \n","            print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms_train), 100*float(len(self.oov_terms_train)) / len(self.vocab)))\n","\n","            print('Building the embedding matrix for train...')\n","            self.embedding_matrix = build_embedding_matrix(embedding_model=self.embedding_model,\n","                                                           word_to_idx=self.vocab,\n","                                                           vocab_size=len(self.vocab)+1,          \n","                                                           embedding_dimension=self.embedding_dimension,\n","                                                           oov_terms=self.oov_terms_train)\n","            print('Done for train!')\n","\n","    def update_vocab(self, data, **kwargs):\n","      self.tokenizer.fit_on_texts(data)\n","      if self.build_embedding_matrix:\n","        old_vocab = self.vocab\n","        self.vocab = self.tokenizer.word_index\n","        print('Checking OOV terms...')\n","        self.oov_terms = check_OOV_terms(embedding_vocabulary=set(old_vocab.keys()), \n","                                         word_listing=list(self.vocab.keys()))\n","        \n","        print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms), 100*float(len(self.oov_terms)) / len(self.vocab)))\n","\n","        print('Building the embedding matrix...')\n","        self.embedding_matrix = update_embedding_matrix(embedding_model=self.embedding_matrix,\n","                                                       word_to_idx=self.vocab,\n","                                                       vocab_size=len(self.vocab)+1,          \n","                                                       embedding_dimension=self.embedding_dimension,\n","                                                       oov_terms=self.oov_terms)\n","\n","    def get_info(self):\n","        return {\n","            'build_embedding_matrix': self.build_embedding_matrix,\n","            'embedding_dimension': self.embedding_dimension,\n","            'embedding_model_type': self.embedding_model_type,\n","            'embedding_matrix': self.embedding_matrix.shape if self.embedding_matrix is not None else self.embedding_matrix,\n","            'embedding_model': self.embedding_model,\n","            'vocab_size': len(self.vocab) + 1,\n","        }\n","\n","    def tokenize(self, text):\n","        return text\n","\n","    def convert_tokens_to_ids(self, tokens):\n","        if type(tokens) == str:\n","            return self.tokenizer.texts_to_sequences([tokens])[0]\n","        else:\n","            return self.tokenizer.texts_to_sequences(tokens)\n","\n","    def convert_ids_to_tokens(self, ids):\n","        return self.tokenizer.sequences_to_texts(ids)\n","\n","\n","def convert_text(df, tokenizer, is_training=False, max_seq_length=None):\n","    \"\"\"\n","    Converts input text sequences using a given tokenizer\n","\n","    :param texts: either a list or numpy ndarray of strings\n","    :tokenizer: an instantiated tokenizer\n","    :is_training: whether input texts are from the training split or not\n","    :max_seq_length: the max token sequence previously computed with\n","    training texts.\n","\n","    :return\n","        text_ids: a nested list on token indices\n","        max_seq_length: the max token sequence previously computed with\n","        training texts.\n","    \"\"\"\n","\n","    text_ids = tokenizer.convert_tokens_to_ids(df)\n","\n","    # Padding\n","    if is_training:\n","        max_seq_length = int(np.quantile([len(seq) for seq in text_ids], 0.95))\n","    else:\n","        assert max_seq_length is not None\n","\n","    text_ids = [seq + [0] * (max_seq_length - len(seq)) for seq in text_ids]\n","    text_ids = np.array([seq[:max_seq_length] for seq in text_ids])\n","\n","    if is_training:\n","        return text_ids, max_seq_length\n","    else:\n","        return text_ids\n","\n","\n","def decode_sentence(input_sentence, preprocess):\n","    # Mapping the input sentence to tokens and adding start and end tokens\n","    tokenized_input_sentence = tokenizer.convert_tokens_to_ids(\n","        [preprocess(input_sentence)]\n","    )[0]\n","    tokenized_input_sentence = tf.pad(\n","        tokenized_input_sentence,\n","        [[0, max_seq_length - tf.shape(tokenized_input_sentence)[0]]])\n","    # Initializing the initial sentence consisting of only the start token.\n","    tokenized_target_sentence = tf.expand_dims(tokenizer.vocab[\"[start]\"], 0)\n","    decoded_sentence = \"\"\n","\n","    predictions = fnet.predict(\n","        {\"encoder_inputs\": tf.expand_dims(tokenized_input_sentence, 0)}\n","    )\n","\n","    sampled_token_index = tf.argmax(predictions[0, :, :], axis=1)\n","    decoded_sentence = tokenizer.convert_ids_to_tokens([sampled_token_index.numpy()])\n","    \n","    return decoded_sentence"],"metadata":{"id":"Eu2t2HfLTIA0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loading "],"metadata":{"id":"Spn1n_WYTIhl"}},{"cell_type":"markdown","source":["Pre-training data"],"metadata":{"id":"_ZB4Qf0jTuKh"}},{"cell_type":"code","source":["pre_df = pd.read_csv(pre_train_dataset, sep='\\t')\n","pre_df = pre_df[pre_df['label']!=\"NS\"]\n","pre_df = pre_df[pre_df['label']!=\"DTORCD\"]\n","pre_df = pre_df.reset_index()\n","pre_df = pre_df.drop([\"index\", \"topic\", \"label\"], axis=1)\n","pre_df.head()"],"metadata":{"id":"UAGCEd_giml7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training data"],"metadata":{"id":"vS_F6le3TwYW"}},{"cell_type":"code","source":["df = pd.read_csv(train_dataset)\n","df = df.drop([\"WA\", \"stance_WA\", \"stance_WA_conf\"], axis=1)\n","df.head()"],"metadata":{"id":"YAHZs6KbTyXn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data preprocessing"],"metadata":{"id":"M_xjCBDUKm_i"}},{"cell_type":"markdown","source":["Pre-training"],"metadata":{"id":"gT_M3NdjUKzS"}},{"cell_type":"code","source":["sentence_1 = list(pre_df['sentence_1'].apply(preprocess_pretrain).values)\n","sentence_2 = list(pre_df['sentence_2'].apply(preprocess_pretrain).values)\n","sentence_1[:5]"],"metadata":{"id":"mZ-b9rEeKq3c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training"],"metadata":{"id":"HquNCeY3UMDH"}},{"cell_type":"code","source":["df['argument'] = df.apply(lambda row : preprocess_train(row['argument']), axis = 1)\n","df.loc[2,\"argument\"] = \"zero tolerance policy in schools should not be adopted as circumstances are often not black and white, being more nuanced. no one should be written off due to a mistake of judgement.\"\n","df.head()"],"metadata":{"id":"bYwrsXZvUM3_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train, test, val splits"],"metadata":{"id":"8RK-KLKUUl5b"}},{"cell_type":"code","source":["is_training_data =  df['set']=='train'\n","is_validation_data =  df['set']=='dev'\n","is_test_data =  df['set']=='test'\n","\n","training_data = df[is_training_data]\n","validation_data = df[is_validation_data]\n","test_data  = df[is_test_data ]\n","\n","x_train = training_data['argument']\n","x_val = validation_data['argument']\n","x_test = test_data['argument']"],"metadata":{"id":"BgkpkAnPUfm5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OwNEJrQoShm1"},"source":["### Tokenization"]},{"cell_type":"code","source":["# load embeddings from glove\n","import pickle\n","if os.path.exists(embed_model):\n","  with open(embed_model, \"rb\") as f:\n","    embedding_model = pickle.load(f)\n","else:\n","  embedding_model = load_embedding_model(model_type=\"glove\", \n","                                         embedding_dimension=EMBED_DIM)"],"metadata":{"id":"ecxi9TNColzy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating tokenizer and vocabulary\n","\n","tokenizer_args = {\n","    'oov_token': \"OOV_TOKEN\",  # The vocabulary id for unknown terms during text conversion\n","    'lower' : True,  # default\n","    'filters' : '' \n","}\n","\n","tokenizer = KerasTokenizer(tokenizer_args=tokenizer_args,\n","                           build_embedding_matrix=True,\n","                           embedding_dimension=EMBED_DIM,\n","                           embedding_model_type=\"glove\", \n","                           embedding_model=embedding_model)\n","\n","tokenizer.build_vocab(sentence_1)\n","tokenizer.update_vocab(sentence_2)\n","tokenizer.update_vocab(x_train)\n","tokenizer.update_vocab(x_val)\n","VOCAB_SIZE = len(tokenizer.vocab)\n","\n","tokenizer_info = tokenizer.get_info()\n","\n","print('Tokenizer info: ', tokenizer_info)"],"metadata":{"id":"4DIT2jkkooWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PD4GNbAvShm3"},"source":["### Tokenizing and padding sentences using `TextVectorization`"]},{"cell_type":"markdown","source":["Training"],"metadata":{"id":"yl0_iNLOYvrQ"}},{"cell_type":"code","source":["x_train, max_seq_length = convert_text(x_train, tokenizer, True)\n","x_val = convert_text(x_val, tokenizer, max_seq_length=max_seq_length)\n","print(\"Max token sequence: {}\".format(max_seq_length))\n","print('X train shape: ', x_train.shape)\n","print('X val shape: ', x_val.shape)"],"metadata":{"id":"yppEeUKlYwmP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pre-training"],"metadata":{"id":"9zT2SkLJYuRM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZCzVOJnShm4"},"outputs":[],"source":["x_pretrain = convert_text(sentence_1, tokenizer, \n","                          max_seq_length=max_seq_length)\n","x_preval = convert_text(sentence_2, tokenizer, max_seq_length=max_seq_length)\n","print('X pre-train shape: ', x_pretrain.shape)\n","print('X pre-val shape: ', x_preval.shape)"]},{"cell_type":"markdown","source":["### Tensorflow Dataset for Pre-training"],"metadata":{"id":"jbuI0HveZGHJ"}},{"cell_type":"code","source":["train_dataset = tf.data.Dataset.from_tensor_slices((x_pretrain, x_pretrain))\n","val_dataset = tf.data.Dataset.from_tensor_slices((x_preval, x_preval))\n","\n","def vectorize_text(inputs, outputs):\n","    # One extra padding token to the right to match the output shape\n","    outputs = tf.pad(outputs, [[0, 1]])\n","    return (\n","        {\"encoder_inputs\": inputs},\n","        {\"outputs\": outputs[1:]},\n","    )\n","\n","train_dataset = train_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n","val_dataset = val_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","train_dataset = (\n","    train_dataset.cache()\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE)\n","    .prefetch(tf.data.AUTOTUNE)\n",")\n","val_dataset = val_dataset.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"],"metadata":{"id":"tyzOlDBixOkx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Gumbel-softmax "],"metadata":{"id":"VudUzldM6lK4"}},{"cell_type":"code","source":["def gumbel_func(x, tau, dist):\n","  return K.softmax(1/tau * (x - dist.sample(K.shape(x))))\n","\n","class gumbel_softmax(Layer):\n","  def __init__(self, tau=0.3, alpha=0.999, **kwargs):\n","    super(gumbel_softmax, self).__init__(**kwargs)\n","    self.gumbel_dist = tfp.distributions.Gumbel(0.0, 0.0, name='Gumbel')\n","    self.tau = K.cast_to_floatx(tau)\n","    self.alpha = K.cast_to_floatx(alpha)\n","\n","  def call(self, x):\n","    result = gumbel_func(x, self.tau, self.gumbel_dist)\n","    return result"],"metadata":{"id":"4vd9lBUy6bKr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class activation_callback(Callback):\n","  def __init__(self, tau, alpha):\n","    self.alpha = alpha\n","    self.tau = tau\n","\n","  def on_epoch_end(self, epoch, logs={}):\n","    self.tau.assign(self.tau * K.pow(self.alpha, (epoch % FREQ_CHANGE==0)*epoch))"],"metadata":{"id":"5ZXMasL-SsdE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Seq2Seq Model"],"metadata":{"id":"mI3q1UV-Q7Kp"}},{"cell_type":"markdown","metadata":{"id":"lp6jfLNIShm5"},"source":["### Creating the FNet Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kylhhrZyShm5"},"outputs":[],"source":["class FNetEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, **kwargs):\n","        super(FNetEncoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.dense_proj = keras.Sequential(\n","            [\n","                layers.Dense(dense_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","\n","    def call(self, inputs):\n","        # Casting the inputs to complex64\n","        inp_complex = tf.cast(inputs, tf.complex64)\n","        # Projecting the inputs to the frequency domain using FFT2D and\n","        # extracting the real part of the output\n","        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n","        proj_input = self.layernorm_1(inputs + fft)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)"]},{"cell_type":"markdown","metadata":{"id":"mHuNwBidShm6"},"source":["### Creating the Decoder One Step"]},{"cell_type":"code","source":["class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super(PositionalEmbedding, self).__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","\n","class FNetDecoderOneStep(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super(FNetDecoderOneStep, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [\n","                layers.Dense(latent_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(encoder_outputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","        else:\n","          padding_mask = causal_mask\n","        attention_output_2 = self.attention_2(\n","            query=encoder_outputs,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(encoder_outputs + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)\n","\n","\n","def create_model_one_step(max_length, alpha, tau):\n","    # Encoder\n","    encoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n","    x = PositionalEmbedding(max_length, VOCAB_SIZE+1, EMBED_DIM)(encoder_inputs)\n","    encoder_outputs = FNetEncoder(EMBED_DIM, LATENT_DIM)(x)\n","    encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","    # Encoder -> Decoder\n","    encoded_seq_inputs = keras.Input(\n","        shape=(None, EMBED_DIM), name=\"decoder_state_inputs\"\n","    )\n","    \n","    # \"Merge\" inputs Decoder\n","    x = FNetDecoderOneStep(EMBED_DIM, LATENT_DIM, NUM_HEADS)(encoded_seq_inputs)\n","    x = layers.Dropout(0.2)(x)\n","    gumb_act = gumbel_softmax(tau=tau, alpha=alpha)\n","    decoder_outputs = layers.Dense(VOCAB_SIZE+1, \n","                                   activation=gumb_act)(x)\n","\n","    decoder = keras.Model(encoded_seq_inputs, decoder_outputs, name=\"outputs\")\n","    decoder_outputs = decoder(encoder_outputs)\n","    fnet = keras.Model(encoder_inputs, decoder_outputs, name=\"fnet\")\n","    return fnet"],"metadata":{"id":"4gUArwKswdcF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IdrNwBmsShm7"},"source":["### Creating and Pre-Training the seq2seq model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HN58kiEwShm7"},"outputs":[],"source":["alpha = K.constant(ALPHA)\n","tau = K.variable(TAU)\n","fnet = create_model_one_step(max_seq_length, alpha, tau)\n","fnet.compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tA1UHp5gShm7"},"outputs":[],"source":["history = fnet.fit(train_dataset, epochs=90, validation_data=val_dataset, \n","                   callbacks=[activation_callback(tau, alpha)])"]},{"cell_type":"markdown","metadata":{"id":"trqSKiU0Shm8"},"source":["### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2WfVMhSbShm8"},"outputs":[],"source":["sentence = \"marriage isn't keeping up with the times. abandon the old thinking and bring something that incorporates all unions not just those with a man and woman.\"\n","out = decode_sentence(sentence, preprocess_pretrain)\n","print(out)"]},{"cell_type":"markdown","source":["### Save Model"],"metadata":{"id":"h9TpGtgfvXW8"}},{"cell_type":"code","source":["fnet.save(\"drive/MyDrive/Colab Notebooks/NLP/pretrained_fnet.h5\")"],"metadata":{"id":"iLX_SomHvd5k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [Bert](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb)"],"metadata":{"id":"g-Bn64skKOWD"}},{"cell_type":"markdown","source":["### Model to fine-tune"],"metadata":{"id":"_1GvRSj9QpgK"}},{"cell_type":"code","source":["bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n","\n","tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n","tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n","\n","print(f'Model name                    : {bert_model_name}')\n","print(f'BERT model selected           : {tfhub_handle_encoder}')\n","print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"],"metadata":{"id":"FNHbHJK-KPiz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_classifier_model(dense_size=100):\n","  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n","  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n","  encoder_inputs = preprocessing_layer(text_input)\n","  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","  outputs = encoder(encoder_inputs)\n","  net = outputs['pooled_output']\n","  net = tf.keras.layers.Dense(dense_size, activation=keras.activations.relu, name='fc')(net)\n","  net = tf.keras.layers.Dense(1, activation=keras.activations.sigmoid, name='classifier')(net)\n","  return tf.keras.Model(text_input, net)"],"metadata":{"id":"8uH8ohE9KUcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_model = build_classifier_model()"],"metadata":{"id":"CZlbTvKXKXHp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_model.summary()"],"metadata":{"id":"k01nAznZKYrz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load best model"],"metadata":{"id":"jjHmM4jBQsh0"}},{"cell_type":"code","source":["bert_model.load_weights(model_weights)"],"metadata":{"id":"4Hl6UOAKKabL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train"],"metadata":{"id":"-dF73pIvSKQS"}},{"cell_type":"markdown","source":["### RL creation"],"metadata":{"id":"r-up19isdzVS"}},{"cell_type":"code","source":["class GenEval(keras.Model):\n","    def __init__(self, evaluator, generator):\n","        super(GenEval, self).__init__()\n","        self.evaluator = evaluator\n","        self.generator = generator\n","        self.table = tf.lookup.StaticHashTable(\n","            initializer=tf.lookup.KeyValueTensorInitializer(\n","                keys=tf.constant(list(tokenizer.tokenizer.index_word.keys())),\n","                values=tf.constant(list(tokenizer.tokenizer.index_word.values())),\n","            ),\n","            default_value=tf.constant(\"OOV_TOKEN\"),\n","            name=\"index_word\"\n","        )\n","        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n","\n","    @property\n","    def metrics(self):\n","        return [self.gen_loss_tracker]\n","\n","    @tf.function\n","    def join(self, l):\n","      return tf.strings.join(tf.split(l, num_or_size_splits=l.shape[1], axis=1))\n","\n","    def compile(self, g_optimizer, loss_fn):\n","        super(GenEval, self).compile()\n","        self.g_optimizer = g_optimizer\n","        self.loss_fn = loss_fn\n","\n","    def train_step(self, data):\n","        with tf.GradientTape() as tape:\n","            predictions = self.generator(data)\n","            sampled_token_index = tf.argmax(predictions[:, :, :], axis=2, output_type=\"int32\")\n","            decoded_sentences = self.table.lookup(sampled_token_index)\n","            decoded_sentences = self.join(decoded_sentences)\n","            g_loss = self.evaluator(decoded_sentences)\n","        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n","        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n","\n","        # Monitor loss.\n","        self.gen_loss_tracker.update_state(g_loss)\n","        return {\n","            \"g_loss\": self.gen_loss_tracker.result()\n","        }"],"metadata":{"id":"wJNBs53OelPe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"Ku1kOgS5d08y"}},{"cell_type":"code","source":["gen_eval = GenEval(evaluator=bert_model, generator=fnet)\n","gen_eval.compile(\n","    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n","    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",")\n","\n","gen_eval.fit(x_train, epochs=20, batch_size=16)"],"metadata":{"id":"0U3AoXTJq0n1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save Model"],"metadata":{"id":"t5q7qCv4vP59"}},{"cell_type":"code","source":["fnet.save(\"drive/MyDrive/Colab Notebooks/NLP/final_fnet.h5\")"],"metadata":{"id":"fdh92KOZvULc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"tMBTpBSySLOE"}},{"cell_type":"code","source":["sentence = \"marriage isn't keeping up with the times. abandon the old thinking and bring something that incorporates all unions not just those with a man and woman.\"\n","out = decode_sentence(sentence, preprocess_train)\n","print(out)"],"metadata":{"id":"KX14tD14crA6"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"seq2seq_arg_quality.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElFosco/NLP_argument_creation/blob/dev_rl/seq2seq_arg_quality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations"
      ],
      "metadata": {
        "id": "u6HM6NwrJ5XM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official"
      ],
      "metadata": {
        "id": "4gXdnUBOJ7xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5qaWm88Shmy"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIicDYvXShmz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# BERT\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "# Embeddings\n",
        "import gensim  \n",
        "import gensim.downloader as gloader\n",
        "\n",
        "# Data & Pre-processing\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Defining hyperparameters\n",
        "BUFFER_SIZE = 50000\n",
        "TRAIN_BUFFER = 2048  # Shoud be lower than BUFFER_SIZE\n",
        "EMBED_DIM = 100\n",
        "LATENT_DIM = 512\n",
        "NUM_HEADS = 8\n",
        "BATCH_SIZE = 256\n",
        "BATCH_SIZE_RL = 32\n",
        "\n",
        "# Datasets\n",
        "train_dataset = \"/gdrive/MyDrive/NLP/arg_quality_rank_30k.csv\"\n",
        "\n",
        "# Embedding model\n",
        "embed_model = \"/gdrive/MyDrive/NLP/glove_{}_pickle\".format(EMBED_DIM)\n",
        "\n",
        "# BERT models weights\n",
        "quality_model_weights = \"/gdrive/MyDrive/NLP/classifierIBM30k.h5\"\n",
        "r1_model_weights = \"/gdrive/MyDrive/NLP/classifierNLI.h5\"\n",
        "\n",
        "# BERT model scores of start sentences, based on best model of BERT\n",
        "start_scores = \"/gdrive/MyDrive/NLP/start_scores_bert_pickle\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "9f-_HkWYic97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d448XZAuShmz"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Methods"
      ],
      "metadata": {
        "id": "h5rlr-6GTD7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def preprocess_pretrain(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\\"|-|\\\\\\\\|`', ' ', text)  # delete this chars from the string [\"-\\`]\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('^[.]+', '', text)         # delete dots at the beginning of the sentence\n",
        "    text = re.sub(\"([?.!,])\", r\" \\1 \", text)\n",
        "    text = re.sub('\\. \\.', '.', text)        # delete . .\n",
        "    text = re.sub('&', ' and ', text)        # replace & with and\n",
        "    text = re.sub(' +', ' ', text)           # delete additional whitespace\n",
        "    text = text.strip()\n",
        "    text = \" \".join([\"[start]\", text, \"[end]\"])\n",
        "    text = \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_rl(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\\"|-|\\\\\\\\|`', ' ', text)  # delete this chars from the string [\"-\\`]\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('^[.]+', '', text)         # delete dots at the beginning of the sentence\n",
        "    text = re.sub(\"([?.!,])\", r\" \\1 \", text)\n",
        "    text = re.sub('\\. \\.', '.', text)        # delete . .\n",
        "    text = re.sub('&', ' and ', text)        # replace & with and\n",
        "    text = re.sub(' +', ' ', text)           # delete additional whitespace\n",
        "    text = text.strip()\n",
        "    text = \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n",
        "    return text\n",
        "\n",
        "\n",
        "def load_embedding_model(model_type: str,\n",
        "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"\"\n",
        "\n",
        "    # Find the correct embedding model name\n",
        "    if model_type.strip().lower() == 'word2vec':\n",
        "        download_path = \"word2vec-google-news-300\"\n",
        "\n",
        "    elif model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "    elif model_type.strip().lower() == 'fasttext':\n",
        "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
        "\n",
        "    # Check download\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Word2Vec: 300\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model\n",
        "\n",
        "\n",
        "def check_OOV_terms(embedding_vocabulary, word_listing):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_vocabulary: pre-trained word embedding model vocab (list)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    \n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)\n",
        "\n",
        "\n",
        "def build_embedding_matrix(embedding_model,\n",
        "                           embedding_dimension,\n",
        "                           word_to_idx,\n",
        "                           vocab_size,\n",
        "                           oov_terms):\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[word]\n",
        "        except (KeyError, TypeError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def update_embedding_matrix(embedding_model, \n",
        "                            embedding_dimension,\n",
        "                            word_to_idx,\n",
        "                            vocab_size,\n",
        "                            oov_terms):\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained emdedding matrix\n",
        "\n",
        "    :param embedding_model: pre-trained embedding matrix\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[idx]\n",
        "        except (TypeError, IndexError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "class KerasTokenizer(object):\n",
        "    \"\"\"\n",
        "    A simple high-level wrapper for the Keras tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, build_embedding_matrix=False, embedding_dimension=None,\n",
        "                 embedding_model_type=None, tokenizer_args=None, embedding_model=None):\n",
        "        if build_embedding_matrix:\n",
        "            assert embedding_model_type is not None\n",
        "            assert embedding_dimension is not None and type(embedding_dimension) == int\n",
        "\n",
        "        self.build_embedding_matrix = build_embedding_matrix\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.embedding_model_type = embedding_model_type\n",
        "        self.embedding_model = embedding_model\n",
        "        self.embedding_matrix = None\n",
        "        self.vocab = None\n",
        "\n",
        "        tokenizer_args = {} if tokenizer_args is None else tokenizer_args\n",
        "        assert isinstance(tokenizer_args, dict) or isinstance(tokenizer_args, collections.OrderedDict)\n",
        "\n",
        "        self.tokenizer_args = tokenizer_args\n",
        "\n",
        "    def build_vocab(self, data, **kwargs):\n",
        "        print('Fitting tokenizer...')\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(**self.tokenizer_args)\n",
        "        self.tokenizer.fit_on_texts(data)\n",
        "        print('Fit completed!')\n",
        "\n",
        "        self.vocab = self.tokenizer.word_index\n",
        "\n",
        "        if self.build_embedding_matrix:\n",
        "            if self.embedding_model is None:\n",
        "              print('Loading embedding model! It may take a while...')\n",
        "              self.embedding_model = load_embedding_model(model_type=self.embedding_model_type, \n",
        "                                                          embedding_dimension=self.embedding_dimension)\n",
        "            \n",
        "            print('Checking OOV terms in train...')\n",
        "            self.oov_terms_train = check_OOV_terms(embedding_vocabulary=set(self.embedding_model.vocab.keys()),\n",
        "                                             word_listing=list(self.vocab.keys()))\n",
        "            \n",
        "            print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms_train), 100*float(len(self.oov_terms_train)) / len(self.vocab)))\n",
        "\n",
        "            print('Building the embedding matrix for train...')\n",
        "            self.embedding_matrix = build_embedding_matrix(embedding_model=self.embedding_model,\n",
        "                                                           word_to_idx=self.vocab,\n",
        "                                                           vocab_size=len(self.vocab)+1,          \n",
        "                                                           embedding_dimension=self.embedding_dimension,\n",
        "                                                           oov_terms=self.oov_terms_train)\n",
        "            print('Done for train!')\n",
        "\n",
        "    def update_vocab(self, data, **kwargs):\n",
        "      self.tokenizer.fit_on_texts(data)\n",
        "      if self.build_embedding_matrix:\n",
        "        old_vocab = self.vocab\n",
        "        self.vocab = self.tokenizer.word_index\n",
        "        print('Checking OOV terms...')\n",
        "        self.oov_terms = check_OOV_terms(embedding_vocabulary=set(old_vocab.keys()), \n",
        "                                         word_listing=list(self.vocab.keys()))\n",
        "        \n",
        "        print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms), 100*float(len(self.oov_terms)) / len(self.vocab)))\n",
        "\n",
        "        print('Building the embedding matrix...')\n",
        "        self.embedding_matrix = update_embedding_matrix(embedding_model=self.embedding_matrix,\n",
        "                                                       word_to_idx=self.vocab,\n",
        "                                                       vocab_size=len(self.vocab)+1,          \n",
        "                                                       embedding_dimension=self.embedding_dimension,\n",
        "                                                       oov_terms=self.oov_terms)\n",
        "\n",
        "    def get_info(self):\n",
        "        return {\n",
        "            'build_embedding_matrix': self.build_embedding_matrix,\n",
        "            'embedding_dimension': self.embedding_dimension,\n",
        "            'embedding_model_type': self.embedding_model_type,\n",
        "            'embedding_matrix': self.embedding_matrix.shape if self.embedding_matrix is not None else self.embedding_matrix,\n",
        "            'embedding_model': self.embedding_model,\n",
        "            'vocab_size': len(self.vocab) + 1,\n",
        "        }\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return text\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        if type(tokens) == str:\n",
        "            return self.tokenizer.texts_to_sequences([tokens])[0]\n",
        "        else:\n",
        "            return self.tokenizer.texts_to_sequences(tokens)\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        return self.tokenizer.sequences_to_texts(ids)\n",
        "\n",
        "\n",
        "def convert_text(df, tokenizer, is_training=False, max_seq_length=None, \n",
        "                 pretrain=False):\n",
        "    \"\"\"\n",
        "    Converts input text sequences using a given tokenizer\n",
        "\n",
        "    :param texts: either a list or numpy ndarray of strings\n",
        "    :tokenizer: an instantiated tokenizer\n",
        "    :is_training: whether input texts are from the training split or not\n",
        "    :max_seq_length: the max token sequence previously computed with\n",
        "    :pretrain: Converting dataset for training or dataset for RL\n",
        "    training texts.\n",
        "\n",
        "    :return\n",
        "        text_ids: a nested list on token indices\n",
        "        max_seq_length: the max token sequence previously computed with\n",
        "        training texts.\n",
        "    \"\"\"\n",
        "\n",
        "    text_ids = tokenizer.convert_tokens_to_ids(df)\n",
        "\n",
        "    # Padding\n",
        "    if is_training:\n",
        "        max_seq_length = int(np.quantile([len(seq) for seq in text_ids], 0.95))\n",
        "    else:\n",
        "        assert max_seq_length is not None\n",
        "\n",
        "    # Making sure that last token is [end]\n",
        "    if pretrain:\n",
        "      for i in range(len(text_ids)):\n",
        "        if len(text_ids[i]) < max_seq_length:\n",
        "          text_ids[i][-1] = tokenizer.vocab['[end]']\n",
        "        else:\n",
        "          text_ids[i][max_seq_length-1] = tokenizer.vocab['[end]']\n",
        "\n",
        "    text_ids = [seq + [0] * (max_seq_length - len(seq)) for seq in text_ids]\n",
        "    text_ids = np.array([seq[:max_seq_length] for seq in text_ids])\n",
        "\n",
        "    if is_training:\n",
        "        return text_ids, max_seq_length\n",
        "    else:\n",
        "        return text_ids\n",
        "\n",
        "\n",
        "def decode_sentence(input_sentence, preprocess):\n",
        "    # Mapping the input sentence to tokens and adding start and end tokens\n",
        "    tokenized_input_sentence = tokenizer.convert_tokens_to_ids(\n",
        "        [preprocess(input_sentence)]\n",
        "    )[0]\n",
        "    tokenized_input_sentence = tf.pad(\n",
        "        tokenized_input_sentence,\n",
        "        [[0, max_seq_length - tf.shape(tokenized_input_sentence)[0]]])\n",
        "    # Initializing the initial sentence consisting of only the start token.\n",
        "    tokenized_target_sentence = tf.expand_dims(tokenizer.vocab[\"[start]\"], 0)\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    for i in range(max_seq_length):\n",
        "        # Get the predictions\n",
        "        predictions = fnet.predict(\n",
        "            {\n",
        "                \"encoder_inputs\": tf.expand_dims(tokenized_input_sentence, 0),\n",
        "                \"decoder_inputs\": tf.expand_dims(\n",
        "                    tf.pad(\n",
        "                        tokenized_target_sentence,\n",
        "                        [[0, max_seq_length - tf.shape(tokenized_target_sentence)[0]]],\n",
        "                    ),\n",
        "                    0,\n",
        "                ),\n",
        "            }\n",
        "        )\n",
        "        # Calculating the token of step i (sentence len is i-1) with maximum probability and getting the corresponding word\n",
        "        sampled_token_index = tf.argmax(predictions[0, i, :])  # predictions.shape == (batch_size, max_seq_len, vocab_size)\n",
        "        if tf.equal(sampled_token_index, 0):\n",
        "            break\n",
        "        sampled_token = tokenizer.tokenizer.index_word[sampled_token_index.numpy()]\n",
        "        # If sampled token is the end token then stop generating and return the sentence\n",
        "        if tf.equal(sampled_token_index, tokenizer.vocab[\"[end]\"]):\n",
        "            break\n",
        "        decoded_sentence += sampled_token + \" \"\n",
        "        tokenized_target_sentence = tf.concat(\n",
        "            [tokenized_target_sentence, [sampled_token_index]], 0\n",
        "        )\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "Eu2t2HfLTIA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading "
      ],
      "metadata": {
        "id": "Spn1n_WYTIhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(train_dataset)\n",
        "df = df.drop([\"WA\", \"stance_WA\", \"stance_WA_conf\"], axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YAHZs6KbTyXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing"
      ],
      "metadata": {
        "id": "M_xjCBDUKm_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[2,\"argument\"] = \"zero tolerance policy in schools should not be adopted as circumstances are often not black and white, being more nuanced. no one should be written off due to a mistake of judgement.\"\n",
        "pretrain_series = df.apply(lambda row : preprocess_pretrain(row['argument']), axis = 1)\n",
        "\n",
        "rl_series = df.apply(lambda row : preprocess_rl(row['argument']), axis = 1)\n",
        "topic_list = list(df.apply(lambda row : row['topic'].lower(), axis = 1))\n",
        "pretrain_series.head()"
      ],
      "metadata": {
        "id": "bYwrsXZvUM3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train, test, val splits"
      ],
      "metadata": {
        "id": "8RK-KLKUUl5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "is_training_data =  df['set']=='train'\n",
        "is_validation_data =  df['set']=='dev'\n",
        "is_test_data =  df['set']=='test'\n",
        "\n",
        "x_train = pretrain_series[is_training_data]\n",
        "x_train = x_train.append(pretrain_series[is_validation_data])\n",
        "x_test  = pretrain_series[is_test_data]"
      ],
      "metadata": {
        "id": "BgkpkAnPUfm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwNEJrQoShm1"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load embeddings from glove\n",
        "if os.path.exists(embed_model):\n",
        "  with open(embed_model, \"rb\") as f:\n",
        "    embedding_model = pickle.load(f)\n",
        "else:\n",
        "  embedding_model = load_embedding_model(model_type=\"glove\", \n",
        "                                         embedding_dimension=EMBED_DIM)"
      ],
      "metadata": {
        "id": "ecxi9TNColzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating tokenizer and vocabulary\n",
        "\n",
        "tokenizer_args = {\n",
        "    'oov_token': \"OOV_TOKEN\",  # The vocabulary id for unknown terms during text conversion\n",
        "    'lower' : True,  # default\n",
        "    'filters' : '' \n",
        "}\n",
        "\n",
        "tokenizer = KerasTokenizer(tokenizer_args=tokenizer_args,\n",
        "                           build_embedding_matrix=True,\n",
        "                           embedding_dimension=EMBED_DIM,\n",
        "                           embedding_model_type=\"glove\", \n",
        "                           embedding_model=embedding_model)\n",
        "\n",
        "tokenizer.build_vocab(x_train)\n",
        "tokenizer.update_vocab(x_test)\n",
        "VOCAB_SIZE = len(tokenizer.vocab)\n",
        "\n",
        "tokenizer_info = tokenizer.get_info()\n",
        "\n",
        "print('Tokenizer info: ', tokenizer_info)"
      ],
      "metadata": {
        "id": "4DIT2jkkooWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD4GNbAvShm3"
      },
      "source": [
        "### Tokenizing and padding sentences using `TextVectorization`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tokens, max_seq_length = convert_text(x_train, tokenizer, True, pretrain=True)\n",
        "x_test_tokens = convert_text(x_test, tokenizer, max_seq_length=max_seq_length, pretrain=True)\n",
        "print(\"Max token sequence: {}\".format(max_seq_length))\n",
        "print('X train shape: ', x_train_tokens.shape)\n",
        "print('X test shape: ', x_test_tokens.shape)"
      ],
      "metadata": {
        "id": "yppEeUKlYwmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorflow Dataset for Pre-training"
      ],
      "metadata": {
        "id": "jbuI0HveZGHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_tokens, x_train_tokens))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_tokens, x_test_tokens))\n",
        "\n",
        "def vectorize_text(inputs, outputs):\n",
        "    # One extra padding token to the right to match the output shape\n",
        "    outputs = tf.pad(outputs, [[0, 1]])\n",
        "    return (\n",
        "        {\"encoder_inputs\": inputs, \"decoder_inputs\": outputs[:-1]},\n",
        "        {\"outputs\": outputs[1:]},\n",
        "    )\n",
        "\n",
        "train_dataset = train_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset = (\n",
        "    train_dataset.cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "test_dataset = test_dataset.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "tyzOlDBixOkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset for RL"
      ],
      "metadata": {
        "id": "pwJgKbOQVH1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = rl_series[is_training_data]\n",
        "x_val = rl_series[is_validation_data]\n",
        "x_test = rl_series[is_test_data]\n",
        "\n",
        "# -2 because we will add start and end\n",
        "x_train_rl = convert_text(x_train, tokenizer, max_seq_length=max_seq_length-2, pretrain=False)\n",
        "x_val_rl = convert_text(x_val, tokenizer, max_seq_length=max_seq_length-2, pretrain=False)\n",
        "x_test_rl= convert_text(x_test, tokenizer, max_seq_length=max_seq_length-2, pretrain=False)"
      ],
      "metadata": {
        "id": "3oQKDpztVKW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq Model"
      ],
      "metadata": {
        "id": "mI3q1UV-Q7Kp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp6jfLNIShm5"
      },
      "source": [
        "### Creating the FNet Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kylhhrZyShm5"
      },
      "outputs": [],
      "source": [
        "class FNetEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, **kwargs):\n",
        "        super(FNetEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Casting the inputs to complex64\n",
        "        inp_complex = tf.cast(inputs, tf.complex64)\n",
        "        # Projecting the inputs to the frequency domain using FFT2D and\n",
        "        # extracting the real part of the output\n",
        "        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n",
        "        proj_input = self.layernorm_1(inputs + fft)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHuNwBidShm6"
      },
      "source": [
        "### Creating the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class FNetDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(FNetDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(latent_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "        \n",
        "def create_actor(max_length):\n",
        "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n",
        "    x = PositionalEmbedding(max_length, VOCAB_SIZE+1, EMBED_DIM)(encoder_inputs)\n",
        "    encoder_outputs = FNetEncoder(EMBED_DIM, LATENT_DIM)(x)\n",
        "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"decoder_inputs\")\n",
        "    encoded_seq_inputs = keras.Input(\n",
        "        shape=(None, EMBED_DIM), name=\"decoder_state_inputs\"\n",
        "    )\n",
        "    x = PositionalEmbedding(max_length, VOCAB_SIZE+1, EMBED_DIM)(decoder_inputs)\n",
        "    x = FNetDecoder(EMBED_DIM, LATENT_DIM, NUM_HEADS)(x, encoded_seq_inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    decoder_outputs = layers.Dense(VOCAB_SIZE+1, activation=\"softmax\")(x)\n",
        "    decoder = keras.Model(\n",
        "        [decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"outputs\"\n",
        "    )\n",
        "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "    fnet = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"actor\")\n",
        "    return fnet\n",
        "\n",
        "\n",
        "def create_critic(max_length):\n",
        "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n",
        "    x = PositionalEmbedding(max_length, VOCAB_SIZE+1, EMBED_DIM)(encoder_inputs)\n",
        "    encoder_outputs = FNetEncoder(EMBED_DIM, LATENT_DIM)(x)\n",
        "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"decoder_inputs\")\n",
        "    encoded_seq_inputs = keras.Input(\n",
        "        shape=(None, EMBED_DIM), name=\"decoder_state_inputs\"\n",
        "    )\n",
        "    x = PositionalEmbedding(max_length, VOCAB_SIZE+1, EMBED_DIM)(decoder_inputs)\n",
        "    x = FNetDecoder(EMBED_DIM, LATENT_DIM, NUM_HEADS)(x, encoded_seq_inputs)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    decoder_outputs = layers.Dense(VOCAB_SIZE+1, activation=\"softmax\")(x)\n",
        "    decoder = keras.Model(\n",
        "        [decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"outputs\"\n",
        "    )\n",
        "    decoder_outputs_tmp = decoder([decoder_inputs, encoder_outputs])\n",
        "\n",
        "    action_input = keras.Input(shape=(VOCAB_SIZE+1), dtype=\"float32\", name=\"action_inputs\")\n",
        "    action_out = layers.Dense(16, activation=\"relu\")(action_input)\n",
        "\n",
        "    decoder_outputs = []\n",
        "    for i in tf.range(0, BATCH_SIZE_RL):\n",
        "      decoder_outputs.append(decoder_outputs_tmp[i, tf.math.count_nonzero(decoder_inputs[i], dtype='int32'), :])\n",
        "    decoder_outputs = tf.convert_to_tensor(decoder_outputs)\n",
        "\n",
        "    concat = layers.Concatenate()([decoder_outputs, action_out])\n",
        "    out = layers.Dense(1, activation=\"tanh\")(concat)\n",
        "    fnet = keras.Model([encoder_inputs, decoder_inputs, action_input], out, name=\"critic\")\n",
        "    return fnet"
      ],
      "metadata": {
        "id": "4gUArwKswdcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdrNwBmsShm7"
      },
      "source": [
        "### Creating the seq2seq model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN58kiEwShm7"
      },
      "outputs": [],
      "source": [
        "fnet = create_actor(max_seq_length)\n",
        "fnet.compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model"
      ],
      "metadata": {
        "id": "elDPc9Vpkoa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnet.load_weights(\"/gdrive/MyDrive/NLP/pretrained_fnet.h5\")"
      ],
      "metadata": {
        "id": "NRHobre2krAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-Training"
      ],
      "metadata": {
        "id": "o0xDfmLTk1P5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA1UHp5gShm7"
      },
      "outputs": [],
      "source": [
        "start_pretraining = False  # Added for security against \"run all\"\n",
        "if start_pretraining:\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3)\n",
        "  history = fnet.fit(train_dataset, epochs=90, validation_data=test_dataset, \n",
        "                    callbacks=[callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trqSKiU0Shm8"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WfVMhSbShm8"
      },
      "outputs": [],
      "source": [
        "sentence = \"marriage isn't keeping up with the times. abandon the old thinking and bring something that incorporates all unions not just those with a man and woman.\"\n",
        "out = decode_sentence(sentence, preprocess_pretrain)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model"
      ],
      "metadata": {
        "id": "h9TpGtgfvXW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_pretraining = False  # Added for security against \"run all\"\n",
        "if start_pretraining:\n",
        "  fnet.save_weights(\"/gdrive/MyDrive/NLP/pretrained_fnet.h5\")"
      ],
      "metadata": {
        "id": "iLX_SomHvd5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Bert](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb)"
      ],
      "metadata": {
        "id": "g-Bn64skKOWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model to fine-tune"
      ],
      "metadata": {
        "id": "_1GvRSj9QpgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_name_quality = 'bert_en_uncased_L-12_H-768_A-12'\n",
        "\n",
        "tfhub_handle_encoder_quality = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n",
        "tfhub_handle_preprocess_quality = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "\n",
        "bert_model_name_r1 = 'albert_en_base'\n",
        "\n",
        "tfhub_handle_encoder_r1 = 'https://tfhub.dev/tensorflow/albert_en_base/3'\n",
        "tfhub_handle_preprocess_r1 = 'https://tfhub.dev/tensorflow/albert_en_preprocess/3'\n",
        "\n",
        "print(f'Model name quality                       : {bert_model_name_quality}')\n",
        "print(f'BERT model selected quality              : {tfhub_handle_encoder_quality}')\n",
        "print(f'Preprocess model auto-selected quality   : {tfhub_handle_preprocess_quality}')\n",
        "print(f'Model name r1                            : {bert_model_name_r1}')\n",
        "print(f'BERT model selected r1                   : {tfhub_handle_encoder_r1}')\n",
        "print(f'Preprocess model auto-selected r1        : {tfhub_handle_preprocess_r1}')"
      ],
      "metadata": {
        "id": "FNHbHJK-KPiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model(dense_size=100):\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess_quality, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder_quality, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dense(dense_size, activation=keras.activations.relu, name='fc')(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=keras.activations.sigmoid, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "8uH8ohE9KUcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model_r1(dense_size=100):  # model used to compute the score of the topic\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess_r1, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder_r1, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dense(dense_size, activation=keras.activations.relu, name='fc_1')(net)\n",
        "  net = tf.keras.layers.Dense(3, activation=keras.activations.softmax, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "inBOy7ttZHKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_quality = build_classifier_model()\n",
        "bert_model_r1 = build_classifier_model_r1()"
      ],
      "metadata": {
        "id": "CZlbTvKXKXHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_quality.summary()"
      ],
      "metadata": {
        "id": "k01nAznZKYrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load best model R1"
      ],
      "metadata": {
        "id": "jjHmM4jBQsh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_quality.load_weights(quality_model_weights)"
      ],
      "metadata": {
        "id": "4Hl6UOAKKabL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load best model Topic"
      ],
      "metadata": {
        "id": "-5YyLKBoXgo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_r1.load_weights(r1_model_weights)"
      ],
      "metadata": {
        "id": "PuOixaWzXgBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "-dF73pIvSKQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RL Stuff"
      ],
      "metadata": {
        "id": "tOHI59Dqq90b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Noise"
      ],
      "metadata": {
        "id": "6Y8q8yY4foJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        )\n",
        "        # Store x into x_prev\n",
        "        # Makes next noise dependent on current one\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)"
      ],
      "metadata": {
        "id": "TxIZiVo66Wed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Buffer"
      ],
      "metadata": {
        "id": "LCjf3ZHuflyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=32, num_epochs=1, \n",
        "                 train_buffer=100000):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Number of epochs\n",
        "        self.num_epochs = num_epochs\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "        # Num of obs to use in epochs\n",
        "        self.train_buffer = train_buffer\n",
        "        assert self.train_buffer <= self.buffer_capacity\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # It tells us num of episode recorder\n",
        "        self.buffer_used = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, 2, max_seq_length))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, VOCAB_SIZE+1))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, max_seq_length))\n",
        "        self.dones = np.zeros((self.buffer_capacity))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "        self.dones[index] = 1 - obs_tuple[4]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "        if self.buffer_used < len(self.state_buffer):\n",
        "          self.buffer_used += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tf.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_new_batch, dones,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        start_batch = state_batch[:, 0, :]\n",
        "        new_batch = state_batch[:, 1, :]\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions_tmp = actor_model([start_batch, new_batch], training=True)\n",
        "            # Removing dimension 1 from output\n",
        "            actions = []\n",
        "            for i in range(actions_tmp.shape[0]):\n",
        "              actions.append(actions_tmp[i, tf.math.count_nonzero(new_batch[i])-1, :])\n",
        "            actions = tf.convert_to_tensor(actions)\n",
        "            critic_value = critic_model([start_batch, new_batch, actions], training=True)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        # tf.print(\"Actor Loss: \", actor_loss)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions_tmp = target_actor([start_batch, next_new_batch], training=True)\n",
        "            target_actions = []\n",
        "            # Removing dimension 1 from output\n",
        "            for i in range(target_actions_tmp.shape[0]):\n",
        "              target_actions.append(target_actions_tmp[i, tf.math.count_nonzero(next_new_batch[i]), :])\n",
        "            target_actions = tf.convert_to_tensor(target_actions)\n",
        "            y = reward_batch + dones * gamma * target_critic([start_batch, \n",
        "                                                                    next_new_batch, \n",
        "                                                                    target_actions], \n",
        "                                                                   training=True)\n",
        "            critic_value = critic_model([start_batch, new_batch, action_batch], training=True)\n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        # tf.print(\"Critic Loss: \", critic_loss)\n",
        "\n",
        "        return actor_loss, critic_loss\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        size_memory = self.buffer_used\n",
        "        # Random permutation of indices of dataset\n",
        "        obs_perm = np.random.permutation(size_memory)\n",
        "        # Get sampling range\n",
        "        record_range = range(0, min(size_memory, self.train_buffer))\n",
        "        actor_loss, critic_loss = [], []\n",
        "        for i in range(self.num_epochs):\n",
        "          steps = 0\n",
        "          while len(record_range) <= size_memory and self.batch_size <= len(record_range):\n",
        "            # Randomly sample indices\n",
        "            batch_indices = np.random.choice(record_range, self.batch_size, replace=False)\n",
        "            # Convert to tensors\n",
        "            state_batch = tf.convert_to_tensor(self.state_buffer[obs_perm[batch_indices]])\n",
        "            action_batch = tf.convert_to_tensor(self.action_buffer[obs_perm[batch_indices]])\n",
        "            reward_batch = tf.convert_to_tensor(self.reward_buffer[obs_perm[batch_indices]])\n",
        "            reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
        "            next_state_batch = tf.convert_to_tensor(self.next_state_buffer[obs_perm[batch_indices]])\n",
        "            dones = tf.convert_to_tensor(self.dones[obs_perm[batch_indices]])\n",
        "            dones = tf.cast(dones, dtype=tf.float32)\n",
        "\n",
        "            actor_loss_tmp, critic_loss_tmp = self.update(state_batch, action_batch, \n",
        "                                                          reward_batch, next_state_batch, \n",
        "                                                          dones)\n",
        "            actor_loss.append(actor_loss_tmp)\n",
        "            critic_loss.append(critic_loss_tmp)\n",
        "            steps += 1\n",
        "\n",
        "            # Update sampling range\n",
        "            record_range = [x for x in record_range if x not in batch_indices]\n",
        "            print()\n",
        "          actor_loss[-1] /= steps\n",
        "          critic_loss[-1] /= steps\n",
        "        return np.array(actor_loss).mean(), np.array(critic_loss).mean()\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tf.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))"
      ],
      "metadata": {
        "id": "3aO_DrBq5p-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy"
      ],
      "metadata": {
        "id": "1rQmP0sOft_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy(state, noise_object, step):\n",
        "    sampled_actions = tf.squeeze(actor_model(state))\n",
        "    if step >= BATCH_SIZE_RL:\n",
        "      noise = noise_object()\n",
        "      # Adding noise to action\n",
        "      sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    # We make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, 0.0, 1.0)\n",
        "\n",
        "    return np.array([np.squeeze(legal_action)])"
      ],
      "metadata": {
        "id": "RmOM2fho8A6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentences_bert(sentence):\n",
        "  sentence = list(sentence) + [0] * (max_seq_length - len(sentence))\n",
        "  sentence = np.array(sentence[:max_seq_length])\n",
        "  sentence = \" \".join(tokenizer.convert_ids_to_tokens([sentence]))\n",
        "  return sentence\n",
        "\n",
        "\n",
        "if os.path.exists(start_scores):\n",
        "  with open(start_scores, \"rb\") as f:\n",
        "    score_predictions = pickle.load(f)\n",
        "else:\n",
        "  score_predictions = bert_model_quality.predict(np.array([sentences_bert(x) for x in x_train_rl]), batch_size=256, verbose=1)\n",
        "  with open(start_scores, \"wb\") as f:\n",
        "    pickle.dump(score_predictions, f)"
      ],
      "metadata": {
        "id": "WARjKfhpT1Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment"
      ],
      "metadata": {
        "id": "QeFp6ABvfxcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def r4_reward(sentence):\n",
        "  sentence_w0 = [i for i in sentence if i!=0]  # remove the zeros in the list\n",
        "  unique_tokens = np.unique(np.array(sentence_w0))  # scroll the list according to the unique elements, in such a way I do not need to remove each elem everytime\n",
        "  n_repetead_tokens = 0\n",
        "\n",
        "  for elem in unique_tokens:\n",
        "    repetitions = sentence_w0.count(elem)  # count the repetitions for the elem\n",
        "\n",
        "    if repetitions > 1:  # if I have more than 1 repetitions\n",
        "      n_repetead_tokens += 1  # increase the counter\n",
        "  \n",
        "  r4 = 1 - (n_repetead_tokens/len(unique_tokens))\n",
        "  return r4"
      ],
      "metadata": {
        "id": "K46xsGv93TiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Env:\n",
        "  def __init__(self, tokenizer, max_seq_length, train_dataset, train_dataset_topics, \n",
        "               evaluator_quality, evaluator_r1, score_predictions):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.start_sentence = random.choice(train_dataset)\n",
        "    self.train_dataset = train_dataset\n",
        "    self.prev_sentence = []\n",
        "    self.evaluator_quality = evaluator_quality\n",
        "    self.evaluator_r1 = evaluator_r1\n",
        "    self.train_dataset = [list(self.train_dataset)]\n",
        "    self.train_dataset.append([])\n",
        "    self.train_dataset[1] = score_predictions\n",
        "    self.train_dataset_topics = train_dataset_topics\n",
        "      \n",
        "  def get_reward(self, new_sentence, done):\n",
        "    # Padding new sentence\n",
        "    new_sentence = new_sentence[1:-1]\n",
        "    sentence_r = new_sentence + [0] * (self.max_seq_length - len(new_sentence))\n",
        "    sentence_r = np.array(sentence_r[:self.max_seq_length])\n",
        "\n",
        "    if np.count_nonzero(sentence_r) >= 1:\n",
        "      bert_sentence = \" \".join(tokenizer.convert_ids_to_tokens([sentence_r]))\n",
        "      new_score = self.evaluator_quality(np.array([bert_sentence]))\n",
        "      r_quality = new_score - self.score_start_sentence\n",
        "      topic_sentence = bert_sentence + \". \" + self.topic\n",
        "      r1_scores = self.evaluator_r1(np.array([topic_sentence]))[0]\n",
        "      r1 = r1_scores[0] - 1.0 * r1_scores[1]\n",
        "\n",
        "      r4 = r4_reward(sentence_r)\n",
        "      if done:\n",
        "          print(0.30 * r_quality, \" \", 0.40 * r4, \" \", 0.30 * r1)\n",
        "      reward = 0.30 * r_quality + 0.40 * r4 + 0.30 * r1\n",
        "    else:\n",
        "      reward = tf.constant([[-1.0]])\n",
        "\n",
        "    return reward\n",
        "\n",
        "  def step(self, action, step):\n",
        "    buff_action = action[0, step, :]\n",
        "    new_sentence = self.prev_sentence + [tf.argmax(buff_action)]\n",
        "\n",
        "    done = False\n",
        "    if new_sentence[-1] == self.tokenizer.vocab[\"[end]\"] or len(new_sentence) >= self.max_seq_length:\n",
        "      done = True\n",
        "    # Padding new sentence\n",
        "    print(new_sentence)\n",
        "    \n",
        "    sentence_r = new_sentence[1:-1]\n",
        "    print(sentence_r)\n",
        "    sentence_r = sentence_r + [0] * (self.max_seq_length - len(sentence_r))\n",
        "    sentence_r = np.array(sentence_r[:self.max_seq_length])\n",
        "    \n",
        "    self.prev_sentence = new_sentence\n",
        "\n",
        "    return sentence_r, buff_action, done\n",
        "\n",
        "  def reset(self):\n",
        "    id = random.randint(0, len(self.train_dataset[0]))\n",
        "    self.start_sentence = self.train_dataset[0][id]\n",
        "    self.topic = self.train_dataset_topics[id]\n",
        "    self.score_start_sentence = self.train_dataset[1][id]\n",
        "    self.prev_sentence = [self.tokenizer.vocab[\"[start]\"]]\n",
        "\n",
        "    sentence_r = self.prev_sentence + [0] * (self.max_seq_length - len(self.prev_sentence))\n",
        "    sentence_r = np.array(sentence_r[:self.max_seq_length])\n",
        "\n",
        "    start_sentence = [self.tokenizer.vocab[\"[start]\"]] \n",
        "    start_sentence.extend(self.start_sentence)\n",
        "    start_sentence.extend([self.tokenizer.vocab[\"[end]\"]])\n",
        "\n",
        "    return start_sentence, sentence_r"
      ],
      "metadata": {
        "id": "NFn8mbbrCf0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "RekHwaR1fzab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "std_dev = 0.5\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
        "\n",
        "actor_model = fnet\n",
        "critic_model = create_critic(max_seq_length)\n",
        "\n",
        "for i in range(len(actor_model.layers)):\n",
        "  for j in range(len(critic_model.layers)):\n",
        "    if actor_model.layers[i].name==critic_model.layers[j].name:\n",
        "      critic_model.layers[j].set_weights(actor_model.layers[i].get_weights())\n",
        "\n",
        "target_actor = create_actor(max_seq_length)\n",
        "target_critic = create_critic(max_seq_length)\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.003\n",
        "actor_lr = 0.003\n",
        "\n",
        "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "# Train frequency in episodes\n",
        "train_freq = 3\n",
        "# Plot frequency in episodes\n",
        "plot_freq = 50\n",
        "# Number of episodes\n",
        "total_episodes = 10000\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "\n",
        "buffer = Buffer(BUFFER_SIZE, BATCH_SIZE_RL, 1, TRAIN_BUFFER)"
      ],
      "metadata": {
        "id": "MrOq7ibm8F3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "# To store average actor loss\n",
        "avg_actor_list = []\n",
        "# To store average critic loss\n",
        "avg_critic_list = []\n",
        "\n",
        "env = Env(tokenizer, max_seq_length, x_train_rl, topic_list, bert_model_quality, \n",
        "          bert_model_r1, score_predictions)\n",
        "\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    start_sentence, prev_sentence = env.reset()\n",
        "    tf_prev_state_1 = tf.expand_dims(tf.convert_to_tensor(start_sentence), 0)\n",
        "    tf_prev_state_2 = tf.expand_dims(tf.convert_to_tensor(prev_sentence), 0)\n",
        "\n",
        "    prev_state_tr = (tf_prev_state_1, tf_prev_state_2)\n",
        "    prev_state_sv = (start_sentence, prev_sentence)\n",
        "\n",
        "    step = 0\n",
        "    ep_reward = 0.0\n",
        "\n",
        "    while True:\n",
        "        action = policy(prev_state_tr, ou_noise, step)\n",
        "        # Receive state from environment.\n",
        "        new_sentence, buff_action, done = env.step(action, step)\n",
        "        tf_prev_state_2 = tf.expand_dims(tf.convert_to_tensor(new_sentence), 0)\n",
        "        state_tr = (tf_prev_state_1, tf_prev_state_2)\n",
        "        state_sv = (start_sentence, new_sentence)\n",
        "\n",
        "        # Getting reward\n",
        "        reward_tmp = env.get_reward(env.prev_sentence, done)\n",
        "        ep_reward += reward_tmp\n",
        "        buffer.record((prev_state_sv, buff_action, reward_tmp, state_sv[1], 1 if done else 0))\n",
        "        \n",
        "        # End this episode when `done` is True\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        prev_state_tr = state_tr\n",
        "        prev_state_sv = state_sv\n",
        "\n",
        "        step += 1\n",
        "\n",
        "    ep_reward /= step\n",
        "    ep_reward_list.append(ep_reward)\n",
        "\n",
        "    print([tokenizer.tokenizer.index_word[x] for x in start_sentence if x != 0])\n",
        "    print([tokenizer.tokenizer.index_word[x] for x in new_sentence[1:-1] if x != 0])\n",
        "\n",
        "    # Mean of last 10 episodes\n",
        "    avg_reward = np.mean(ep_reward_list[-10:])\n",
        "    print(\"Buffer size: \", buffer.buffer_used)\n",
        "    print(\"Episode * {} * Avg Reward is ==> {} * Episode Reward is ==> {}\".format(ep, avg_reward, ep_reward))\n",
        "    print()\n",
        "    avg_reward_list.append(avg_reward)\n",
        "\n",
        "    # Training\n",
        "    if ep % train_freq == 0 and ep != 0 and buffer.buffer_counter >= 1.5 * TRAIN_BUFFER:\n",
        "      print(\"Training\")\n",
        "      actor_loss_tmp, critic_loss_tmp = buffer.learn()\n",
        "      avg_actor_list.append(actor_loss_tmp)\n",
        "      avg_critic_list.append(critic_loss_tmp)\n",
        "      update_target(target_actor.variables, actor_model.variables, tau)\n",
        "      update_target(target_critic.variables, critic_model.variables, tau)\n",
        "\n",
        "    if ep != 0 and ep % plot_freq == 0:\n",
        "      # Plotting graph\n",
        "      # Episodes versus Avg. Rewards\n",
        "      plt.plot(avg_reward_list)\n",
        "      plt.xlabel(\"Episode\")\n",
        "      plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "      plt.show()\n",
        "      # Plotting graph\n",
        "      # Episodes versus Avg. Actor loss\n",
        "      plt.plot(avg_actor_list)\n",
        "      plt.xlabel(\"Episode\")\n",
        "      plt.ylabel(\"Avg. Actor Loss\")\n",
        "      plt.show()\n",
        "      # Plotting graph\n",
        "      # Episodes versus Avg. Critic loss\n",
        "      plt.plot(avg_critic_list)\n",
        "      plt.xlabel(\"Episode\")\n",
        "      plt.ylabel(\"Avg. Critic Loss\")\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "wtqJ-YFG8IyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model"
      ],
      "metadata": {
        "id": "t5q7qCv4vP59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert False  # Make sure to avoid deleting other saved models\n",
        "fnet.save_weights(\"/gdrive/MyDrive/NLP/final_fnet.h5\")"
      ],
      "metadata": {
        "id": "fdh92KOZvULc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "0TA02AdRk-ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnet.load_weights(\"/gdrive/MyDrive/NLP/final_fnet.h5\")"
      ],
      "metadata": {
        "id": "c3EZOYd2k-wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "tMBTpBSySLOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"marriage isn't keeping up with the times. abandon the old thinking and bring something that incorporates all unions not just those with a man and woman.\"\n",
        "out = decode_sentence(sentence, preprocess_rl)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "KX14tD14crA6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "seq2seq_arg_quality.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
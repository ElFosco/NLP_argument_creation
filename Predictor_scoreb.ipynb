{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElFosco/NLP_argument_creation/blob/Fosco/Predictor_scoreb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "SWUoqJbhZUaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --force-reinstall numpy==1.20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "Nbfi7mcor0cQ",
        "outputId": "99bfdd09-2388-42e9-c085-f91b335e21da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.20\n",
            "  Using cached numpy-1.20.0-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.20.0\n",
            "    Uninstalling numpy-1.20.0:\n",
            "      Successfully uninstalled numpy-1.20.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.20.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iHiGJ5Ou8Mwo"
      },
      "outputs": [],
      "source": [
        "!pip install -q tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N-VGaZQA8ANi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "162d4551-0218-43d8-b6ac-27218b8cbf28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (2.8.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.23.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.43.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.20.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.0.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.13.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (13.0.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.10.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrxfQlV-7lbr"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EmPHpfi8AkgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e231f1-6491-4f14-bd29-6f08d53471f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE_to76fAVf7"
      },
      "source": [
        "#Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wn6dyvb0AzdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "059f84c8-697f-49fe-abb6-fa3c1736d563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Using google drive to upload the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# dir_path = \"drive/MyDrive/NLP_project/Datasets/\"\n",
        "dir_path = \"drive/MyDrive/Magistrale/NLP/Project/Data/\"\n",
        "dataset = \"arg_quality_rank_30k.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ca1VdgDoAURT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "fcbc1f52-871b-4ba3-b297-411d49b02660"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c9b56ddd-1013-47e3-8030-c67cec72aa24\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>argument</th>\n",
              "      <th>topic</th>\n",
              "      <th>set</th>\n",
              "      <th>WA</th>\n",
              "      <th>MACE-P</th>\n",
              "      <th>stance_WA</th>\n",
              "      <th>stance_WA_conf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"marriage\" isn't keeping up with the times.  a...</td>\n",
              "      <td>We should abandon marriage</td>\n",
              "      <td>train</td>\n",
              "      <td>0.846165</td>\n",
              "      <td>0.297659</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>.a multi-party system would be too confusing a...</td>\n",
              "      <td>We should adopt a multi-party system</td>\n",
              "      <td>train</td>\n",
              "      <td>0.891271</td>\n",
              "      <td>0.726133</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\ero-tolerance policy in schools should not be...</td>\n",
              "      <td>We should adopt a zero-tolerance policy in sch...</td>\n",
              "      <td>dev</td>\n",
              "      <td>0.721192</td>\n",
              "      <td>0.396953</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>`people reach their limit when it comes to the...</td>\n",
              "      <td>Assisted suicide should be a criminal offence</td>\n",
              "      <td>train</td>\n",
              "      <td>0.730395</td>\n",
              "      <td>0.225212</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100% agree, should they do that, it would be a...</td>\n",
              "      <td>We should abolish safe spaces</td>\n",
              "      <td>train</td>\n",
              "      <td>0.236686</td>\n",
              "      <td>0.004104</td>\n",
              "      <td>1</td>\n",
              "      <td>0.805517</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9b56ddd-1013-47e3-8030-c67cec72aa24')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c9b56ddd-1013-47e3-8030-c67cec72aa24 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c9b56ddd-1013-47e3-8030-c67cec72aa24');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            argument  ... stance_WA_conf\n",
              "0  \"marriage\" isn't keeping up with the times.  a...  ...       1.000000\n",
              "1  .a multi-party system would be too confusing a...  ...       1.000000\n",
              "2  \\ero-tolerance policy in schools should not be...  ...       1.000000\n",
              "3  `people reach their limit when it comes to the...  ...       1.000000\n",
              "4  100% agree, should they do that, it would be a...  ...       0.805517\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df = pd.read_csv(dir_path + dataset)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9HC-FCfELNYo"
      },
      "outputs": [],
      "source": [
        "set_topic = df.topic.unique()\n",
        "dict_topic = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TKs69SnCMKff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9e054a-45da-4bf9-b8bb-08172c2ad4ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We should fight for the abolition of nuclear weapons', 554),\n",
              " ('We should legalize cannabis', 548),\n",
              " ('We should ban naturopathy', 540),\n",
              " ('Foster care brings more harm than good', 538),\n",
              " ('Blockade of the Gaza Strip should be ended', 521),\n",
              " ('We should legalize prostitution', 504),\n",
              " ('We should ban cosmetic surgery for minors', 502),\n",
              " ('We should legalize polygamy', 500),\n",
              " ('We should abolish the three-strikes laws', 499),\n",
              " ('We should end mandatory retirement', 484),\n",
              " ('We should abandon the use of school uniform', 480),\n",
              " ('Intelligence tests bring more harm than good', 472),\n",
              " ('We should abolish capital punishment', 470),\n",
              " ('Holocaust denial should be a criminal offence', 466),\n",
              " ('We should adopt a zero-tolerance policy in schools', 459),\n",
              " ('We should end affirmative action', 456),\n",
              " ('We should oppose collectivism', 454),\n",
              " ('We should close Guantanamo Bay detention camp', 447),\n",
              " ('Payday loans should be banned', 446),\n",
              " ('We should stop the development of autonomous cars', 446),\n",
              " ('We should ban telemarketing', 438),\n",
              " ('We should ban missionary work', 438),\n",
              " ('We should ban the use of child actors', 436),\n",
              " ('Surrogacy should be banned', 435),\n",
              " ('We should abolish intellectual property rights', 432),\n",
              " ('The vow of celibacy should be abandoned', 432),\n",
              " ('We should subsidize vocational education', 431),\n",
              " ('We should ban cosmetic surgery', 430),\n",
              " ('We should prohibit flag burning', 428),\n",
              " ('We should ban whaling', 428),\n",
              " ('The use of public defenders should be mandatory', 427),\n",
              " ('We should prohibit school prayer', 427),\n",
              " ('We should adopt an austerity regime', 424),\n",
              " ('We should ban human cloning', 423),\n",
              " ('We should ban fast food', 422),\n",
              " ('We should abandon marriage', 421),\n",
              " ('We should end racial profiling', 418),\n",
              " ('We should abolish the right to keep and bear arms', 415),\n",
              " ('We should ban factory farming', 414),\n",
              " ('We should ban targeted killing', 414),\n",
              " ('We should legalize organ trade', 413),\n",
              " ('We should abolish the Olympic Games', 410),\n",
              " ('We should ban the Church of Scientology', 407),\n",
              " ('We should introduce compulsory voting', 406),\n",
              " ('Entrapment should be legalized', 406),\n",
              " ('We should legalize sex selection', 405),\n",
              " ('We should fight urbanization', 405),\n",
              " ('We should subsidize stay-at-home dads', 405),\n",
              " ('We should abolish safe spaces', 404),\n",
              " ('We should adopt a multi-party system', 402),\n",
              " ('We should limit judicial activism', 402),\n",
              " ('We should adopt libertarianism', 400),\n",
              " ('We should subsidize Wikipedia', 400),\n",
              " ('We should end the use of economic sanctions', 400),\n",
              " ('We should subsidize embryonic stem cell research', 400),\n",
              " ('We should cancel pride parades', 399),\n",
              " ('We should ban private military companies', 397),\n",
              " ('Assisted suicide should be a criminal offence', 396),\n",
              " ('Homeschooling should be banned', 395),\n",
              " ('We should abolish zoos', 395),\n",
              " ('We should ban algorithmic trading', 390),\n",
              " ('We should abandon television', 389),\n",
              " ('We should subsidize space exploration', 388),\n",
              " ('We should subsidize student loans', 386),\n",
              " ('We should adopt gender-neutral language', 386),\n",
              " ('We should limit executive compensation', 381),\n",
              " ('We should adopt atheism', 379),\n",
              " ('We should prohibit women in combat', 375),\n",
              " ('We should subsidize journalism', 368),\n",
              " ('Homeopathy brings more harm than good', 358),\n",
              " ('Social media brings more harm than good', 331)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "for i in set_topic:\n",
        "  dict_topic[i] = df.loc[i==df['topic'], 'topic'].values.size\n",
        "sorted(dict_topic.items(), key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV1gnJwQNIZd"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "  text = re.sub('\\\"|-|\\\\\\\\|`', ' ', text)  # delete this chars from the string [\"-\\`]\n",
        "  text = re.sub('\\n', ' ', text)\n",
        "  text = re.sub('^[.]+', '', text)         # delete dots at the beginning of the sentence\n",
        "  text = re.sub(\"([?.!,])\", r\" \\1 \", text)\n",
        "  text = re.sub('\\. \\.', '.', text)        # delete . .\n",
        "  text = re.sub('&', ' and ', text)        # replace & with and\n",
        "  text = re.sub(' +', ' ', text)           # delete additional whitespace\n",
        "  text = text.rstrip()                  \n",
        "  text = text.lstrip()\n",
        "  text = \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n",
        "  return text"
      ],
      "metadata": {
        "id": "nBRUbfFqwIyu"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[2, \"argument\"] = \"zero tolerance policy in schools should not be adopted as circumstances are often not black and white, being more nuanced. no one should be written off due to a mistake of judgement.\"\n",
        "df['argument'] = df.apply(lambda row : clean_text(row['argument']), axis = 1)\n"
      ],
      "metadata": {
        "id": "J6314Kqgy-bl"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['argument']"
      ],
      "metadata": {
        "id": "Ck4uQ4rOEmSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c76ef5-0f40-4747-98f5-73d49bb92417"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        marriage isn't keeping up with the time . aban...\n",
              "1        a multi party system would be too confusing an...\n",
              "2        zero tolerance policy in school should not be ...\n",
              "3        people reach their limit when it come to their...\n",
              "4        100% agree , should they do that , it would be...\n",
              "                               ...                        \n",
              "30492    zoo trap animal into a meaningless life only t...\n",
              "30493    zoo treat animal badly they should be closed i...\n",
              "30494     zoo unfairly imprison animal and cause them harm\n",
              "30495    Zoos work a educational center and are not the...\n",
              "30496             zoo work to help breed endangered specie\n",
              "Name: argument, Length: 30497, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQfS-UzU6QXS"
      },
      "source": [
        "##Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "A82m4_1K5pfv"
      },
      "outputs": [],
      "source": [
        "is_training_data =  df['set']=='train'\n",
        "is_validation_data =  df['set']=='dev'\n",
        "is_test_data =  df['set']=='test'\n",
        "\n",
        "training_data = df[is_training_data]\n",
        "validation_data = df[is_validation_data]\n",
        "test_data  = df[is_test_data ]\n",
        "\n",
        "x_train = training_data['argument']\n",
        "Y_train = training_data['MACE-P']\n",
        "\n",
        "x_val = validation_data['argument']\n",
        "Y_val = validation_data['MACE-P']\n",
        "\n",
        "x_test = test_data['argument']\n",
        "Y_test = test_data['MACE-P']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data from UKP"
      ],
      "metadata": {
        "id": "yGfxf4XmwyT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ukp_path = dir_path + \"UKPConvArg1-Ranking-CSV/\" # need to download https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2427 in the given directory\n",
        "i=0\n",
        "ukp_dataset_train = pd.DataFrame()\n",
        "ukp_dataset_valid = pd.DataFrame()\n",
        "ukp_dataset_test = pd.DataFrame()\n",
        "for csv in os.listdir(ukp_path):\n",
        "  if i<=20:\n",
        "    df = pd.read_csv(ukp_path+csv,sep='\\t')\n",
        "    ukp_dataset_train = ukp_dataset_train.append(df)\n",
        "  elif 20<i<=25 :\n",
        "    df = pd.read_csv(ukp_path+csv,sep='\\t')\n",
        "    ukp_dataset_valid = ukp_dataset_valid.append(df)\n",
        "  else:\n",
        "    df = pd.read_csv(ukp_path+csv,sep='\\t')\n",
        "    ukp_dataset_test = ukp_dataset_test.append(df)\n",
        "  i+=1\n",
        "\n",
        "print(ukp_dataset_train.shape)\n",
        "print(ukp_dataset_valid.shape)\n",
        "print(ukp_dataset_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyCbglAfI8Ba",
        "outputId": "91ae7fc2-21ae-4f60-a9b3-80f93b4ff59c"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(702, 3)\n",
            "(160, 3)\n",
            "(190, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text_ukp(text):\n",
        "  text = re.sub('\\\"|-|\\\\\\\\|`|/|\\'', ' ', text)  # delete this chars from the string [\"-\\`]\n",
        "  text = re.sub('<br/>', ' ', text)\n",
        "  text = re.sub(':\\)', ' ', text)\n",
        "  text = re.sub('[\\.]+[\\.]+', ' ', text)         # delete ...\n",
        "  #text = re.sub(\"([?.!,])\", r\" \\1 \", text)\n",
        "  text = re.sub('&', ' and ', text)        # replace & with and\n",
        "  text = re.sub(' +', ' ', text)           # delete additional whitespace\n",
        "  text = text.rstrip()                  \n",
        "  text = text.lstrip()\n",
        "  text = \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n",
        "  return text\n",
        "  "
      ],
      "metadata": {
        "id": "KWI35htq-wdX"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ukp_dataset_train['argument'] = ukp_dataset_train.apply(lambda row : clean_text_ukp(row['argument']), axis = 1)\n",
        "ukp_dataset_valid['argument'] = ukp_dataset_valid.apply(lambda row : clean_text_ukp(row['argument']), axis = 1)\n",
        "ukp_dataset_test['argument'] = ukp_dataset_test.apply(lambda row : clean_text_ukp(row['argument']), axis = 1)\n",
        "\n",
        "x_train_ukp = ukp_dataset_train['argument']\n",
        "Y_train_ukp = ukp_dataset_train['rank']\n",
        "\n",
        "x_val_ukp = ukp_dataset_valid['argument']\n",
        "Y_val_ukp = ukp_dataset_valid['rank']\n",
        "\n",
        "x_test_ukp = ukp_dataset_test['argument']\n",
        "Y_test_ukp = ukp_dataset_test['rank']"
      ],
      "metadata": {
        "id": "Omw-W9EyHIpP"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Dataset"
      ],
      "metadata": {
        "id": "yZOP4Qz9NhBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.append(x_train_ukp)\n",
        "Y_train = Y_train.append(Y_train_ukp)\n",
        "\n",
        "x_val = x_val.append(x_val_ukp)\n",
        "Y_val = Y_val.append(Y_val_ukp)\n",
        "\n",
        "x_test = x_test.append(x_test_ukp)\n",
        "Y_test = Y_test.append(Y_test_ukp)"
      ],
      "metadata": {
        "id": "gJtlb4EsNgrH"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6PDqt2Kfz_T"
      },
      "source": [
        "#[Bert](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "y8_ctG55-uTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f1b22a8-c215-42f0-96f4-3d5df131232f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ],
      "source": [
        "# @title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'  # @param [\"bert_en_uncased_L-24_H-1024_A-16\",\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "Gzql3TDrBtg-"
      },
      "outputs": [],
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "Pduw5iP9Dgz_"
      },
      "outputs": [],
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "wdhOVzGzt1uc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d42df25-8e31-4ae5-e68a-b86e3d5fedfb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
              "array([[-1.94766521e-02, -1.63640231e-01,  3.24933901e-02, ...,\n",
              "        -3.22463185e-01, -1.03215784e-01,  2.76807010e-01],\n",
              "       [-3.08533795e-02, -3.36306602e-01, -1.65708363e-04, ...,\n",
              "        -4.87345725e-01,  5.70318043e-01,  4.68657613e-01]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "def get_sentence_embeding(sentences):\n",
        "    preprocessed_text = bert_preprocess_model(sentences)\n",
        "    return bert_model(preprocessed_text)['encoder_outputs'][-1][:,0,:]\n",
        "\n",
        "get_sentence_embeding([\n",
        "    \"500$ discount. hurry up\", \n",
        "    \"Bhavin, are you up for a volleybal game tomorrow?\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "vWN6VYotHCw9"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model(dense_size=100):\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dense(dense_size, activation=keras.activations.relu, name='fc')(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=keras.activations.sigmoid, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "-_pX1FzsK2uj"
      },
      "outputs": [],
      "source": [
        "classifier_model = build_classifier_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "mAijDE-crAt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49455f9-a43a-4505-925b-751900e03405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " preprocessing (KerasLayer)     {'input_word_ids':   0           ['text[0][0]']                   \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_type_ids':                                                \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_mask': (Non                                               \n",
            "                                e, 128)}                                                          \n",
            "                                                                                                  \n",
            " BERT_encoder (KerasLayer)      {'pooled_output': (  109482241   ['preprocessing[0][0]',          \n",
            "                                None, 768),                       'preprocessing[0][1]',          \n",
            "                                 'sequence_output':               'preprocessing[0][2]']          \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 'default': (None,                                                \n",
            "                                768),                                                             \n",
            "                                 'encoder_outputs':                                               \n",
            "                                 [(None, 128, 768),                                               \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)]}                                               \n",
            "                                                                                                  \n",
            " fc (Dense)                     (None, 100)          76900       ['BERT_encoder[0][13]']          \n",
            "                                                                                                  \n",
            " classifier (Dense)             (None, 1)            101         ['fc[0][0]']                     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,559,242\n",
            "Trainable params: 109,559,241\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "classifier_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "1L4k5p-bIdyq"
      },
      "outputs": [],
      "source": [
        "def pearson_loss(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x, axis=0)\n",
        "    my = K.mean(y, axis=0)\n",
        "    xm, ym = x - mx, y - my\n",
        "    r_num = K.sum(xm * ym)\n",
        "    x_square_sum = K.sum(xm * xm)\n",
        "    y_square_sum = K.sum(ym * ym)\n",
        "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
        "    r = -r_num / r_den\n",
        "    return K.mean(r)\n",
        "\n",
        "def pearson_metric(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x, axis=0)\n",
        "    my = K.mean(y, axis=0)\n",
        "    xm, ym = x - mx, y - my\n",
        "    r_num = K.sum(xm * ym)\n",
        "    x_square_sum = K.sum(xm * xm)\n",
        "    y_square_sum = K.sum(ym * ym)\n",
        "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
        "    r = r_num / r_den\n",
        "    return K.mean(r)\n",
        "\n",
        "loss = pearson_loss\n",
        "loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "metric_pearson = pearson_metric\n",
        "metric_mse = tf.keras.metrics.MeanSquaredError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpXy0Zm7JnbJ"
      },
      "outputs": [],
      "source": [
        "epochs = 1\n",
        "batch_size = 32\n",
        "steps_per_epoch = x_train.shape[0] / batch_size \n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(epochs * x_train.shape[0] * 0.1 / batch_size)\n",
        "\n",
        "# solution 1\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI_L17okKvNf"
      },
      "outputs": [],
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=[metric_pearson, metric_mse])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9rUCXRzLhmL"
      },
      "outputs": [],
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=x_train, y=Y_train, epochs=epochs, \n",
        "                               batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjGrtWz6Iu8L"
      },
      "outputs": [],
      "source": [
        "loss, metric_1, metric_2 = classifier_model.evaluate(x=x_test, y=Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcXmtg26cEAG"
      },
      "outputs": [],
      "source": [
        "Y_predicted = classifier_model.predict(x_test)\n",
        "ris = pd.DataFrame(\n",
        "    {'x_test': x_test,\n",
        "     'Y_test': Y_test,\n",
        "     'Y_predicted': list(Y_predicted)\n",
        "    })\n",
        "ris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a07yUREWKK8s"
      },
      "source": [
        "#Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ7fM7G1KNJK"
      },
      "outputs": [],
      "source": [
        "parameters = {'epochs': [1, 2, 3], \n",
        "              'batch_size':[32],\n",
        "              'init_lr': [3e-6, 3e-5],\n",
        "              'dense_size' : [100,200],\n",
        "              'loss' : [tf.keras.losses.MeanSquaredError(), pearson_loss]\n",
        "              }\n",
        "\n",
        "best_scores = -1\n",
        "best_params = {1: dict()}\n",
        "\n",
        "for loss in parameters['loss']:\n",
        "  print(\"Loss: \", loss)\n",
        "  for epochs in parameters['epochs']:\n",
        "    print(\" Epochs: \", epochs)\n",
        "    for init_lr in parameters['init_lr']:\n",
        "      print(\"  Start Learning Rate: \", init_lr)\n",
        "      for batch_size in parameters['batch_size']:\n",
        "        print(\"   Batch Size: \", batch_size)\n",
        "        for dense_size in parameters['dense_size']:\n",
        "          print(\"    Dense size: \", dense_size)\n",
        "          steps_per_epoch = x_train.shape[0] / batch_size \n",
        "          num_train_steps = steps_per_epoch * epochs\n",
        "          num_warmup_steps = int(epochs * x_train.shape[0] * 0.1 / batch_size)\n",
        "          optimizer = optimization.create_optimizer(init_lr=init_lr, \n",
        "                                                    num_train_steps=num_train_steps, \n",
        "                                                    num_warmup_steps=num_warmup_steps, \n",
        "                                                    optimizer_type='adamw')\n",
        "          classifier_model = build_classifier_model(dense_size)\n",
        "          classifier_model.compile(optimizer=optimizer, loss=loss, \n",
        "                                   metrics=[metric_pearson, metric_mse])\n",
        "          history = classifier_model.fit(x=x_train, y=Y_train, epochs=epochs, \n",
        "                                         batch_size=batch_size)\n",
        "          loss_calculated, pearson ,mse = classifier_model.evaluate(x=x_val, \n",
        "                                                                    y=Y_val)\n",
        "          print(\"     Pearson: \", pearson)\n",
        "          print(\"     MSE: \", mse)\n",
        "          if pearson > best_scores:                 \n",
        "            best_score = pearson\n",
        "            best_params = {'epochs': epochs, \n",
        "                           'batch_size': batch_size, \n",
        "                           'start_lr': init_lr,  \n",
        "                           'dense_size': dense_size,\n",
        "                           'loss': loss_calculated}\n",
        "print(best_scores)\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "JGxguBd7tbb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best parameter found on grid search\n",
        "parameters = {'epochs': 2, \n",
        "              'batch_size': 32,\n",
        "              'init_lr': 3e-5,\n",
        "              'dense_size': 100,\n",
        "              'loss': pearson_loss\n",
        "              }\n",
        "\n",
        "epochs = parameters['epochs']\n",
        "batch_size = parameters['batch_size']\n",
        "init_lr = parameters['init_lr']\n",
        "dense_size = parameters['dense_size']\n",
        "loss = parameters['loss']\n",
        "\n",
        "steps_per_epoch = x_train.shape[0] / batch_size \n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(epochs * x_train.shape[0] * 0.1 / batch_size)\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr, \n",
        "                                          num_train_steps=num_train_steps, \n",
        "                                          num_warmup_steps=num_warmup_steps, \n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model = build_classifier_model(dense_size)\n",
        "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metric_pearson, \n",
        "                                                                  metric_mse])\n",
        "history = classifier_model.fit(x=x_train.append(x_val), y=Y_train.append(Y_val), validation_data=(x_test, Y_test), \n",
        "                               epochs=epochs, batch_size=batch_size)\n",
        "loss_calculated, pearson, mse = classifier_model.evaluate(x_test, Y_test)\n",
        "print(\"Pearson: \", pearson)\n",
        "print(\"MSE: \", mse)"
      ],
      "metadata": {
        "id": "s8DfZjrPIvy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9195df18-e447-4edf-ded4-75964ac2de83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "810/810 [==============================] - 1528s 2s/step - loss: -0.5465 - pearson_metric: 0.5465 - mean_squared_error: 0.1154 - val_loss: -0.4177 - val_pearson_metric: 0.4136 - val_mean_squared_error: 0.1124\n",
            "Epoch 2/2\n",
            "264/810 [========>.....................] - ETA: 15:24 - loss: -0.6896 - pearson_metric: 0.6896 - mean_squared_error: 0.0825"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model"
      ],
      "metadata": {
        "id": "dTZwGeyPtd4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.save(\"drive/MyDrive/Colab Notebooks/NLP/classifierIBM30k.h5\")"
      ],
      "metadata": {
        "id": "R5eU9VjBLV8T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Predictor_score.jpynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
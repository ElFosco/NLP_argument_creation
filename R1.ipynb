{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElFosco/NLP_argument_creation/blob/main/R1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installation"
      ],
      "metadata": {
        "id": "yF85RhAe_rG9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2qRNRty-YBk"
      },
      "outputs": [],
      "source": [
        "!pip install -q tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-text"
      ],
      "metadata": {
        "id": "7-UIt-40_ct-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import"
      ],
      "metadata": {
        "id": "T9riNQ0y_wZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.layers import concatenate\n",
        "from keras import Sequential\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder  # one-hot encoding\n",
        "\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "from urllib import request\n",
        "import zipfile\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "hZPzn4Yp_m4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Extraction"
      ],
      "metadata": {
        "id": "a7_c-TgJ5c6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using google drive to upload the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VF_LvtxAf6kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Data\")\n",
        "url = 'https://nlp.stanford.edu/projects/snli/snli_1.0.zip'\n",
        "dataset_path = os.path.join(dataset_folder, \"snli_1.0.zip\")\n",
        "dataset_unzip = os.path.join(dataset_folder,\"snli_1.0\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "  os.makedirs(dataset_folder)\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "  print(\"Downloading dataset...\")\n",
        "  request.urlretrieve(url, dataset_path)\n",
        "  print(\"Download complete!\")\n",
        "\n",
        "if not os.path.exists(dataset_unzip):\n",
        "  print(\"Extracting dataset... (it may take a while...)\")\n",
        "  with zipfile.ZipFile(dataset_path) as loaded_tar:\n",
        "    loaded_tar.extractall(dataset_folder)\n",
        "  print(\"Extraction completed!\")\n",
        "\n",
        "dataset_train =  os.path.join(dataset_unzip,\"snli_1.0_train.jsonl\")\n",
        "dataset_dev = os.path.join(dataset_unzip,\"snli_1.0_dev.jsonl\")\n",
        "dataset_test = os.path.join(dataset_unzip,\"snli_1.0_test.jsonl\")\n"
      ],
      "metadata": {
        "id": "XkSkvESG_qfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_json(dataset_train,lines = True)[['sentence1','sentence2','gold_label']]\n",
        "df_dev = pd.read_json(dataset_dev,lines = True)[['sentence1','sentence2','gold_label']]\n",
        "df_test = pd.read_json(dataset_test,lines = True)[['sentence1','sentence2','gold_label']]"
      ],
      "metadata": {
        "id": "gBvvUcYM-ibD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def append_sentences(sentence_1,sentence_2):\n",
        "  if not (re.search('[\\.|?|!]$',sentence_1)):  # append the topic \n",
        "    sentence_1 = sentence_1+'. '\n",
        "  else:\n",
        "    sentence_1 = re.sub('[\\.|?|!]$','. ',sentence_1)\n",
        "  sentence_1 = (sentence_1 + sentence_2).lower()\n",
        "  return sentence_1"
      ],
      "metadata": {
        "id": "BGYXT9s-DMY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['sentences'] = df_train.apply(lambda row: append_sentences(row['sentence1'],row['sentence2']), axis = 1)\n",
        "df_dev['sentences'] =   df_dev.apply(lambda row: append_sentences(row['sentence1'],row['sentence2']), axis = 1)\n",
        "df_test['sentences'] =  df_test.apply(lambda row: append_sentences(row['sentence1'],row['sentence2']), axis = 1)\n",
        "\n",
        "\n",
        "df_train = df_train.drop(['sentence1','sentence2'],axis=1)\n",
        "df_dev = df_dev.drop(['sentence1','sentence2'],axis=1)\n",
        "df_test = df_test.drop(['sentence1','sentence2'],axis=1)"
      ],
      "metadata": {
        "id": "aRIJLUm0DFpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[df_train.gold_label != '-']\n",
        "df_dev = df_dev[df_dev.gold_label != '-']\n",
        "df_test = df_test[df_test.gold_label != '-']\n",
        "\n",
        "df_train_grid = df_train.sample(frac=0.5)\n",
        "df_dev_grid = df_dev.sample(frac=0.5)\n",
        "\n",
        "x_train = df_train['sentences']\n",
        "Y_train = df_train['gold_label']\n",
        "\n",
        "x_train_grid = df_train_grid['sentences']\n",
        "Y_train_grid = df_train_grid['gold_label']\n",
        "\n",
        "x_dev = df_dev['sentences']\n",
        "Y_dev = df_dev['gold_label']\n",
        "\n",
        "x_dev_grid = df_dev_grid['sentences']\n",
        "Y_dev_grid = df_dev_grid['gold_label']\n",
        "\n",
        "x_test = df_test['sentences']\n",
        "Y_test = df_test['gold_label']"
      ],
      "metadata": {
        "id": "eJQ6o_ZoPXd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##One hot encoding"
      ],
      "metadata": {
        "id": "-l21QzLNMvZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHotEncoderWrapper(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.label_encoder = LabelEncoder()\n",
        "\n",
        "  def get_one_hot_encoding(self, list_pos):\n",
        "    # creates a dictionary containing pos and its one hot encoding\n",
        "    ris = {}\n",
        "    integer_encoded = self.label_encoder.fit_transform(list_pos)\n",
        "    # binary encode\n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\n",
        "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "    for i in range(len(list_pos)):\n",
        "      ris[list_pos[i]] = onehot_encoded[i]\n",
        "    return ris\n",
        "  \n",
        "  def get_inverse(self, encoded):\n",
        "    # from one hot encoding to original pos\n",
        "    return self.label_encoder.inverse_transform([np.argmax(encoded)])\n",
        "\n",
        "  def get_pos_from_label(self, label):\n",
        "    # from label to original pos\n",
        "    return self.label_encoder.inverse_transform([label])"
      ],
      "metadata": {
        "id": "Wbodk6vQMxHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoder = OneHotEncoderWrapper()\n",
        "set_label = df_train['gold_label'].unique()\n",
        "ris = one_hot_encoder.get_one_hot_encoding(set_label)\n",
        "\n",
        "Y_train_encoded = [ris[label] for label in Y_train]\n",
        "Y_train_encoded_grid = [ris[label] for label in Y_train_grid]\n",
        "\n",
        "Y_dev_encoded = [ris[label] for label in Y_dev]\n",
        "Y_dev_encoded_grid = [ris[label] for label in Y_dev_grid]\n",
        "\n",
        "Y_test_encoded = [ris[label] for label in Y_test]\n",
        "\n",
        "Y_train_encoded = np.asarray(Y_train_encoded)\n",
        "Y_train_encoded_grid = np.asarray(Y_train_encoded_grid)\n",
        "\n",
        "Y_dev_encoded = np.asarray(Y_dev_encoded)\n",
        "Y_dev_encoded_grid = np.asarray(Y_dev_encoded_grid)\n",
        "\n",
        "Y_test_encoded = np.asarray(Y_test_encoded)"
      ],
      "metadata": {
        "id": "pv9jz2YeNF7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bert"
      ],
      "metadata": {
        "id": "L2tUALOfJN63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'albert_en_base'  # @param [\"bert_en_uncased_L-24_H-1024_A-16\",\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-IgMLE24JJ66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) #preprocessing layer"
      ],
      "metadata": {
        "id": "ABHw1xYzKjEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)               #bert model"
      ],
      "metadata": {
        "id": "rdHHOttyKjY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model(dense_size=100):                     # model used to compute the inference between topic and argument\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dense(dense_size, activation=keras.activations.relu, name='fc_1')(net)\n",
        "  net = tf.keras.layers.Dense(3, activation=keras.activations.softmax, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "j4e_ySupLAZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Grid Search"
      ],
      "metadata": {
        "id": "3zTsyy4zU1yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_grid.shape "
      ],
      "metadata": {
        "id": "Fvo87LZSif-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {'epochs': [1,2], \n",
        "              'batch_size':[16,32,64],\n",
        "              'init_lr': [3e-5],\n",
        "              'dense_size' : [100,200,300]\n",
        "              }\n",
        "best_scores = -1\n",
        "best_params = {1: dict()}\n",
        "\n",
        "\n",
        "\n",
        "for epochs in parameters['epochs']:\n",
        "  print(\" Epochs: \", epochs)\n",
        "  for init_lr in parameters['init_lr']:\n",
        "    print(\"  Start Learning Rate: \", init_lr)\n",
        "    for batch_size in parameters['batch_size']:\n",
        "      print(\"   Batch Size: \", batch_size)\n",
        "      for dense_size in parameters['dense_size']:\n",
        "        print(\"    Dense size: \", dense_size)\n",
        "        steps_per_epoch = x_train_grid.shape[0] / batch_size \n",
        "        num_train_steps = steps_per_epoch * epochs\n",
        "        num_warmup_steps = int(epochs * x_train_grid.shape[0] * 0.1 / batch_size)\n",
        "        optimizer = optimization.create_optimizer(init_lr=init_lr, \n",
        "                                                    num_train_steps=num_train_steps, \n",
        "                                                    num_warmup_steps=num_warmup_steps, \n",
        "                                                    optimizer_type='adamw')\n",
        "        classifier_model = build_classifier_model(dense_size)\n",
        "        classifier_model.compile(optimizer=optimizer, loss='categorical_crossentropy', \n",
        "                                   metrics=['accuracy'])\n",
        "        history = classifier_model.fit(x=x_train_grid, y=Y_train_encoded_grid, epochs=epochs, \n",
        "                                         batch_size=batch_size)\n",
        "        loss_calculated, accuracy = classifier_model.evaluate(x=x_dev_grid, y=Y_dev_encoded_grid)\n",
        "        print(\"     Loss: \", loss_calculated)\n",
        "        print(\"     Accuracy: \", accuracy)\n",
        "        if accuracy > best_scores:                 \n",
        "          best_score = accuracy\n",
        "          best_params = {'epochs': epochs, \n",
        "                          'batch_size': batch_size, \n",
        "                          'start_lr': init_lr,  \n",
        "                          'dense_size': dense_size,\n",
        "                          'loss': loss_calculated}\n",
        "print(best_scores)\n",
        "print(best_params)"
      ],
      "metadata": {
        "id": "TI2mefjeU3al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "lgFmY532aNBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best parameter found on grid search\n",
        "parameters = {'epochs': 1, \n",
        "              'batch_size': 32,\n",
        "              'init_lr': 3e-5,\n",
        "              'dense_size': 100\n",
        "              }\n",
        "\n",
        "epochs = parameters['epochs']\n",
        "batch_size = parameters['batch_size']\n",
        "init_lr = parameters['init_lr']\n",
        "dense_size = parameters['dense_size']\n",
        "\n",
        "\n",
        "x_final = x_train_grid.append(x_dev_grid)\n",
        "Y_final_encoded = np.array(Y_train_encoded_grid.tolist() + Y_dev_encoded_grid.tolist())\n",
        "\n",
        "steps_per_epoch = (x_final).shape[0] / batch_size \n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(epochs * x_final.shape[0] * 0.1 / batch_size)\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr, \n",
        "                                          num_train_steps=num_train_steps, \n",
        "                                          num_warmup_steps=num_warmup_steps, \n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model = build_classifier_model(dense_size)\n",
        "classifier_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = classifier_model.fit(x=(x_final), y=(Y_final_encoded), \n",
        "                               epochs=epochs, batch_size=batch_size)\n",
        "ris = classifier_model.evaluate(x=x_test, y=Y_test_encoded)\n",
        "print(\"Accuracy: \", ris)"
      ],
      "metadata": {
        "id": "2YTJbMK2aMzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.save(\"Models/<model_name>\")"
      ],
      "metadata": {
        "id": "9Ij2EL1lfvqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
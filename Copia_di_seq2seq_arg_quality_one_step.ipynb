{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElFosco/NLP_argument_creation/blob/main/Copia_di_seq2seq_arg_quality_one_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6HM6NwrJ5XM"
      },
      "source": [
        "## Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gXdnUBOJ7xT"
      },
      "outputs": [],
      "source": [
        "!pip install -q tf-models-official\n",
        "!pip install pytorch_pretrained_bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5qaWm88Shmy"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIicDYvXShmz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Perplexity\n",
        "import torch\n",
        "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel, OpenAIGPTLMHeadModel\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# BERT\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "# Embeddings\n",
        "import gensim  \n",
        "import gensim.downloader as gloader\n",
        "\n",
        "# Data & Pre-processing\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Defining hyperparameters\n",
        "BUFFER_SIZE = 1024\n",
        "TRAIN_BUFFER = 256  # Shoud be lower than BUFFER_SIZE\n",
        "EMBED_DIM = 100\n",
        "LATENT_DIM = 512\n",
        "NUM_HEADS = 8\n",
        "BATCH_SIZE = 256\n",
        "BATCH_SIZE_RL = 8\n",
        "\n",
        "# Datasets\n",
        "train_dataset = \"/gdrive/MyDrive/NLP/arg_quality_rank_30k.csv\"\n",
        "\n",
        "# Embedding model\n",
        "embed_model = \"/gdrive/MyDrive/NLP/glove_{}_pickle\".format(EMBED_DIM)\n",
        "\n",
        "# BERT models weights\n",
        "quality_model_weights = \"/gdrive/MyDrive/NLP/classifierIBM30k.h5\"\n",
        "r1_model_weights = \"/gdrive/MyDrive/NLP/classifierNLI.h5\"\n",
        "\n",
        "# BERT model scores of start sentences, based on best model of BERT\n",
        "start_scores = \"/gdrive/MyDrive/NLP/start_scores_bert_pickle\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f-_HkWYic97"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d448XZAuShmz"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5rlr-6GTD7F"
      },
      "source": [
        "### Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu2t2HfLTIA0"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def preprocess_pretrain(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\\"|-|\\\\\\\\|`', ' ', text)  # delete this chars from the string [\"-\\`]\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('^[.]+', '', text)         # delete dots at the beginning of the sentence\n",
        "    text = re.sub(\"([?.!,])\", r\" \\1 \", text)\n",
        "    text = re.sub('\\. \\.', '.', text)        # delete . .\n",
        "    text = re.sub('&', ' and ', text)        # replace & with and\n",
        "    text = re.sub(' +', ' ', text)           # delete additional whitespace\n",
        "    text = text.strip()\n",
        "    text = \" \".join([\"[start]\", text, \"[end]\"])\n",
        "    text = \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_rl(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\\"|-|\\\\\\\\|`', ' ', text)  # delete this chars from the string [\"-\\`]\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('^[.]+', '', text)         # delete dots at the beginning of the sentence\n",
        "    text = re.sub(\"([?.!,])\", r\" \\1 \", text)\n",
        "    text = re.sub('\\. \\.', '.', text)        # delete . .\n",
        "    text = re.sub('&', ' and ', text)        # replace & with and\n",
        "    text = re.sub(' +', ' ', text)           # delete additional whitespace\n",
        "    text = text.strip()\n",
        "    text = \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n",
        "    return text\n",
        "\n",
        "\n",
        "def load_embedding_model(model_type: str,\n",
        "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"\"\n",
        "\n",
        "    # Find the correct embedding model name\n",
        "    if model_type.strip().lower() == 'word2vec':\n",
        "        download_path = \"word2vec-google-news-300\"\n",
        "\n",
        "    elif model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "    elif model_type.strip().lower() == 'fasttext':\n",
        "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
        "\n",
        "    # Check download\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Word2Vec: 300\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model\n",
        "\n",
        "\n",
        "def check_OOV_terms(embedding_vocabulary, word_listing):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_vocabulary: pre-trained word embedding model vocab (list)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    \n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)\n",
        "\n",
        "\n",
        "def build_embedding_matrix(embedding_model,\n",
        "                           embedding_dimension,\n",
        "                           word_to_idx,\n",
        "                           vocab_size,\n",
        "                           oov_terms):\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[word]\n",
        "        except (KeyError, TypeError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def update_embedding_matrix(embedding_model, \n",
        "                            embedding_dimension,\n",
        "                            word_to_idx,\n",
        "                            vocab_size,\n",
        "                            oov_terms):\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained emdedding matrix\n",
        "\n",
        "    :param embedding_model: pre-trained embedding matrix\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[idx]\n",
        "        except (TypeError, IndexError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "class KerasTokenizer(object):\n",
        "    \"\"\"\n",
        "    A simple high-level wrapper for the Keras tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, build_embedding_matrix=False, embedding_dimension=None,\n",
        "                 embedding_model_type=None, tokenizer_args=None, embedding_model=None):\n",
        "        if build_embedding_matrix:\n",
        "            assert embedding_model_type is not None\n",
        "            assert embedding_dimension is not None and type(embedding_dimension) == int\n",
        "\n",
        "        self.build_embedding_matrix = build_embedding_matrix\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.embedding_model_type = embedding_model_type\n",
        "        self.embedding_model = embedding_model\n",
        "        self.embedding_matrix = None\n",
        "        self.vocab = None\n",
        "\n",
        "        tokenizer_args = {} if tokenizer_args is None else tokenizer_args\n",
        "        assert isinstance(tokenizer_args, dict) or isinstance(tokenizer_args, collections.OrderedDict)\n",
        "\n",
        "        self.tokenizer_args = tokenizer_args\n",
        "\n",
        "    def build_vocab(self, data, **kwargs):\n",
        "        print('Fitting tokenizer...')\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(**self.tokenizer_args)\n",
        "        self.tokenizer.fit_on_texts(data)\n",
        "        print('Fit completed!')\n",
        "\n",
        "        self.vocab = self.tokenizer.word_index\n",
        "\n",
        "        if self.build_embedding_matrix:\n",
        "            if self.embedding_model is None:\n",
        "              print('Loading embedding model! It may take a while...')\n",
        "              self.embedding_model = load_embedding_model(model_type=self.embedding_model_type, \n",
        "                                                          embedding_dimension=self.embedding_dimension)\n",
        "            \n",
        "            print('Checking OOV terms in train...')\n",
        "            self.oov_terms_train = check_OOV_terms(embedding_vocabulary=set(self.embedding_model.vocab.keys()),\n",
        "                                             word_listing=list(self.vocab.keys()))\n",
        "            \n",
        "            print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms_train), 100*float(len(self.oov_terms_train)) / len(self.vocab)))\n",
        "\n",
        "            print('Building the embedding matrix for train...')\n",
        "            self.embedding_matrix = build_embedding_matrix(embedding_model=self.embedding_model,\n",
        "                                                           word_to_idx=self.vocab,\n",
        "                                                           vocab_size=len(self.vocab)+1,          \n",
        "                                                           embedding_dimension=self.embedding_dimension,\n",
        "                                                           oov_terms=self.oov_terms_train)\n",
        "            print('Done for train!')\n",
        "\n",
        "    def update_vocab(self, data, **kwargs):\n",
        "      self.tokenizer.fit_on_texts(data)\n",
        "      if self.build_embedding_matrix:\n",
        "        old_vocab = self.vocab\n",
        "        self.vocab = self.tokenizer.word_index\n",
        "        print('Checking OOV terms...')\n",
        "        self.oov_terms = check_OOV_terms(embedding_vocabulary=set(old_vocab.keys()), \n",
        "                                         word_listing=list(self.vocab.keys()))\n",
        "        \n",
        "        print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(self.oov_terms), 100*float(len(self.oov_terms)) / len(self.vocab)))\n",
        "\n",
        "        print('Building the embedding matrix...')\n",
        "        self.embedding_matrix = update_embedding_matrix(embedding_model=self.embedding_matrix,\n",
        "                                                       word_to_idx=self.vocab,\n",
        "                                                       vocab_size=len(self.vocab)+1,          \n",
        "                                                       embedding_dimension=self.embedding_dimension,\n",
        "                                                       oov_terms=self.oov_terms)\n",
        "\n",
        "    def get_info(self):\n",
        "        return {\n",
        "            'build_embedding_matrix': self.build_embedding_matrix,\n",
        "            'embedding_dimension': self.embedding_dimension,\n",
        "            'embedding_model_type': self.embedding_model_type,\n",
        "            'embedding_matrix': self.embedding_matrix.shape if self.embedding_matrix is not None else self.embedding_matrix,\n",
        "            'embedding_model': self.embedding_model,\n",
        "            'vocab_size': len(self.vocab) + 1,\n",
        "        }\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return text\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        if type(tokens) == str:\n",
        "            return self.tokenizer.texts_to_sequences([tokens])[0]\n",
        "        else:\n",
        "            return self.tokenizer.texts_to_sequences(tokens)\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        return self.tokenizer.sequences_to_texts(ids)\n",
        "\n",
        "\n",
        "def convert_text(df, tokenizer, is_training=False, max_seq_length=None):\n",
        "    \"\"\"\n",
        "    Converts input text sequences using a given tokenizer\n",
        "\n",
        "    :param texts: either a list or numpy ndarray of strings\n",
        "    :tokenizer: an instantiated tokenizer\n",
        "    :is_training: whether input texts are from the training split or not\n",
        "    :max_seq_length: the max token sequence previously computed with\n",
        "    training texts.\n",
        "\n",
        "    :return\n",
        "        text_ids: a nested list on token indices\n",
        "        max_seq_length: the max token sequence previously computed with\n",
        "        training texts.\n",
        "    \"\"\"\n",
        "\n",
        "    text_ids = tokenizer.convert_tokens_to_ids(df)\n",
        "\n",
        "    # Padding\n",
        "    if is_training:\n",
        "        max_seq_length = int(np.quantile([len(seq) for seq in text_ids], 0.95))\n",
        "    else:\n",
        "        assert max_seq_length is not None\n",
        "\n",
        "    text_ids = [seq + [0] * (max_seq_length - len(seq)) for seq in text_ids]\n",
        "    text_ids = np.array([seq[:max_seq_length] for seq in text_ids])\n",
        "\n",
        "    if is_training:\n",
        "        return text_ids, max_seq_length\n",
        "    else:\n",
        "        return text_ids\n",
        "\n",
        "\n",
        "def decode_sentence(input_sentence, preprocess):\n",
        "    # Mapping the input sentence to tokens and adding start and end tokens\n",
        "    tokenized_input_sentence = tokenizer.convert_tokens_to_ids(\n",
        "        [preprocess(input_sentence)]\n",
        "    )[0]\n",
        "    tokenized_input_sentence = tf.pad(\n",
        "        tokenized_input_sentence,\n",
        "        [[0, max_seq_length - tf.shape(tokenized_input_sentence)[0]]])\n",
        "    # Initializing the initial sentence consisting of only the start token.\n",
        "    tokenized_target_sentence = tf.expand_dims(tokenizer.vocab[\"[start]\"], 0)\n",
        "\n",
        "    # Get the predictions\n",
        "    predictions = fnet.predict(\n",
        "        {\n",
        "            \"encoder_inputs\": tf.expand_dims(tokenized_input_sentence, 0),\n",
        "        }\n",
        "    )\n",
        "    # Calculating the token of step i (sentence len is i-1) with maximum probability and getting the corresponding word\n",
        "    sampled_token_index = tf.argmax(predictions[0, :], axis=1)  # predictions.shape == (batch_size, max_seq_len, vocab_size)\n",
        "    decoded_sentence = \" \".join(tokenizer.convert_ids_to_tokens([sampled_token_index.numpy()]))\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spn1n_WYTIhl"
      },
      "source": [
        "### Loading "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAHZs6KbTyXn"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(train_dataset)\n",
        "df = df.drop([\"WA\", \"stance_WA\", \"stance_WA_conf\", \"MACE-P\"], axis=1)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xjCBDUKm_i"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYwrsXZvUM3_"
      },
      "outputs": [],
      "source": [
        "df.loc[2,\"argument\"] = \"zero tolerance policy in schools should not be adopted as circumstances are often not black and white, being more nuanced. no one should be written off due to a mistake of judgement.\"\n",
        "pretrain_series = df.apply(lambda row : preprocess_pretrain(row['argument']), axis = 1)\n",
        "\n",
        "rl_series = df.apply(lambda row : preprocess_rl(row['argument']), axis = 1)\n",
        "topic_list = list(df.apply(lambda row : row['topic'].lower(), axis = 1))\n",
        "pretrain_series.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RK-KLKUUl5b"
      },
      "source": [
        "### Train, test, val splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgkpkAnPUfm5"
      },
      "outputs": [],
      "source": [
        "is_training_data =  df['set']=='train'\n",
        "is_validation_data =  df['set']=='dev'\n",
        "is_test_data =  df['set']=='test'\n",
        "\n",
        "x_train = pretrain_series[is_training_data]\n",
        "x_train = x_train.append(pretrain_series[is_validation_data])\n",
        "x_test  = pretrain_series[is_test_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwNEJrQoShm1"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecxi9TNColzy"
      },
      "outputs": [],
      "source": [
        "# load embeddings from glove\n",
        "import pickle\n",
        "if os.path.exists(embed_model):\n",
        "  with open(embed_model, \"rb\") as f:\n",
        "    embedding_model = pickle.load(f)\n",
        "else:\n",
        "  embedding_model = load_embedding_model(model_type=\"glove\", \n",
        "                                         embedding_dimension=EMBED_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DIT2jkkooWC"
      },
      "outputs": [],
      "source": [
        "# creating tokenizer and vocabulary\n",
        "\n",
        "tokenizer_args = {\n",
        "    'oov_token': \"OOV_TOKEN\",  # The vocabulary id for unknown terms during text conversion\n",
        "    'lower' : True,  # default\n",
        "    'filters' : '' \n",
        "}\n",
        "\n",
        "tokenizer = KerasTokenizer(tokenizer_args=tokenizer_args,\n",
        "                           build_embedding_matrix=True,\n",
        "                           embedding_dimension=EMBED_DIM,\n",
        "                           embedding_model_type=\"glove\", \n",
        "                           embedding_model=embedding_model)\n",
        "\n",
        "tokenizer.build_vocab(x_train)\n",
        "tokenizer.update_vocab(x_test)\n",
        "VOCAB_SIZE = len(tokenizer.vocab)\n",
        "\n",
        "tokenizer_info = tokenizer.get_info()\n",
        "\n",
        "print('Tokenizer info: ', tokenizer_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD4GNbAvShm3"
      },
      "source": [
        "### Tokenizing and padding sentences using `TextVectorization`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yppEeUKlYwmP"
      },
      "outputs": [],
      "source": [
        "x_train_tokens, max_seq_length = convert_text(x_train, tokenizer, True)\n",
        "x_test_tokens = convert_text(x_test, tokenizer, max_seq_length=max_seq_length)\n",
        "print(\"Max token sequence: {}\".format(max_seq_length))\n",
        "print('X train shape: ', x_train_tokens.shape)\n",
        "print('X test shape: ', x_test_tokens.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbuI0HveZGHJ"
      },
      "source": [
        "### Tensorflow Dataset for Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyzOlDBixOkx"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_tokens, x_train_tokens))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_tokens, x_test_tokens))\n",
        "\n",
        "def vectorize_text(inputs, outputs):\n",
        "    # One extra padding token to the right to match the output shape\n",
        "    outputs = tf.pad(outputs, [[0, 1]])\n",
        "    return (\n",
        "        {\"encoder_inputs\": inputs},\n",
        "        {\"outputs\": outputs[1:]},\n",
        "    )\n",
        "\n",
        "train_dataset = train_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset = (\n",
        "    train_dataset.cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "test_dataset = test_dataset.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwJgKbOQVH1j"
      },
      "source": [
        "### Dataset for RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oQKDpztVKW0"
      },
      "outputs": [],
      "source": [
        "x_train = rl_series[is_training_data]\n",
        "x_val = rl_series[is_validation_data]\n",
        "x_test  = rl_series[is_test_data]\n",
        "\n",
        "# -2 because we will add start and end\n",
        "x_train_rl = convert_text(x_train, tokenizer, max_seq_length=max_seq_length-2)\n",
        "x_val_rl = convert_text(x_val, tokenizer, max_seq_length=max_seq_length-2)\n",
        "x_test_rl= convert_text(x_test, tokenizer, max_seq_length=max_seq_length-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI3q1UV-Q7Kp"
      },
      "source": [
        "## Seq2Seq Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp6jfLNIShm5"
      },
      "source": [
        "### Creating the FNet Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kylhhrZyShm5"
      },
      "outputs": [],
      "source": [
        "class FNetEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, **kwargs):\n",
        "        super(FNetEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Casting the inputs to complex64\n",
        "        inp_complex = tf.cast(inputs, tf.complex64)\n",
        "        # Projecting the inputs to the frequency domain using FFT2D and\n",
        "        # extracting the real part of the output\n",
        "        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n",
        "        proj_input = self.layernorm_1(inputs + fft)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHuNwBidShm6"
      },
      "source": [
        "### Creating the Decoder One Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gUArwKswdcF"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class FNetDecoderOneStep(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(FNetDecoderOneStep, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(latent_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(encoder_outputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "          padding_mask = causal_mask\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=encoder_outputs,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(encoder_outputs + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "def create_actor(max_length):\n",
        "    # Encoder\n",
        "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n",
        "    x = PositionalEmbedding(max_length, VOCAB_SIZE+1, EMBED_DIM)(encoder_inputs)\n",
        "    encoder_outputs = FNetEncoder(EMBED_DIM, LATENT_DIM)(x)\n",
        "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "    # Encoder -> Decoder\n",
        "    encoded_seq_inputs = keras.Input(\n",
        "        shape=(None, EMBED_DIM), name=\"decoder_state_inputs\"\n",
        "    )\n",
        "    \n",
        "    # \"Merge\" inputs Decoder\n",
        "    x = FNetDecoderOneStep(EMBED_DIM, LATENT_DIM, NUM_HEADS)(encoded_seq_inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    decoder_outputs = layers.Dense(VOCAB_SIZE+1, \n",
        "                                   activation=\"softmax\")(x)\n",
        "\n",
        "    decoder = keras.Model(encoded_seq_inputs, decoder_outputs, name=\"outputs\")\n",
        "    decoder_outputs = decoder(encoder_outputs)\n",
        "    fnet = keras.Model(encoder_inputs, decoder_outputs, name=\"actor\")\n",
        "    return fnet\n",
        "\n",
        "\n",
        "def create_critic(max_length):\n",
        "    # Encoder 1\n",
        "    encoder_inputs_1 = keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs_1\")\n",
        "    x_1 = PositionalEmbedding(max_length, VOCAB_SIZE+1, EMBED_DIM)(encoder_inputs_1)\n",
        "    encoder_outputs_1 = FNetEncoder(EMBED_DIM, LATENT_DIM)(x_1)\n",
        "    encoder_1 = keras.Model(encoder_inputs_1, encoder_outputs_1)\n",
        "\n",
        "    # Encoder 2\n",
        "    encoder_inputs_2 = keras.Input(shape=(max_length, VOCAB_SIZE+1), dtype=tf.float32, name=\"encoder_inputs_2\")\n",
        "    x_2 = layers.Dot(axes=(2, 1), trainable=True)([encoder_inputs_2, K.repeat_elements(x=K.expand_dims(tokenizer.embedding_matrix, 0), rep=BATCH_SIZE_RL, axis=0)])\n",
        "    encoder_outputs_2 = FNetEncoder(EMBED_DIM, LATENT_DIM)(x_2)\n",
        "    encoder_2 = keras.Model(encoder_inputs_2, encoder_outputs_2)\n",
        "\n",
        "    merge = layers.Add()([encoder_outputs_1, -encoder_outputs_2])\n",
        "    x = layers.Dense(128, activation=\"relu\")(merge)\n",
        "    flattened = layers.Flatten()(x)\n",
        "    out = layers.Dense(1, activation=\"tanh\")(flattened)\n",
        "    fnet = keras.Model([encoder_inputs_1, encoder_inputs_2], out, name=\"critic\")\n",
        "    return fnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdrNwBmsShm7"
      },
      "source": [
        "### Creating the seq2seq model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN58kiEwShm7"
      },
      "outputs": [],
      "source": [
        "fnet = create_actor(max_seq_length)\n",
        "fnet.compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elDPc9Vpkoa1"
      },
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRHobre2krAX"
      },
      "outputs": [],
      "source": [
        "fnet.load_weights(\"/gdrive/MyDrive/NLP/pretrained_fnet_one_step.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0xDfmLTk1P5"
      },
      "source": [
        "### Pre-Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA1UHp5gShm7"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3)\n",
        "  history = fnet.fit(train_dataset, epochs=90, validation_data=test_dataset, \n",
        "                    callbacks=[callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trqSKiU0Shm8"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WfVMhSbShm8"
      },
      "outputs": [],
      "source": [
        "sentence = \"marriage isn't keeping up with the times. abandon the old thinking and bring something that incorporates all unions not just those with a man and woman.\"\n",
        "out = decode_sentence(sentence, preprocess_pretrain)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9TpGtgfvXW8"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLX_SomHvd5k"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  fnet.save_weights(\"/gdrive/MyDrive/NLP/pretrained_fnet_one_step.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-Bn64skKOWD"
      },
      "source": [
        "## [Bert](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1GvRSj9QpgK"
      },
      "source": [
        "### Model to fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNHbHJK-KPiz"
      },
      "outputs": [],
      "source": [
        "bert_model_name_quality = 'bert_en_uncased_L-12_H-768_A-12'\n",
        "\n",
        "tfhub_handle_encoder_quality = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n",
        "tfhub_handle_preprocess_quality = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "\n",
        "bert_model_name_r1 = 'albert_en_base'\n",
        "\n",
        "tfhub_handle_encoder_r1 = 'https://tfhub.dev/tensorflow/albert_en_base/3'\n",
        "tfhub_handle_preprocess_r1 = 'https://tfhub.dev/tensorflow/albert_en_preprocess/3'\n",
        "\n",
        "print(f'Model name quality                       : {bert_model_name_quality}')\n",
        "print(f'BERT model selected quality              : {tfhub_handle_encoder_quality}')\n",
        "print(f'Preprocess model auto-selected quality   : {tfhub_handle_preprocess_quality}')\n",
        "print(f'Model name r1                            : {bert_model_name_r1}')\n",
        "print(f'BERT model selected r1                   : {tfhub_handle_encoder_r1}')\n",
        "print(f'Preprocess model auto-selected r1        : {tfhub_handle_preprocess_r1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uH8ohE9KUcA"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model(dense_size=100):\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess_quality, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder_quality, trainable=False, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dense(dense_size, activation=keras.activations.relu, name='fc')(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=keras.activations.sigmoid, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFJEdFOKgfHc"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model_r1(dense_size=100):  # model used to compute the score of the topic\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess_r1, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder_r1, trainable=False, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dense(dense_size, activation=keras.activations.relu, name='fc_1')(net)\n",
        "  net = tf.keras.layers.Dense(3, activation=keras.activations.softmax, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZlbTvKXKXHp"
      },
      "outputs": [],
      "source": [
        "bert_model_quality = build_classifier_model()\n",
        "bert_model_r1 = build_classifier_model_r1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k01nAznZKYrz"
      },
      "outputs": [],
      "source": [
        "bert_model_quality.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjHmM4jBQsh0"
      },
      "source": [
        "### Load best model quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Hl6UOAKKabL"
      },
      "outputs": [],
      "source": [
        "bert_model_quality.load_weights(quality_model_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEGbpMK5glj-"
      },
      "source": [
        "### Load best model r1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii1xBlpeglkF"
      },
      "outputs": [],
      "source": [
        "bert_model_r1.load_weights(r1_model_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dF73pIvSKQS"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOHI59Dqq90b"
      },
      "source": [
        "## RL Stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y8q8yY4foJ9"
      },
      "source": [
        "### Noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxIZiVo66Wed"
      },
      "outputs": [],
      "source": [
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None, gaussian=False):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.gaussian = gaussian\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        if not self.gaussian:\n",
        "          # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "          x = (\n",
        "              self.x_prev\n",
        "              + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "              + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "          )\n",
        "          # Store x into x_prev\n",
        "          # Makes next noise dependent on current one\n",
        "          self.x_prev = x\n",
        "        else:\n",
        "          x = np.random.normal(size=self.mean.shape, scale=self.std_dev)\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCjf3ZHuflyl"
      },
      "source": [
        "### Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aO_DrBq5p-T"
      },
      "outputs": [],
      "source": [
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=32, num_epochs=1, \n",
        "                 train_buffer=100000):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Number of epochs\n",
        "        self.num_epochs = num_epochs\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "        # Num of obs to use in epochs\n",
        "        self.train_buffer = train_buffer\n",
        "        assert self.train_buffer <= self.buffer_capacity\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # It tells us num of episode recorder\n",
        "        self.buffer_used = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, max_seq_length))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, max_seq_length, VOCAB_SIZE+1))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "        #if index < int(self.batch_size / 2) and self.buffer_counter >= self.buffer_capacity:  # Avoid deleting pre-training results\n",
        "        #  index += int(self.batch_size / 2)\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.reward_buffer[index] = obs_tuple[1]\n",
        "        self.next_state_buffer[index] = obs_tuple[2]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "        if self.buffer_used < len(self.state_buffer):\n",
        "          self.buffer_used += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tf.function\n",
        "    def update_critic(\n",
        "        self, state_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tf.GradientTape() as tape:\n",
        "            critic_value = critic_model([state_batch, next_state_batch], training=True)\n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(reward_batch - critic_value))\n",
        "            #tf.print(\"critic_value: \", critic_value)\n",
        "\n",
        "            critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "            critic_optimizer.apply_gradients(\n",
        "                zip(critic_grad, critic_model.trainable_variables)\n",
        "            )\n",
        "\n",
        "            #tf.print(\"Critic Loss: \", critic_loss)\n",
        "        return critic_loss\n",
        "\n",
        "    @tf.function\n",
        "    def update_actor(\n",
        "        self, state_batch\n",
        "    ):\n",
        "\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = target_critic([state_batch, actions], training=False)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "            #tf.print(\"Actor Loss: \", critic_value)\n",
        "\n",
        "            actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "            actor_optimizer.apply_gradients(\n",
        "              zip(actor_grad, actor_model.trainable_variables)\n",
        "            )\n",
        "            #tf.print(\"Actor Loss: \", actor_loss)\n",
        "        \n",
        "        return actor_loss\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        size_memory = self.buffer_used\n",
        "        # Random permutation of indices of dataset\n",
        "        obs_perm = np.random.permutation(size_memory)\n",
        "        actor_loss, critic_loss = [], []\n",
        "        print(\"\")\n",
        "        for i in range(self.num_epochs):\n",
        "          steps = 0\n",
        "          # Get sampling range\n",
        "          record_range = range(0, min(size_memory, self.train_buffer))\n",
        "          actor_loss.append([0.0])\n",
        "          critic_loss.append([0.0])\n",
        "          while len(record_range) <= size_memory and self.batch_size <= len(record_range):\n",
        "            # Randomly sample indices\n",
        "            batch_indices = np.random.choice(record_range, self.batch_size, replace=False)\n",
        "            # Convert to tensors\n",
        "            state_batch = tf.convert_to_tensor(self.state_buffer[obs_perm[batch_indices]])\n",
        "            reward_batch = tf.convert_to_tensor(self.reward_buffer[obs_perm[batch_indices]])\n",
        "            reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
        "            next_state_batch = tf.convert_to_tensor(self.next_state_buffer[obs_perm[batch_indices]])\n",
        "            next_state_batch = tf.cast(next_state_batch, dtype=tf.float32)\n",
        "\n",
        "            critic_loss_tmp = self.update_critic(state_batch, reward_batch, next_state_batch)\n",
        "            actor_loss_tmp = self.update_actor(state_batch)\n",
        "            actor_loss[-1] += actor_loss_tmp\n",
        "            critic_loss[-1] += critic_loss_tmp\n",
        "            steps += 1\n",
        "\n",
        "            # Update sampling range\n",
        "            record_range = [x for x in record_range if x not in batch_indices]\n",
        "            #print()\n",
        "          actor_loss[-1] /= steps\n",
        "          critic_loss[-1] /= steps\n",
        "        return np.array(actor_loss).mean(), np.array(critic_loss).mean()\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tf.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rQmP0sOft_N"
      },
      "source": [
        "### Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmOM2fho8A6g"
      },
      "outputs": [],
      "source": [
        "def policy(state, noise_object):\n",
        "    sampled_actions = tf.squeeze(actor_model(state))\n",
        "    if noise_object is not None and ep >= TRAIN_BUFFER:\n",
        "      noise = noise_object()\n",
        "      # Adding noise to action\n",
        "      sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    return np.array([np.squeeze(sampled_actions)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeFp6ABvfxcg"
      },
      "source": [
        "### Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WARjKfhpT1Ah"
      },
      "outputs": [],
      "source": [
        "def sentences_bert(sentence):\n",
        "  sentence = list(sentence) + [0] * (max_seq_length - len(sentence))\n",
        "  sentence = np.array(sentence[:max_seq_length])\n",
        "  sentence = \" \".join(tokenizer.convert_ids_to_tokens([sentence]))\n",
        "  return sentence\n",
        "\n",
        "\n",
        "if os.path.exists(start_scores):\n",
        "  with open(start_scores, \"rb\") as f:\n",
        "    score_predictions = pickle.load(f)\n",
        "else:\n",
        "  score_predictions = bert_model_quality.predict(np.array([sentences_bert(x) for x in x_train_rl]), batch_size=256, verbose=1)\n",
        "  with open(start_scores, \"wb\") as f:\n",
        "    pickle.dump(score_predictions, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlDv8coCg73S"
      },
      "outputs": [],
      "source": [
        "def r4_reward(sentence):\n",
        "  sentence_w0 = [i for i in sentence if i!=0]  # remove the zeros in the list\n",
        "  unique_tokens = np.unique(np.array(sentence_w0))  # scroll the list according to the unique elements, in such a way I do not need to remove each elem everytime\n",
        "  n_repetead_tokens = 0\n",
        "\n",
        "  for elem in unique_tokens:\n",
        "    repetitions = sentence_w0.count(elem)  # count the repetitions for the elem\n",
        "\n",
        "    if repetitions > 1:  # if I have more than 1 repetitions\n",
        "      n_repetead_tokens += 1  # increase the counter\n",
        "  \n",
        "  r4 = 1 - (n_repetead_tokens/len(unique_tokens))\n",
        "  return r4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#del model_perplexity\n",
        "#import gc\n",
        "#gc.collect()"
      ],
      "metadata": {
        "id": "TTWWbteFnqf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_perplexity = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
        "model_perplexity.load_state_dict(torch.load(\"/gdrive/MyDrive/NLP/model_perplexity_best.th\"))\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer_perplexity = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
        "# Alpha equal to max of ppl\n",
        "alpha = 70000\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model_perplexity.eval()\n",
        "model_perplexity.to(device)\n",
        "\n",
        "def perplexity_score(sentence):\n",
        "    tokenize_input = tokenizer_perplexity.tokenize(sentence)\n",
        "    tensor_input = torch.tensor([tokenizer_perplexity.convert_tokens_to_ids(tokenize_input)])\n",
        "    tensor_input = tensor_input.to(device)\n",
        "    loss=model_perplexity(tensor_input, lm_labels=tensor_input)\n",
        "    score = ((alpha-math.exp(loss))/alpha)**3\n",
        "    return np.clip(score, -1.0, 1.0)"
      ],
      "metadata": {
        "id": "gPZgJUKWuIY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eq_sentence(sen1, sen2):\n",
        "  return (len(sen1)-np.count_nonzero(sen1-sen2))/len(sen1)"
      ],
      "metadata": {
        "id": "6N-qN8VHoT7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r_quality_list, r4_list, r1_list, r3_list = [], [], [], []"
      ],
      "metadata": {
        "id": "R5TD27buLCsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFn8mbbrCf0A"
      },
      "outputs": [],
      "source": [
        "class Env:\n",
        "  def __init__(self, tokenizer, max_seq_length, train_dataset, train_dataset_topics, \n",
        "               evaluator_quality, evaluator_r1, score_predictions):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.start_sentence = random.choice(train_dataset)\n",
        "    self.train_dataset = train_dataset\n",
        "    self.prev_sentence = []\n",
        "    self.evaluator_quality = evaluator_quality\n",
        "    self.evaluator_r1 = evaluator_r1\n",
        "    self.train_dataset = [list(self.train_dataset)]\n",
        "    self.train_dataset.append([])\n",
        "    self.train_dataset[1] = score_predictions\n",
        "    self.train_dataset_topics = train_dataset_topics\n",
        "      \n",
        "  def get_reward(self, new_sentence):\n",
        "    # Padding new sentence\n",
        "    sentence_r = new_sentence[1:] if new_sentence[1] == self.tokenizer.vocab[\"[start]\"] else new_sentence\n",
        "    oov_token_np = np.where(np.array(sentence_r)==self.tokenizer.vocab[\"OOV_TOKEN\"])[0]\n",
        "    end_token_np = np.where(np.array(sentence_r)==self.tokenizer.vocab[\"[end]\"])[0]\n",
        "    if len(oov_token_np)>0 and len(end_token_np)>0:\n",
        "      token_np = np.min([oov_token_np[0], end_token_np[0]])\n",
        "    elif len(oov_token_np)>0 and not len(end_token_np)>0:\n",
        "      token_np =oov_token_np[0]\n",
        "    elif not len(oov_token_np)>0 and len(end_token_np)>0:\n",
        "      token_np = end_token_np[0]\n",
        "    else:\n",
        "      token_np = len(sentence_r)\n",
        "    sentence_r = sentence_r[:token_np]\n",
        "    sentence_r = sentence_r + [0] * (self.max_seq_length - len(sentence_r))\n",
        "    sentence_r = np.array(sentence_r[:self.max_seq_length])\n",
        "\n",
        "    bert_sentence = \" \".join(tokenizer.convert_ids_to_tokens([sentence_r]))\n",
        "    oov_index = bert_sentence.find(\"OOV_TOKEN\")\n",
        "    bert_sentence = bert_sentence[:oov_index]\n",
        "    bert_sentence = bert_sentence.strip()\n",
        "\n",
        "    if len(bert_sentence) >= 1:\n",
        "      print(\"bert_sentence: \", bert_sentence)\n",
        "      \n",
        "      topic_sentence = bert_sentence + \". \" + self.topic\n",
        "      \n",
        "      with torch.no_grad():\n",
        "          r3 = perplexity_score(bert_sentence)\n",
        "          r1_scores = self.evaluator_r1(np.array([topic_sentence]))[0]\n",
        "          new_score = self.evaluator_quality(np.array([bert_sentence]))\n",
        "\n",
        "      r_quality = new_score - self.score_start_sentence\n",
        "      r1 = r1_scores[0] - 1.0 * r1_scores[1]\n",
        "      r4 = r4_reward(sentence_r)\n",
        "      \n",
        "      print(0.0* r_quality.numpy()[0][0], \" \", 0.0 * r4, \" \", 0.0 * r1.numpy(), \" \", 1.0 * r3)\n",
        "      reward = 0.0 * r_quality.numpy()[0][0] + 0.0 * r4 + 0.0 * r1.numpy() + 1.0 * r3\n",
        "      # reward = (len(sentence_r[1:-1])-np.count_nonzero(sentence_r[1:-1]-self.start_sentence))/len(sentence_r[1:-1])\n",
        "\n",
        "      r_quality_list.append(r_quality)\n",
        "      r4_list.append(r4)\n",
        "      r1_list.append(r1)\n",
        "      r3_list.append(r3)\n",
        "    else:\n",
        "      reward = -1.0\n",
        "\n",
        "    return reward\n",
        "\n",
        "  def reset(self):\n",
        "    id = random.randint(0, len(self.train_dataset[0])-1)\n",
        "    self.start_sentence = self.train_dataset[0][id]\n",
        "    self.topic = self.train_dataset_topics[id]\n",
        "    self.score_start_sentence = self.train_dataset[1][id]\n",
        "\n",
        "    if len(self.start_sentence) >= self.max_seq_length:\n",
        "      self.start_sentence = np.array(self.start_sentence[:max_seq_length-1])\n",
        "\n",
        "    if len(self.start_sentence) <= self.max_seq_length-2:\n",
        "      start_sentence = [self.tokenizer.vocab[\"[start]\"]] \n",
        "      start_sentence.extend(self.start_sentence)\n",
        "      start_sentence.extend([self.tokenizer.vocab[\"[end]\"]])\n",
        "      start_sentence = list(start_sentence) + [0] * (max_seq_length - len(start_sentence))\n",
        "      start_sentence = np.array(start_sentence[:max_seq_length])\n",
        "    elif len(self.start_sentence) <= self.max_seq_length-1:\n",
        "      start_sentence = [self.tokenizer.vocab[\"[start]\"]] \n",
        "      start_sentence.extend(self.start_sentence)\n",
        "      start_sentence[-1] = self.tokenizer.vocab[\"[end]\"]\n",
        "\n",
        "    return start_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RekHwaR1fzab"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrOq7ibm8F3q"
      },
      "outputs": [],
      "source": [
        "std_dev = 0.01\n",
        "ou_noise = OUActionNoise(mean=np.zeros((max_seq_length, VOCAB_SIZE+1)), \n",
        "                         std_deviation=float(std_dev) * np.ones((max_seq_length, VOCAB_SIZE+1)), \n",
        "                         gaussian=True)\n",
        "\n",
        "fnet.load_weights(\"/gdrive/MyDrive/NLP/pretrained_fnet_one_step.h5\")\n",
        "actor_model = create_actor(max_seq_length)\n",
        "critic_model = create_critic(max_seq_length)\n",
        "\n",
        "target_critic = create_critic(max_seq_length)\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 3e-4\n",
        "actor_lr = 3e-4\n",
        "\n",
        "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "# Train frequency in episodes\n",
        "train_freq = 4\n",
        "\n",
        "# Plot frequency in episodes\n",
        "plot_freq = 10\n",
        "# Number of episodes\n",
        "total_episodes = 16000\n",
        "\n",
        "tau = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deleting datasets already used"
      ],
      "metadata": {
        "id": "ON-aY-IcaojF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del pretrain_series\n",
        "del df"
      ],
      "metadata": {
        "id": "_-TciJVCaoD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0reOJ38gFzJo"
      },
      "outputs": [],
      "source": [
        "#del buffer\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtqJ-YFG8IyU"
      },
      "outputs": [],
      "source": [
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "# To store average actor loss\n",
        "avg_actor_list = []\n",
        "# To store average critic loss\n",
        "avg_critic_list = []\n",
        "\n",
        "buffer = Buffer(BUFFER_SIZE, BATCH_SIZE_RL, 1, TRAIN_BUFFER)\n",
        "env = Env(tokenizer, max_seq_length, x_train_rl, topic_list, bert_model_quality, \n",
        "          bert_model_r1, score_predictions)\n",
        "\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    start_sentence = env.reset()\n",
        "    prev_state_tr = tf.expand_dims(tf.convert_to_tensor(start_sentence), 0)\n",
        "\n",
        "    episodic_reward = 0\n",
        "\n",
        "    if np.random.randint(0, 100) <= 20:\n",
        "      new_sentence_softmax = policy(prev_state_tr, ou_noise)\n",
        "    else:\n",
        "      new_sentence_softmax = policy(prev_state_tr, None)\n",
        "    new_sentence = list(tf.argmax(new_sentence_softmax[0, :, :], axis=1).numpy())\n",
        "    # Receive state and reward from environment.\n",
        "    reward = env.get_reward(new_sentence)\n",
        "    tf_prev_state_2 = tf.expand_dims(tf.convert_to_tensor(new_sentence_softmax), 0)\n",
        "\n",
        "    buffer.record((prev_state_tr, reward, tf_prev_state_2))\n",
        "\n",
        "    ep_reward_list.append(reward)\n",
        "\n",
        "    print([tokenizer.tokenizer.index_word[x] for x in start_sentence if x != 0])\n",
        "    print([tokenizer.tokenizer.index_word[x] for x in new_sentence if x != 0])\n",
        "\n",
        "    # Mean of last 10 episodes\n",
        "    avg_reward = np.mean(ep_reward_list[-10:])\n",
        "    print(\"Episode * {} * Avg Reward is ==> {} * Episode Reward is ==> {}\".format(ep, avg_reward, reward))\n",
        "    print()\n",
        "    avg_reward_list.append(avg_reward)\n",
        "\n",
        "    # Training\n",
        "    if ep % train_freq == 0 and ep != 0 and buffer.buffer_counter >= 2*TRAIN_BUFFER:\n",
        "      print(\"Training\")\n",
        "      actor_loss_tmp, critic_loss_tmp = buffer.learn()\n",
        "      avg_actor_list.append(actor_loss_tmp)\n",
        "      avg_critic_list.append(critic_loss_tmp)\n",
        "      update_target(target_critic.variables, critic_model.variables, tau)\n",
        "      print(\"Actor Loss: \", actor_loss_tmp)\n",
        "      print(\"Critic Loss: \", critic_loss_tmp)\n",
        "      print()\n",
        "\n",
        "    if ep != 0 and ep % plot_freq == 0 and buffer.buffer_counter >= 2*TRAIN_BUFFER:\n",
        "      # Plotting graph\n",
        "      # Episodes versus Avg. Rewards\n",
        "      plt.plot(avg_reward_list)\n",
        "      plt.xlabel(\"Episode\")\n",
        "      plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "      plt.show()\n",
        "      # Episodes versus Avg. actor loss\n",
        "      plt.plot(avg_actor_list)\n",
        "      plt.xlabel(\"Episode\")\n",
        "      plt.ylabel(\"Avg. Actor Loss\")\n",
        "      plt.show()\n",
        "      # Episodes versus Avg. critic loss\n",
        "      plt.plot(avg_critic_list)\n",
        "      plt.xlabel(\"Episode\")\n",
        "      plt.ylabel(\"Avg. Critic Loss\")\n",
        "      plt.show()\n",
        " \n",
        "    \n",
        "plt.plot(avg_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()\n",
        "plt.plot(avg_actor_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Actor Loss\")\n",
        "plt.show()\n",
        "plt.plot(avg_critic_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Critic Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_batch = tf.convert_to_tensor(buffer.state_buffer[np.array([-2, -1])])\n",
        "next_state_batch = tf.convert_to_tensor(buffer.next_state_buffer[np.array([-2, -1])])\n",
        "next_state_batch = tf.cast(next_state_batch, dtype=tf.float32)\n",
        "critic_model([state_batch, next_state_batch])"
      ],
      "metadata": {
        "id": "kGJP5YnCDFZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "update_target(target_critic.variables, critic_model.variables, 0.2)"
      ],
      "metadata": {
        "id": "s2Ttx-71EJNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = actor_model(state_batch, training=False)"
      ],
      "metadata": {
        "id": "gusGKMPsDqjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_critic([state_batch, a])"
      ],
      "metadata": {
        "id": "9-y5I2ujDw30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "id": "qutptYWuDvk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_state_batch"
      ],
      "metadata": {
        "id": "YOnSfDq2Dnvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5q7qCv4vP59"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdh92KOZvULc"
      },
      "outputs": [],
      "source": [
        "assert False\n",
        "fnet.save_weights(\"/gdrive/MyDrive/NLP/final_fnet_one_step_00_00_00_1_cc.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TA02AdRk-ww"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3EZOYd2k-wz"
      },
      "outputs": [],
      "source": [
        "assert False\n",
        "fnet.load_weights(\"/gdrive/MyDrive/NLP/final_fnet.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMBTpBSySLOE"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX14tD14crA6"
      },
      "outputs": [],
      "source": [
        "sentence = \"marriage isn't keeping up with the times. abandon the old thinking and bring something that incorporates all unions not just those with a man and woman.\"\n",
        "out = decode_sentence(sentence, preprocess_rl)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Degeneration analysis"
      ],
      "metadata": {
        "id": "EUMizu4kV-Be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.array(r_quality_list).ravel())\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Quality Score\")\n",
        "plt.show()\n",
        "plt.plot(r4_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Words Repetion\")\n",
        "plt.show()\n",
        "plt.plot(r1_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Topic Score\")\n",
        "plt.show()\n",
        "plt.plot(r3_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Syntantical Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xiFelkmbNeN9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copia_di_seq2seq_arg_quality_one_step.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
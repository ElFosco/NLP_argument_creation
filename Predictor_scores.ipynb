{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElFosco/NLP_score/blob/Fosco/Predictor_scores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrxfQlV-7lbr"
      },
      "source": [
        "#Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHiGJ5Ou8Mwo"
      },
      "outputs": [],
      "source": [
        "pip install -q tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-VGaZQA8ANi",
        "outputId": "3202ca77-a7c2-480c-c508-cf2fee4f5895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (2.7.3)\n",
            "Requirement already satisfied: tensorflow<2.8,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (2.7.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (2.7.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (1.19.5)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (2.7.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (1.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (3.10.0.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (1.43.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (0.23.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (12.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.8,>=2.7.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmPHpfi8AkgA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import keras\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE_to76fAVf7"
      },
      "source": [
        "#Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn6dyvb0AzdK",
        "outputId": "58173798-e8b8-4597-8156-88dfde4c70c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Using google drive to upload the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#dir_path = \"drive/MyDrive/NLP_project/Datasets/\"\n",
        "dir_path = \"drive/MyDrive/Magistrale/NLP/Project/Data/\"\n",
        "dataset = \"arg_quality_rank_30k.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ca1VdgDoAURT",
        "outputId": "71b41da8-e59b-4bb4-ac6c-859fef02258d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-378648a9-cd38-4059-9910-8603a1f9c93f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>argument</th>\n",
              "      <th>topic</th>\n",
              "      <th>set</th>\n",
              "      <th>WA</th>\n",
              "      <th>MACE-P</th>\n",
              "      <th>stance_WA</th>\n",
              "      <th>stance_WA_conf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"marriage\" isn't keeping up with the times.  a...</td>\n",
              "      <td>We should abandon marriage</td>\n",
              "      <td>train</td>\n",
              "      <td>0.846165</td>\n",
              "      <td>0.297659</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>.a multi-party system would be too confusing a...</td>\n",
              "      <td>We should adopt a multi-party system</td>\n",
              "      <td>train</td>\n",
              "      <td>0.891271</td>\n",
              "      <td>0.726133</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\ero-tolerance policy in schools should not be...</td>\n",
              "      <td>We should adopt a zero-tolerance policy in sch...</td>\n",
              "      <td>dev</td>\n",
              "      <td>0.721192</td>\n",
              "      <td>0.396953</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>`people reach their limit when it comes to the...</td>\n",
              "      <td>Assisted suicide should be a criminal offence</td>\n",
              "      <td>train</td>\n",
              "      <td>0.730395</td>\n",
              "      <td>0.225212</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100% agree, should they do that, it would be a...</td>\n",
              "      <td>We should abolish safe spaces</td>\n",
              "      <td>train</td>\n",
              "      <td>0.236686</td>\n",
              "      <td>0.004104</td>\n",
              "      <td>1</td>\n",
              "      <td>0.805517</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-378648a9-cd38-4059-9910-8603a1f9c93f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-378648a9-cd38-4059-9910-8603a1f9c93f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-378648a9-cd38-4059-9910-8603a1f9c93f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            argument  ... stance_WA_conf\n",
              "0  \"marriage\" isn't keeping up with the times.  a...  ...       1.000000\n",
              "1  .a multi-party system would be too confusing a...  ...       1.000000\n",
              "2  \\ero-tolerance policy in schools should not be...  ...       1.000000\n",
              "3  `people reach their limit when it comes to the...  ...       1.000000\n",
              "4  100% agree, should they do that, it would be a...  ...       0.805517\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df = pd.read_csv(dir_path + dataset)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HC-FCfELNYo"
      },
      "outputs": [],
      "source": [
        "set_topic = df.topic.unique()\n",
        "dict_topic = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKs69SnCMKff",
        "outputId": "c0e3b361-a656-4de9-c24f-78e17b4f4feb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We should fight for the abolition of nuclear weapons', 554),\n",
              " ('We should legalize cannabis', 548),\n",
              " ('We should ban naturopathy', 540),\n",
              " ('Foster care brings more harm than good', 538),\n",
              " ('Blockade of the Gaza Strip should be ended', 521),\n",
              " ('We should legalize prostitution', 504),\n",
              " ('We should ban cosmetic surgery for minors', 502),\n",
              " ('We should legalize polygamy', 500),\n",
              " ('We should abolish the three-strikes laws', 499),\n",
              " ('We should end mandatory retirement', 484),\n",
              " ('We should abandon the use of school uniform', 480),\n",
              " ('Intelligence tests bring more harm than good', 472),\n",
              " ('We should abolish capital punishment', 470),\n",
              " ('Holocaust denial should be a criminal offence', 466),\n",
              " ('We should adopt a zero-tolerance policy in schools', 459),\n",
              " ('We should end affirmative action', 456),\n",
              " ('We should oppose collectivism', 454),\n",
              " ('We should close Guantanamo Bay detention camp', 447),\n",
              " ('Payday loans should be banned', 446),\n",
              " ('We should stop the development of autonomous cars', 446),\n",
              " ('We should ban telemarketing', 438),\n",
              " ('We should ban missionary work', 438),\n",
              " ('We should ban the use of child actors', 436),\n",
              " ('Surrogacy should be banned', 435),\n",
              " ('We should abolish intellectual property rights', 432),\n",
              " ('The vow of celibacy should be abandoned', 432),\n",
              " ('We should subsidize vocational education', 431),\n",
              " ('We should ban cosmetic surgery', 430),\n",
              " ('We should prohibit flag burning', 428),\n",
              " ('We should ban whaling', 428),\n",
              " ('The use of public defenders should be mandatory', 427),\n",
              " ('We should prohibit school prayer', 427),\n",
              " ('We should adopt an austerity regime', 424),\n",
              " ('We should ban human cloning', 423),\n",
              " ('We should ban fast food', 422),\n",
              " ('We should abandon marriage', 421),\n",
              " ('We should end racial profiling', 418),\n",
              " ('We should abolish the right to keep and bear arms', 415),\n",
              " ('We should ban factory farming', 414),\n",
              " ('We should ban targeted killing', 414),\n",
              " ('We should legalize organ trade', 413),\n",
              " ('We should abolish the Olympic Games', 410),\n",
              " ('We should ban the Church of Scientology', 407),\n",
              " ('We should introduce compulsory voting', 406),\n",
              " ('Entrapment should be legalized', 406),\n",
              " ('We should legalize sex selection', 405),\n",
              " ('We should fight urbanization', 405),\n",
              " ('We should subsidize stay-at-home dads', 405),\n",
              " ('We should abolish safe spaces', 404),\n",
              " ('We should adopt a multi-party system', 402),\n",
              " ('We should limit judicial activism', 402),\n",
              " ('We should adopt libertarianism', 400),\n",
              " ('We should subsidize Wikipedia', 400),\n",
              " ('We should end the use of economic sanctions', 400),\n",
              " ('We should subsidize embryonic stem cell research', 400),\n",
              " ('We should cancel pride parades', 399),\n",
              " ('We should ban private military companies', 397),\n",
              " ('Assisted suicide should be a criminal offence', 396),\n",
              " ('Homeschooling should be banned', 395),\n",
              " ('We should abolish zoos', 395),\n",
              " ('We should ban algorithmic trading', 390),\n",
              " ('We should abandon television', 389),\n",
              " ('We should subsidize space exploration', 388),\n",
              " ('We should subsidize student loans', 386),\n",
              " ('We should adopt gender-neutral language', 386),\n",
              " ('We should limit executive compensation', 381),\n",
              " ('We should adopt atheism', 379),\n",
              " ('We should prohibit women in combat', 375),\n",
              " ('We should subsidize journalism', 368),\n",
              " ('Homeopathy brings more harm than good', 358),\n",
              " ('Social media brings more harm than good', 331)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "for i in set_topic:\n",
        "  dict_topic[i] = df.loc[i==df['topic'],'topic'].values.size\n",
        "sorted(dict_topic.items(), key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV1gnJwQNIZd"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  text = re.sub('\\\"|-|\\\\\\\\|`',' ',text) #delete this chars from the string [\"-\\`]\n",
        "  text = re.sub('\\n', ' ',text)\n",
        "  text = re.sub('^[.]+','',text)        #delete dots at the beginning of the sentence\n",
        "  text = re.sub('\\. \\.','.',text)       #delete . .\n",
        "  text = re.sub('&',' and ',text)       #replace & with and\n",
        "  text = re.sub(' +', ' ',text)         #delete additional whitespace\n",
        "  text = text.rstrip()                  \n",
        "  text = text.lstrip()\n",
        "  return text"
      ],
      "metadata": {
        "id": "nBRUbfFqwIyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['argument'] = df.apply(lambda row : clean_text(row['argument']), axis = 1)\n",
        "df.loc[2,\"argument\"] = \"zero tolerance policy in schools should not be adopted as circumstances are often not black and white, being more nuanced. no one should be written off due to a mistake of judgement.\""
      ],
      "metadata": {
        "id": "J6314Kqgy-bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQfS-UzU6QXS"
      },
      "source": [
        "##Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A82m4_1K5pfv"
      },
      "outputs": [],
      "source": [
        "is_training_data =  df['set']=='train'\n",
        "is_validation_data =  df['set']=='dev'\n",
        "is_test_data =  df['set']=='test'\n",
        "\n",
        "training_data = df[is_training_data]\n",
        "validation_data = df[is_validation_data]\n",
        "test_data  = df[is_test_data ]\n",
        "\n",
        "x_train = training_data['argument']\n",
        "Y_train = training_data['MACE-P']\n",
        "\n",
        "x_val = validation_data['argument']\n",
        "Y_val = validation_data['MACE-P']\n",
        "\n",
        "x_test = test_data['argument']\n",
        "Y_test = test_data['MACE-P']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs14cTT92k-l",
        "outputId": "1e28f7f6-c00b-436f-92cf-9672d1a7ee6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20974,)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6PDqt2Kfz_T"
      },
      "source": [
        "#[Bert](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8_ctG55-uTX",
        "outputId": "6b23fb8e-a2ae-454a-f84b-b49eb4023e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ],
      "source": [
        "#@title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'  #@param [\"bert_en_uncased_L-24_H-1024_A-16\",\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gzql3TDrBtg-"
      },
      "outputs": [],
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pduw5iP9Dgz_"
      },
      "outputs": [],
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdhOVzGzt1uc",
        "outputId": "47c8d70a-d74b-4706-d138-12c4e2b76338"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
              "array([[-1.9476332e-02, -1.6363966e-01,  3.2493129e-02, ...,\n",
              "        -3.2246384e-01, -1.0321505e-01,  2.7680629e-01],\n",
              "       [-3.0853543e-02, -3.3630687e-01, -1.6466528e-04, ...,\n",
              "        -4.8734567e-01,  5.7031804e-01,  4.6865925e-01]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "def get_sentence_embeding(sentences):\n",
        "    preprocessed_text = bert_preprocess_model(sentences)\n",
        "    return bert_model(preprocessed_text)['encoder_outputs'][-1][:,0,:]\n",
        "\n",
        "get_sentence_embeding([\n",
        "    \"500$ discount. hurry up\", \n",
        "    \"Bhavin, are you up for a volleybal game tomorrow?\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWN6VYotHCw9"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model(dense_size=100):\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dense(dense_size, activation=keras.activations.relu, name='fc')(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=keras.activations.sigmoid, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_pX1FzsK2uj"
      },
      "outputs": [],
      "source": [
        "classifier_model = build_classifier_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAijDE-crAt4",
        "outputId": "de2d0d81-7512-48bc-8f76-7462ed90d007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " preprocessing (KerasLayer)     {'input_word_ids':   0           ['text[0][0]']                   \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_type_ids':                                                \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_mask': (Non                                               \n",
            "                                e, 128)}                                                          \n",
            "                                                                                                  \n",
            " BERT_encoder (KerasLayer)      {'encoder_outputs':  109482241   ['preprocessing[0][0]',          \n",
            "                                 [(None, 128, 768),               'preprocessing[0][1]',          \n",
            "                                 (None, 128, 768),                'preprocessing[0][2]']          \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)],                                               \n",
            "                                 'sequence_output':                                               \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 'default': (None,                                                \n",
            "                                768),                                                             \n",
            "                                 'pooled_output': (                                               \n",
            "                                None, 768)}                                                       \n",
            "                                                                                                  \n",
            " fc (Dense)                     (None, 100)          76900       ['BERT_encoder[0][13]']          \n",
            "                                                                                                  \n",
            " classifier (Dense)             (None, 1)            101         ['fc[0][0]']                     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,559,242\n",
            "Trainable params: 109,559,241\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "classifier_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L4k5p-bIdyq"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def pearson_loss(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x, axis=0)\n",
        "    my = K.mean(y, axis=0)\n",
        "    xm, ym = x - mx, y - my\n",
        "    r_num = K.sum(xm * ym)\n",
        "    x_square_sum = K.sum(xm * xm)\n",
        "    y_square_sum = K.sum(ym * ym)\n",
        "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
        "    r = -r_num / r_den\n",
        "    return K.mean(r)\n",
        "\n",
        "def pearson_metric(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x, axis=0)\n",
        "    my = K.mean(y, axis=0)\n",
        "    xm, ym = x - mx, y - my\n",
        "    r_num = K.sum(xm * ym)\n",
        "    x_square_sum = K.sum(xm * xm)\n",
        "    y_square_sum = K.sum(ym * ym)\n",
        "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
        "    r = r_num / r_den\n",
        "    return K.mean(r)\n",
        "\n",
        "loss = pearson_loss\n",
        "loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "metric_pearson = pearson_metric\n",
        "metric_mse = tf.keras.metrics.MeanSquaredError()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpXy0Zm7JnbJ"
      },
      "outputs": [],
      "source": [
        "epochs = 1\n",
        "batch_size = 64\n",
        "steps_per_epoch = x_train.shape[0] / batch_size \n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(epochs * x_train.shape[0] * 0.1 / batch_size)\n",
        "\n",
        "###solution 1\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jI_L17okKvNf"
      },
      "outputs": [],
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=[metric_pearson,metric_mse])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9rUCXRzLhmL",
        "outputId": "f2c40cd5-5a20-420e-ad18-e584f05fb8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n",
            "656/656 [==============================] - 608s 903ms/step - loss: -0.4881 - pearson_metric: 0.4882 - mean_squared_error: 0.1059\n"
          ]
        }
      ],
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=x_train,y=Y_train,epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjGrtWz6Iu8L",
        "outputId": "72478080-1f1b-4ab9-bad3-7e94412c56b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "198/198 [==============================] - 66s 330ms/step - loss: -0.4372 - pearson_metric: 0.4366 - mean_squared_error: 0.1081\n"
          ]
        }
      ],
      "source": [
        "loss, metric_1,metric_2 = classifier_model.evaluate(x=x_test, y=Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcXmtg26cEAG",
        "outputId": "27583a03-6925-43a4-946a-b2b201455ac1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.1080745750293189"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_predicted = classifier_model.predict(x_test)\n",
        "ris = pd.DataFrame(\n",
        "    {'x_test': x_test,\n",
        "     'Y_test': Y_test,\n",
        "     'Y_predicted': list(Y_predicted)\n",
        "    })\n",
        "ris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a07yUREWKK8s"
      },
      "source": [
        "#Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ7fM7G1KNJK",
        "outputId": "06978bab-8a9e-4abd-9728-d49f6ebaa161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  <function pearson_loss at 0x7ff0c64ca320>\n",
            " Epochs:  2\n",
            "  Start Learning Rate:  3e-05\n",
            "   Batch Size:  32\n",
            "    Dense size:  100\n",
            "Epoch 1/2\n",
            "656/656 [==============================] - 619s 914ms/step - loss: -0.4402 - pearson_metric: 0.4399 - mean_squared_error: 0.1235\n",
            "Epoch 2/2\n",
            "656/656 [==============================] - 599s 912ms/step - loss: -0.6259 - pearson_metric: 0.6256 - mean_squared_error: 0.0930\n",
            "101/101 [==============================] - 36s 345ms/step - loss: -0.4874 - pearson_metric: 0.4870 - mean_squared_error: 0.1132\n",
            "     Pearson:  0.4870048463344574\n",
            "     MSE:  0.11317948251962662\n",
            "    Dense size:  200\n",
            "Epoch 1/2\n",
            "656/656 [==============================] - 620s 913ms/step - loss: -0.4510 - pearson_metric: 0.4513 - mean_squared_error: 0.1271\n",
            "Epoch 2/2\n",
            "656/656 [==============================] - 599s 913ms/step - loss: -0.6359 - pearson_metric: 0.6356 - mean_squared_error: 0.0882\n",
            "101/101 [==============================] - 37s 354ms/step - loss: -0.4929 - pearson_metric: 0.4916 - mean_squared_error: 0.1085\n",
            "     Pearson:  0.49160316586494446\n",
            "     MSE:  0.10852888971567154\n",
            "    Dense size:  300\n",
            "Epoch 1/2\n",
            "656/656 [==============================] - 615s 912ms/step - loss: -0.4422 - pearson_metric: 0.4422 - mean_squared_error: 0.1237\n",
            "Epoch 2/2\n",
            "656/656 [==============================] - 597s 910ms/step - loss: -0.6409 - pearson_metric: 0.6408 - mean_squared_error: 0.0890\n",
            "101/101 [==============================] - 36s 348ms/step - loss: -0.4915 - pearson_metric: 0.4893 - mean_squared_error: 0.1119\n",
            "     Pearson:  0.4893450140953064\n",
            "     MSE:  0.11191451549530029\n",
            "-1\n",
            "{'epochs': 2, 'batch_size': 32, 'start_lr': 3e-05, 'dense_size': 300, 'loss': -0.4914795458316803}\n"
          ]
        }
      ],
      "source": [
        "parameters = {'epochs': [1,2,3], \n",
        "              'batch_size':[32],\n",
        "              'init_lr': [3e-6,3e-5],\n",
        "              'dense_size' : [100,200],\n",
        "              'loss' : [tf.keras.losses.MeanSquaredError(),pearson_loss]\n",
        "              }\n",
        "\n",
        "\n",
        "best_scores = -1\n",
        "best_params = {1: dict()}\n",
        "\n",
        "for loss in parameters['loss']:\n",
        "  print(\"Loss: \", loss)\n",
        "  for epochs in parameters['epochs']:\n",
        "    print(\" Epochs: \", epochs)\n",
        "    for init_lr in parameters['init_lr']:\n",
        "      print(\"  Start Learning Rate: \", init_lr)\n",
        "      for batch_size in parameters['batch_size']:\n",
        "        print(\"   Batch Size: \", batch_size)\n",
        "        for dense_size in parameters['dense_size']:\n",
        "          print(\"    Dense size: \", dense_size)\n",
        "          steps_per_epoch = x_train.shape[0] / batch_size \n",
        "          num_train_steps = steps_per_epoch * epochs\n",
        "          num_warmup_steps = int(epochs * x_train.shape[0] * 0.1 / batch_size)\n",
        "          optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                            num_train_steps=num_train_steps,\n",
        "                                            num_warmup_steps=num_warmup_steps,\n",
        "                                            optimizer_type='adamw')\n",
        "          classifier_model = build_classifier_model(dense_size)\n",
        "          classifier_model.compile(optimizer=optimizer,\n",
        "                          loss=loss,\n",
        "                          metrics=[metric_pearson,metric_mse])\n",
        "          history = classifier_model.fit(x=x_train,y=Y_train,batch_size=batch_size,epochs=epochs)\n",
        "          loss_calculated, pearson ,mse = classifier_model.evaluate(x=x_val, y=Y_val)\n",
        "          print(\"     Pearson: \", pearson)\n",
        "          print(\"     MSE: \", mse)\n",
        "          if pearson > best_scores:                 \n",
        "            best_score = pearson\n",
        "            best_params = {'epochs': epochs, \n",
        "                           'batch_size': batch_size, \n",
        "                           'start_lr': init_lr,  \n",
        "                           'dense_size':dense_size,\n",
        "                           'loss':loss_calculated}\n",
        "print(best_scores)\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#best parameter found on grid search\n",
        "parameters = {'epochs': 2, \n",
        "              'batch_size':32,\n",
        "              'init_lr': 3e-5,\n",
        "              'dense_size' : 100,\n",
        "              'loss' : pearson_loss\n",
        "              }\n",
        "\n",
        "epochs = parameters['epochs']\n",
        "batch_size = parameters['batch_size']\n",
        "init_lr = parameters['init_lr']\n",
        "dense_size = parameters['dense_size']\n",
        "loss = parameters['loss']\n",
        "\n",
        "steps_per_epoch = x_train.shape[0] / batch_size \n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(epochs * x_train.shape[0] * 0.1 / batch_size)\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                            num_train_steps=num_train_steps,\n",
        "                                            num_warmup_steps=num_warmup_steps,\n",
        "                                            optimizer_type='adamw')\n",
        "classifier_model = build_classifier_model(dense_size)\n",
        "classifier_model.compile(optimizer=optimizer,loss=loss,metrics=[metric_pearson,metric_mse])\n",
        "history = classifier_model.fit(x=x_train,y=Y_train,batch_size=batch_size,validation_data=(x_val,Y_val),epochs=epochs)\n",
        "loss_calculated, pearson ,mse = classifier_model.evaluate(x_test, Y_test)\n",
        "print(\"Pearson: \", pearson)\n",
        "print(\"MSE: \", mse)\n"
      ],
      "metadata": {
        "id": "s8DfZjrPIvy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.save(\"classifierIBM30k.h5\")"
      ],
      "metadata": {
        "id": "R5eU9VjBLV8T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Predictor_scores.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
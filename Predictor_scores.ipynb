{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predictor_scores.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElFosco/NLP_score/blob/Fosco/Predictor_scores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import"
      ],
      "metadata": {
        "id": "IrxfQlV-7lbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q tf-models-official"
      ],
      "metadata": {
        "id": "iHiGJ5Ou8Mwo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-VGaZQA8ANi",
        "outputId": "12c00ebd-0aae-42b0-fa92-5fc45ecdd011"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (2.7.3)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.8,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (2.7.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (12.0.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (1.13.3)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (2.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (3.10.0.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (1.43.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (2.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (0.23.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (2.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.8,>=2.7.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import keras\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n"
      ],
      "metadata": {
        "id": "EmPHpfi8AkgA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Exploration"
      ],
      "metadata": {
        "id": "xE_to76fAVf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using google drive to upload the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#dir_path = \"drive/MyDrive/NLP_project/Datasets/\"\n",
        "dir_path = \"drive/MyDrive/Magistrale/NLP/Project/Data/\"\n",
        "dataset = \"arg_quality_rank_30k.csv\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn6dyvb0AzdK",
        "outputId": "5026adc2-2adf-478f-bd75-5fc1a92b01f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ca1VdgDoAURT",
        "outputId": "da010260-8f1a-4301-80b4-7f08789977ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5bd88471-410b-4837-af5f-ded1999e9d7e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>argument</th>\n",
              "      <th>topic</th>\n",
              "      <th>set</th>\n",
              "      <th>WA</th>\n",
              "      <th>MACE-P</th>\n",
              "      <th>stance_WA</th>\n",
              "      <th>stance_WA_conf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"marriage\" isn't keeping up with the times.  a...</td>\n",
              "      <td>We should abandon marriage</td>\n",
              "      <td>train</td>\n",
              "      <td>0.846165</td>\n",
              "      <td>0.297659</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>.a multi-party system would be too confusing a...</td>\n",
              "      <td>We should adopt a multi-party system</td>\n",
              "      <td>train</td>\n",
              "      <td>0.891271</td>\n",
              "      <td>0.726133</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\ero-tolerance policy in schools should not be...</td>\n",
              "      <td>We should adopt a zero-tolerance policy in sch...</td>\n",
              "      <td>dev</td>\n",
              "      <td>0.721192</td>\n",
              "      <td>0.396953</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>`people reach their limit when it comes to the...</td>\n",
              "      <td>Assisted suicide should be a criminal offence</td>\n",
              "      <td>train</td>\n",
              "      <td>0.730395</td>\n",
              "      <td>0.225212</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100% agree, should they do that, it would be a...</td>\n",
              "      <td>We should abolish safe spaces</td>\n",
              "      <td>train</td>\n",
              "      <td>0.236686</td>\n",
              "      <td>0.004104</td>\n",
              "      <td>1</td>\n",
              "      <td>0.805517</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5bd88471-410b-4837-af5f-ded1999e9d7e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5bd88471-410b-4837-af5f-ded1999e9d7e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5bd88471-410b-4837-af5f-ded1999e9d7e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            argument  ... stance_WA_conf\n",
              "0  \"marriage\" isn't keeping up with the times.  a...  ...       1.000000\n",
              "1  .a multi-party system would be too confusing a...  ...       1.000000\n",
              "2  \\ero-tolerance policy in schools should not be...  ...       1.000000\n",
              "3  `people reach their limit when it comes to the...  ...       1.000000\n",
              "4  100% agree, should they do that, it would be a...  ...       0.805517\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df = pd.read_csv(dir_path + dataset)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_topic = df.topic.unique()\n",
        "dict_topic = {}"
      ],
      "metadata": {
        "id": "9HC-FCfELNYo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in set_topic:\n",
        "  dict_topic[i] = df.loc[i==df['topic'],'topic'].values.size\n",
        "sorted(dict_topic.items(), key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKs69SnCMKff",
        "outputId": "d04bf5c3-b1bd-466a-cc0d-b2bcb330b390"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We should fight for the abolition of nuclear weapons', 554),\n",
              " ('We should legalize cannabis', 548),\n",
              " ('We should ban naturopathy', 540),\n",
              " ('Foster care brings more harm than good', 538),\n",
              " ('Blockade of the Gaza Strip should be ended', 521),\n",
              " ('We should legalize prostitution', 504),\n",
              " ('We should ban cosmetic surgery for minors', 502),\n",
              " ('We should legalize polygamy', 500),\n",
              " ('We should abolish the three-strikes laws', 499),\n",
              " ('We should end mandatory retirement', 484),\n",
              " ('We should abandon the use of school uniform', 480),\n",
              " ('Intelligence tests bring more harm than good', 472),\n",
              " ('We should abolish capital punishment', 470),\n",
              " ('Holocaust denial should be a criminal offence', 466),\n",
              " ('We should adopt a zero-tolerance policy in schools', 459),\n",
              " ('We should end affirmative action', 456),\n",
              " ('We should oppose collectivism', 454),\n",
              " ('We should close Guantanamo Bay detention camp', 447),\n",
              " ('Payday loans should be banned', 446),\n",
              " ('We should stop the development of autonomous cars', 446),\n",
              " ('We should ban telemarketing', 438),\n",
              " ('We should ban missionary work', 438),\n",
              " ('We should ban the use of child actors', 436),\n",
              " ('Surrogacy should be banned', 435),\n",
              " ('We should abolish intellectual property rights', 432),\n",
              " ('The vow of celibacy should be abandoned', 432),\n",
              " ('We should subsidize vocational education', 431),\n",
              " ('We should ban cosmetic surgery', 430),\n",
              " ('We should prohibit flag burning', 428),\n",
              " ('We should ban whaling', 428),\n",
              " ('The use of public defenders should be mandatory', 427),\n",
              " ('We should prohibit school prayer', 427),\n",
              " ('We should adopt an austerity regime', 424),\n",
              " ('We should ban human cloning', 423),\n",
              " ('We should ban fast food', 422),\n",
              " ('We should abandon marriage', 421),\n",
              " ('We should end racial profiling', 418),\n",
              " ('We should abolish the right to keep and bear arms', 415),\n",
              " ('We should ban factory farming', 414),\n",
              " ('We should ban targeted killing', 414),\n",
              " ('We should legalize organ trade', 413),\n",
              " ('We should abolish the Olympic Games', 410),\n",
              " ('We should ban the Church of Scientology', 407),\n",
              " ('We should introduce compulsory voting', 406),\n",
              " ('Entrapment should be legalized', 406),\n",
              " ('We should legalize sex selection', 405),\n",
              " ('We should fight urbanization', 405),\n",
              " ('We should subsidize stay-at-home dads', 405),\n",
              " ('We should abolish safe spaces', 404),\n",
              " ('We should adopt a multi-party system', 402),\n",
              " ('We should limit judicial activism', 402),\n",
              " ('We should adopt libertarianism', 400),\n",
              " ('We should subsidize Wikipedia', 400),\n",
              " ('We should end the use of economic sanctions', 400),\n",
              " ('We should subsidize embryonic stem cell research', 400),\n",
              " ('We should cancel pride parades', 399),\n",
              " ('We should ban private military companies', 397),\n",
              " ('Assisted suicide should be a criminal offence', 396),\n",
              " ('Homeschooling should be banned', 395),\n",
              " ('We should abolish zoos', 395),\n",
              " ('We should ban algorithmic trading', 390),\n",
              " ('We should abandon television', 389),\n",
              " ('We should subsidize space exploration', 388),\n",
              " ('We should subsidize student loans', 386),\n",
              " ('We should adopt gender-neutral language', 386),\n",
              " ('We should limit executive compensation', 381),\n",
              " ('We should adopt atheism', 379),\n",
              " ('We should prohibit women in combat', 375),\n",
              " ('We should subsidize journalism', 368),\n",
              " ('Homeopathy brings more harm than good', 358),\n",
              " ('Social media brings more harm than good', 331)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "rV1gnJwQNIZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_sentence = \"^[a-zA-Z0-9_ ].*$\"  # \n",
        "pattern_sentence = re.compile(start_sentence)\n",
        "\n"
      ],
      "metadata": {
        "id": "FFH5IHTjO_ot"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows():\n",
        "   if pattern_sentence.match(row['argument']) is None:\n",
        "      print(index)\n",
        "      print(row['argument'])\n",
        "      print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BfwBTpgNINt",
        "outputId": "df255fc2-6e0b-4582-e01c-ebeec02692d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "\"marriage\" isn't keeping up with the times.  abandon the old thinking and bring something that incorporates all unions - not just those with a man and woman.\n",
            "\n",
            "\n",
            "1\n",
            ".a multi-party system would be too confusing and getting a consensus from the general public would be difficult.\n",
            "\n",
            "\n",
            "2\n",
            "\\ero-tolerance policy in schools should not be adopted as circumstances are often not black and white, being more nuanced. no one should be written off due to a mistake of judgement.\n",
            "\n",
            "\n",
            "3\n",
            "`people reach their limit when it comes to their quality of life and should be able to end their suffering.  this can be done with little or no suffering  by assistance and the person is able to say good bye.\n",
            "\n",
            "\n",
            "337\n",
            "a school has students of all types of religions and beliefs.  these beliefs encompass different entities to pray to.\n",
            "banning school prayer allows all students to worship as they choose.\n",
            "\n",
            "\n",
            "529\n",
            "abuse and bullying are rife among foster children\n",
            "it does not provide stability just temporary solutions\n",
            "\n",
            "\n",
            "707\n",
            "affirmative action gives extra support and privileged to groups of people that might have been held back in the past.\n",
            "but today people are no longer suppressed as before so no extra help should be needed.\n",
            "\n",
            "\n",
            "1338\n",
            "an autonomous car would be better able to make life saving decisions than a human\n",
            "there is no risk of drink driving or emotions affecting ability\n",
            "we could use the time we travel more productively\n",
            "\n",
            "\n",
            "1347\n",
            "an executive has worked hard to get to that position through study and prior work experience.\n",
            "they should be compensated for their value to the company.\n",
            "\n",
            "\n",
            "2343\n",
            "because not all actions are equal a zero tolerance policy can punish a minor infraction as severely as a major one. \n",
            "\n",
            "this can lead to students making worse choices since the punishment is the same.\n",
            "\n",
            "\n",
            "6781\n",
            "foster homes are often environments of bullying and abuse\n",
            "the lack of stability, unlike adoptions, is harmful\n",
            "evidence shows that foster children are more likely to get in trouble with the law\n",
            "\n",
            "\n",
            "10040\n",
            "it connects people from all over the world\n",
            "it allows crowd sourcing for charity work\n",
            "it can help people that are isolated to talk to others\n",
            "\n",
            "\n",
            "10192\n",
            "it is a crucial required service for orphans and abandoned children\n",
            "it can be a very rewarding environment for children that have very few options\n",
            "\n",
            "\n",
            "10292\n",
            "it is an effective deterrent\n",
            "it is not abuse, a slap on the wrist is an effective method of teaching your child\n",
            "it is for the parent to judge how best to raise their child\n",
            "\n",
            "\n",
            "10459\n",
            "it is inhumane and teaches that violence solves violence\n",
            "until the justice system is 100% correct all the time, we cannot risk killing someone that may later be proven innocent\n",
            "\n",
            "\n",
            "10461\n",
            "it is inhumane and unnecessary\n",
            "it teaches that violence is a solution\n",
            "it can open the door to abuse\n",
            "\n",
            "\n",
            "10629\n",
            "it is the future of car travel to allow for more efficient fuel use\n",
            "they are much safer due to the lack of human error\n",
            "the roads will have far less congestion due to planned smart a.i. driving\n",
            "\n",
            "\n",
            "10633\n",
            "it is the most effective deterrent to the worst crimes\n",
            "it is a waste of hard working people's money to pay for someone that committed atrocious crimes, to stay in a prison for the rest of their lives\n",
            "\n",
            "\n",
            "10807\n",
            "it provides a necessary function for abandoned children\n",
            "many foster children are well adapted and happy\n",
            "where would they go otherwise? orphanages are surely a far worst option\n",
            "\n",
            "\n",
            "10947\n",
            "it teaches the young to hide behind a screen and speak without consequence\n",
            "it is impossible to police the social media sites to prevent bullying\n",
            "many cases of youth suicide have been due to social media\n",
            "\n",
            "\n",
            "13689\n",
            "naturopathy has no scientific basis. within the medical profession it is even viewed as harmful and pseudoscientific.\n",
            "those that practice naturopathy can be  perceived as quacks.\n",
            "\n",
            "\n",
            "13903\n",
            "no matter the reason for doing missionary work,\n",
            "missionary work does have the ability to directly benefit those visited\n",
            "\n",
            "\n",
            "14962\n",
            "people are sold a fraudulent lie of products that do not help them\n",
            "it can prevent people using genuine effective treatments\n",
            "a waste of money at best, the cause of death at worst\n",
            "\n",
            "\n",
            "14966\n",
            "people are tricked into thinking they are treating an illness\n",
            "sellers are gaining from fraud\n",
            "people do not get helpful legitimate treatment\n",
            "\n",
            "\n",
            "16781\n",
            "public defense is given by its obligation to assist\n",
            "legally to the person prosecuted criminally, with total orientation to the interests of the client\n",
            "and ensuring the provision of quality service\n",
            "\n",
            "\n",
            "17820\n",
            "Schools now contain students of all religions and of none, by forcing these students to pray in schools you are infringing on their human rights, prayer is personal not public in school.\n",
            ".\n",
            "\n",
            "\n",
            "18933\n",
            "spending time with friends and checking their pictures and their kids pictures and staying connected with them.\n",
            "getting latest games, movies and previous on social media as well.\n",
            "\n",
            "\n",
            "21301\n",
            "the gaza strip blockade hurts people from going about their daily life.  it stops people from making a good living.\n",
            "it has been going on for years and has accomplished nothing.\n",
            "\n",
            "\n",
            "21528\n",
            "the majority of medicine is based on plants or extracts, that started as homeopathy\n",
            "many homeopathic remedies, like garlic and ginger, are effective\n",
            "it is not for anyone to dictate what you do to your own body\n",
            "\n",
            "\n",
            "21973\n",
            "the planet has limited resources\n",
            "if people would spend less older adults by saving money now, and  not overspending people will have more in savings and able to afford a comfortable life as they age.\n",
            "\n",
            "\n",
            "22476\n",
            "The true purposes of telemarketing are to pressure feeble-minded people into buying things they\n",
            "don't really want, and to make it easier to get away with fraud\n",
            "\n",
            "\n",
            "22477\n",
            "The true purposes of telemarketing are to pressure feeble-minded people into buying things they\n",
            "don't really want, and to make it easier to get away with fraud\n",
            "\n",
            "\n",
            "22922\n",
            "there are certain races that commit certain crimes. it's just a fact. by allowing racial profiling\n",
            " you are allowing law enforcement to solve crimes faster than by assuming anyone could have done it.\n",
            "\n",
            "\n",
            "24800\n",
            "vocational education can be useful to students and they cant all afford to pay for their own education.\n",
            "students should also be offered vocational education if they arent academic to learn new skills\n",
            "\n",
            "\n",
            "25018\n",
            "we are convinced of\n",
            "that nuclear weapons go against the international law of the people and it is\n",
            "a crime against humanity\n",
            "\n",
            "\n",
            "25128\n",
            "we do not have the infrastructure required for a safe autonomous driving system\n",
            "how can we code the car to choose between one fatality and another\n",
            "it would destroy the taxi and delivery driver job market\n",
            "\n",
            "\n",
            "25129\n",
            "we do not have the infrastructure to safely use them\n",
            "there are too many risks with regards to hacking\n",
            "no machine can ever work around the nuances of human drivers safely\n",
            "\n",
            "\n",
            "25311\n",
            "we must prohibit subrogation since the person who is committed can not pass on his rights to another and be replaced\n",
            ".\n",
            "\n",
            "\n",
            "26610\n",
            "we should end mandatory retirement some people have to work to supplement government pensions. \n",
            "it gives them social interaction which they may not have if they arent working\n",
            "\n",
            "\n",
            "28955\n",
            "whale meat is an essential part of japan's history & culture.\n",
            "whaling is needed especially for individuals that live in colder parts of the world, as it is used as a survival kit\n",
            "\n",
            "\n",
            "29033\n",
            "whaling is a means of obtaining a living for many families.  the income earned by catching a whale feed families.\n",
            "whales are not close to extinction.  they can be harvested without threats.\n",
            "\n",
            "\n",
            "29235\n",
            "when modern medicine avenues have been exhausted, it is a valuable alternative to keep hope\n",
            "many homeopathic medicines are effective and help the patient to stay positive, which is half the battle sometimes\n",
            "\n",
            "\n",
            "29312\n",
            "while some foster care is wonderful, many people just do it for the money.  the child is not nurtured to his best.\n",
            "children do not develop consistency in relations becoming future problems in their lives.\n",
            "\n",
            "\n",
            "29586\n",
            "will be the best possible solution for the person prosecuted,\n",
            "according to the particular circumstances of the case\n",
            "\n",
            "\n",
            "29618\n",
            "with modern technology,women do not have to compete on the same physical level as men. \n",
            "there are plenty of options were they can function safely\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Split"
      ],
      "metadata": {
        "id": "FQfS-UzU6QXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "is_training_data =  df['set']=='train'\n",
        "is_validation_data =  df['set']=='dev'\n",
        "is_test_data =  df['set']=='test'\n",
        "\n",
        "training_data = df[is_training_data]\n",
        "validation_data = df[is_validation_data]\n",
        "test_data  = df[is_test_data ]\n",
        "\n",
        "x_train = training_data['argument']\n",
        "Y_train = training_data['MACE-P']\n",
        "\n",
        "x_val = validation_data['argument']\n",
        "Y_val = validation_data['MACE-P']\n",
        "\n",
        "x_test = test_data['argument']\n",
        "Y_test = test_data['MACE-P']"
      ],
      "metadata": {
        "id": "A82m4_1K5pfv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs14cTT92k-l",
        "outputId": "85f81fe8-976c-4a6c-fbbe-cbb99146bf27"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20974,)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Bert](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb)\n",
        "\n"
      ],
      "metadata": {
        "id": "a6PDqt2Kfz_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y8_ctG55-uTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a759e58a-416f-4a79-fd88-1ddb7a4f41e0",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ],
      "source": [
        "#@title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ],
      "metadata": {
        "id": "Gzql3TDrBtg-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "metadata": {
        "id": "Pduw5iP9Dgz_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_embeding(sentences):\n",
        "    preprocessed_text = bert_preprocess_model(sentences)\n",
        "    return bert_model(preprocessed_text)['encoder_outputs'][-1][:,0,:]\n",
        "\n",
        "get_sentence_embeding([\n",
        "    \"500$ discount. hurry up\", \n",
        "    \"Bhavin, are you up for a volleybal game tomorrow?\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "wdhOVzGzt1uc",
        "outputId": "37c71207-b161-4ade-fb1f-1c24bd15ec50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
              "array([[-1.9476332e-02, -1.6363966e-01,  3.2493129e-02, ...,\n",
              "        -3.2246384e-01, -1.0321505e-01,  2.7680629e-01],\n",
              "       [-3.0853543e-02, -3.3630687e-01, -1.6466528e-04, ...,\n",
              "        -4.8734567e-01,  5.7031804e-01,  4.6865925e-01]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model(dense_size=100):\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dense(dense_size, activation=keras.activations.relu, name='fc')(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=keras.activations.sigmoid, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "vWN6VYotHCw9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model = build_classifier_model()"
      ],
      "metadata": {
        "id": "-_pX1FzsK2uj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.summary()"
      ],
      "metadata": {
        "id": "mAijDE-crAt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a75c52-1d72-40cc-af10-6189d52899b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " preprocessing (KerasLayer)     {'input_mask': (Non  0           ['text[0][0]']                   \n",
            "                                e, 128),                                                          \n",
            "                                 'input_type_ids':                                                \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_word_ids':                                                \n",
            "                                (None, 128)}                                                      \n",
            "                                                                                                  \n",
            " BERT_encoder (KerasLayer)      {'default': (None,   109482241   ['preprocessing[0][0]',          \n",
            "                                768),                             'preprocessing[0][1]',          \n",
            "                                 'sequence_output':               'preprocessing[0][2]']          \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 'pooled_output': (                                               \n",
            "                                None, 768),                                                       \n",
            "                                 'encoder_outputs':                                               \n",
            "                                 [(None, 128, 768),                                               \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)]}                                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['BERT_encoder[0][12]']          \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1 (Sl  (None, 768)         0           ['BERT_encoder[0][11]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['BERT_encoder[0][10]']          \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_3 (Sl  (None, 768)         0           ['BERT_encoder[0][9]']           \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " tf.concat (TFOpLambda)         (None, 3072)         0           ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 , 'tf.__operators__.getitem_1[0][\n",
            "                                                                 0]',                             \n",
            "                                                                  'tf.__operators__.getitem_2[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'tf.__operators__.getitem_3[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " fc (Dense)                     (None, 100)          307300      ['tf.concat[0][0]']              \n",
            "                                                                                                  \n",
            " classifier (Dense)             (None, 1)            101         ['fc[0][0]']                     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,789,642\n",
            "Trainable params: 109,789,641\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def pearson_loss(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x, axis=0)\n",
        "    my = K.mean(y, axis=0)\n",
        "    xm, ym = x - mx, y - my\n",
        "    r_num = K.sum(xm * ym)\n",
        "    x_square_sum = K.sum(xm * xm)\n",
        "    y_square_sum = K.sum(ym * ym)\n",
        "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
        "    r = -r_num / r_den\n",
        "    return K.mean(r)\n",
        "\n",
        "def pearson_metric(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x, axis=0)\n",
        "    my = K.mean(y, axis=0)\n",
        "    xm, ym = x - mx, y - my\n",
        "    r_num = K.sum(xm * ym)\n",
        "    x_square_sum = K.sum(xm * xm)\n",
        "    y_square_sum = K.sum(ym * ym)\n",
        "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
        "    r = r_num / r_den\n",
        "    return K.mean(r)\n",
        "\n",
        "loss = pearson_loss\n",
        "loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "metric_pearson = pearson_metric\n",
        "metric_mse = tf.keras.metrics.MeanSquaredError()\n"
      ],
      "metadata": {
        "id": "1L4k5p-bIdyq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "batch_size = 64\n",
        "steps_per_epoch = x_train.shape[0] / batch_size \n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(epochs * x_train.shape[0] * 0.1 / batch_size)\n",
        "\n",
        "###solution 1\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n"
      ],
      "metadata": {
        "id": "rpXy0Zm7JnbJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b9ed3342-db83-4e6a-cf52-f4ff0c556229"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom tensorflow.keras.optimizers import Adam\\ninit_lr = 3e-5\\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\\n                      init_lr,\\n                      decay_steps=100000,\\n                      decay_rate=0.96,\\n                      staircase=True)\\noptimizer = Adam(learning_rate=lr_schedule)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=[metric_pearson,metric_mse])"
      ],
      "metadata": {
        "id": "jI_L17okKvNf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=x_train,y=Y_train,epochs=epochs)"
      ],
      "metadata": {
        "id": "n9rUCXRzLhmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2c40cd5-5a20-420e-ad18-e584f05fb8ac"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n",
            "656/656 [==============================] - 608s 903ms/step - loss: -0.4881 - pearson_metric: 0.4882 - mean_squared_error: 0.1059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, metric_1,metric_2 = classifier_model.evaluate(x=x_test, y=Y_test)"
      ],
      "metadata": {
        "id": "bjGrtWz6Iu8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72478080-1f1b-4ab9-bad3-7e94412c56b3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "198/198 [==============================] - 66s 330ms/step - loss: -0.4372 - pearson_metric: 0.4366 - mean_squared_error: 0.1081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_predicted = classifier_model.predict(x_test)\n",
        "ris = pd.DataFrame(\n",
        "    {'x_test': x_test,\n",
        "     'Y_test': Y_test,\n",
        "     'Y_predicted': list(Y_predicted)\n",
        "    })\n",
        "ris"
      ],
      "metadata": {
        "id": "vcXmtg26cEAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27583a03-6925-43a4-946a-b2b201455ac1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1080745750293189"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Grid Search"
      ],
      "metadata": {
        "id": "a07yUREWKK8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "parameters = {'epochs': range(1, 4, 1), \n",
        "              'batch_size':[32, 64, 128],\n",
        "              'init_lr': [3e-5,3e-4,3e-3],\n",
        "              'dense_size' : [100],\n",
        "              'loss' : [pearson_loss,tf.keras.losses.MeanSquaredError()]\n",
        "              }\n",
        "\n",
        "\n",
        "best_scores = -1\n",
        "best_params = {1: dict()}\n",
        "\n",
        "for loss in parameters['loss']:\n",
        "  print(\"Loss: \", loss)\n",
        "  for epochs in parameters['epochs']:\n",
        "    print(\" Epochs: \", epochs)\n",
        "    for init_lr in parameters['init_lr']:\n",
        "      print(\"  Start Learning Rate: \", init_lr)\n",
        "      for batch_size in parameters['batch_size']:\n",
        "        print(\"   Batch Size: \", batch_size)\n",
        "        for dense_size in parameters['dense_size']:\n",
        "          print(\"    Dense size: \", dense_size)\n",
        "          steps_per_epoch = x_train.shape[0] / batch_size \n",
        "          num_train_steps = steps_per_epoch * epochs\n",
        "          num_warmup_steps = int(epochs * x_train.shape[0] * 0.1 / batch_size)\n",
        "          optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                            num_train_steps=num_train_steps,\n",
        "                                            num_warmup_steps=num_warmup_steps,\n",
        "                                            optimizer_type='adamw')\n",
        "          classifier_model = build_classifier_model(dense_size)\n",
        "          classifier_model.compile(optimizer=optimizer,\n",
        "                          loss=loss,\n",
        "                          metrics=[metric_pearson,metric_mse])\n",
        "          history = classifier_model.fit(x=x_train,y=Y_train,epochs=epochs)\n",
        "          loss_calculated, pearson ,mse = classifier_model.evaluate(x=x_val, y=Y_val)\n",
        "          print(\"     Pearson: \", pearson)\n",
        "          print(\"     MSE: \", mse)\n",
        "          if pearson > best_scores:                 \n",
        "            best_score = pearson\n",
        "            best_params = {'epochs': epochs, \n",
        "                           'batch_size': batch_size, \n",
        "                           'start_lr': init_lr,  \n",
        "                           'dense_size':dense_size,\n",
        "                           'loss':loss_calculated}\n",
        "print(best_scores)\n",
        "print(best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "QQ7fM7G1KNJK",
        "outputId": "459b0ad2-44f3-4ba5-e41d-9551b901b042"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  <function pearson_loss at 0x7fc32c2c5c20>\n",
            " Epochs:  1\n",
            "  Start Learning Rate:  3e-05\n",
            "   Batch Size:  32\n",
            "    Dense size:  100\n",
            "656/656 [==============================] - 602s 894ms/step - loss: -0.4886 - pearson_metric: 0.4888 - mean_squared_error: 0.1050\n",
            "101/101 [==============================] - 35s 343ms/step - loss: -0.4859 - pearson_metric: 0.4863 - mean_squared_error: 0.1012\n",
            "     Pearson:  0.4862997233867645\n",
            "     MSE:  0.10118497163057327\n",
            "   Batch Size:  64\n",
            "    Dense size:  100\n",
            "206/656 [========>.....................] - ETA: 6:43 - loss: -0.4609 - pearson_metric: 0.4609 - mean_squared_error: 0.1376"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-ed8fad096ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m                           \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                           metrics=[metric_pearson,metric_mse])\n\u001b[0;32m---> 34\u001b[0;31m           \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m           \u001b[0mloss_calculated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpearson\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"     Pearson: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpearson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
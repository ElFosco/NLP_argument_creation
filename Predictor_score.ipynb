{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElFosco/NLP_argument_creation/blob/main/Predictor_score.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWUoqJbhZUaH"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the libraries, then restart the runtime, then re-install the libraries."
      ],
      "metadata": {
        "id": "o2SF3PVTEg6x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iHiGJ5Ou8Mwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b141bf8b-4357-43bf-c3a6-471b33f35aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-VGaZQA8ANi",
        "outputId": "95b311ca-2953-4436-91a4-79786884babf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (2.8.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.21.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (13.0.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.13.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.43.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.2.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Vzt9o9nfaCDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a6d15c5-d464-4fba-bb42-9f750be96cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "pip install torch>=1.6.0 transformers>=4.11.3 sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twaYdrRVblzG",
        "outputId": "8b86225d-7acb-4c63-daf6-52642b1a1d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.10-py3-none-any.whl (410 kB)\n",
            "\u001b[K     |████████████████████████████████| 410 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->nlpaug) (1.15.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Installing collected packages: nlpaug\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Successfully installed nlpaug-1.1.10\n"
          ]
        }
      ],
      "source": [
        "pip install numpy requests nlpaug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrxfQlV-7lbr"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ElFosco/NLP_argument_creation.git #necessary for the data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBL6xCpaELCM",
        "outputId": "8aa7c29e-7ba7-4737-e5c5-35881ee1753d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'NLP_argument_creation' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmPHpfi8AkgA",
        "outputId": "5706715b-00a0-49b7-ace4-e610320a3d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import nlpaug.augmenter.word as naw #data augmentation\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE_to76fAVf7"
      },
      "source": [
        "#Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn6dyvb0AzdK",
        "outputId": "fff0ea0c-685a-44b2-e95b-f8be90bc7516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Using google drive to upload the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dir_path = \"/content/NLP_argument_creation/Data/\"\n",
        "dataset = \"arg_quality_rank_30k.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ca1VdgDoAURT",
        "outputId": "3f5d79db-c078-4d5d-a0a6-8e056093da0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-92e13391-c3ca-4112-a0d2-c66ef3c21651\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>argument</th>\n",
              "      <th>topic</th>\n",
              "      <th>set</th>\n",
              "      <th>WA</th>\n",
              "      <th>MACE-P</th>\n",
              "      <th>stance_WA</th>\n",
              "      <th>stance_WA_conf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"marriage\" isn't keeping up with the times.  a...</td>\n",
              "      <td>We should abandon marriage</td>\n",
              "      <td>train</td>\n",
              "      <td>0.846165</td>\n",
              "      <td>0.297659</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>.a multi-party system would be too confusing a...</td>\n",
              "      <td>We should adopt a multi-party system</td>\n",
              "      <td>train</td>\n",
              "      <td>0.891271</td>\n",
              "      <td>0.726133</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\ero-tolerance policy in schools should not be...</td>\n",
              "      <td>We should adopt a zero-tolerance policy in sch...</td>\n",
              "      <td>dev</td>\n",
              "      <td>0.721192</td>\n",
              "      <td>0.396953</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>`people reach their limit when it comes to the...</td>\n",
              "      <td>Assisted suicide should be a criminal offence</td>\n",
              "      <td>train</td>\n",
              "      <td>0.730395</td>\n",
              "      <td>0.225212</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100% agree, should they do that, it would be a...</td>\n",
              "      <td>We should abolish safe spaces</td>\n",
              "      <td>train</td>\n",
              "      <td>0.236686</td>\n",
              "      <td>0.004104</td>\n",
              "      <td>1</td>\n",
              "      <td>0.805517</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92e13391-c3ca-4112-a0d2-c66ef3c21651')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-92e13391-c3ca-4112-a0d2-c66ef3c21651 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-92e13391-c3ca-4112-a0d2-c66ef3c21651');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            argument  ... stance_WA_conf\n",
              "0  \"marriage\" isn't keeping up with the times.  a...  ...       1.000000\n",
              "1  .a multi-party system would be too confusing a...  ...       1.000000\n",
              "2  \\ero-tolerance policy in schools should not be...  ...       1.000000\n",
              "3  `people reach their limit when it comes to the...  ...       1.000000\n",
              "4  100% agree, should they do that, it would be a...  ...       0.805517\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df = pd.read_csv(dir_path + dataset)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV1gnJwQNIZd"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nBRUbfFqwIyu"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text):\n",
        "  text = re.sub('\\\"|-|\\\\\\\\|`', ' ', text)  # delete this chars from the string [\"-\\`]\n",
        "  text = re.sub('\\n', ' ', text)\n",
        "  text = re.sub('^[.]+', '', text)         # delete dots at the beginning of the sentence\n",
        "  text = re.sub(\"([?.!,])\", r\" \\1 \", text)\n",
        "  text = re.sub('\\. \\.', '.', text)        # delete . .\n",
        "  text = re.sub('&', ' and ', text)        # replace & with and\n",
        "  text = re.sub(' +', ' ', text)           # delete additional whitespace\n",
        "  text = text.rstrip()                  \n",
        "  text = text.lstrip()\n",
        "  text = \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J6314Kqgy-bl"
      },
      "outputs": [],
      "source": [
        "df.loc[2, \"argument\"] = \"zero tolerance policy in schools should not be adopted as circumstances are often not black and white, being more nuanced. no one should be written off due to a mistake of judgement.\"\n",
        "df['argument'] = df.apply(lambda row : clean_text(row['argument']), axis = 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQfS-UzU6QXS"
      },
      "source": [
        "##Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "A82m4_1K5pfv"
      },
      "outputs": [],
      "source": [
        "is_training_data =  df['set']=='train'\n",
        "is_validation_data =  df['set']=='dev'\n",
        "is_test_data =  df['set']=='test'\n",
        "\n",
        "training_data = df[is_training_data]\n",
        "validation_data = df[is_validation_data]\n",
        "test_data  = df[is_test_data ]\n",
        "\n",
        "x_train = training_data['argument'].reset_index(drop=True)\n",
        "Y_train = training_data['MACE-P'].reset_index(drop=True)\n",
        "\n",
        "x_val = validation_data['argument'].reset_index(drop=True)\n",
        "Y_val = validation_data['MACE-P'].reset_index(drop=True)\n",
        "\n",
        "x_test = test_data['argument'].reset_index(drop=True)\n",
        "Y_test = test_data['MACE-P'].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTdXuAQSUMHm"
      },
      "source": [
        "#Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LBGSpoUYcrLD"
      },
      "outputs": [],
      "source": [
        "def create_augmented_data(x,Y):                   # run this two cells if you want to create other augmented data\n",
        "                                                  # right now, we have created around 8k of new data\n",
        "  back_translation_aug = naw.BackTranslationAug(\n",
        "    from_model_name='facebook/wmt19-en-de', \n",
        "    to_model_name='facebook/wmt19-de-en'\n",
        "  )\n",
        "\n",
        "  tmp_df = pd.DataFrame()\n",
        "  for i in range(8000,int(x.shape[0])):\n",
        "    print('['+str(i+1)+'/'+str(int(x.shape[0])+1)+']')\n",
        "    new_argument = back_translation_aug.augment(x[i])\n",
        "    score = Y[i]\n",
        "    tmp = {'argument': new_argument, 'score': score}\n",
        "    tmp_df = tmp_df.append(tmp,ignore_index=True)\n",
        "    if ((i+1)%500)==0:\n",
        "      print(\"Print on file:\"+str(int(i+1)))\n",
        "      tmp_df.to_csv(dir_path+\"aug_\"+str(int(i+1))+\".csv\", index=False, encoding='utf-8-sig')\n",
        "      tmp_df = pd.DataFrame()\n",
        "  tmp_df.to_csv(dir_path+\"aug.csv\", index=False, encoding='utf-8-sig')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZm0HqoJRSeA"
      },
      "outputs": [],
      "source": [
        "create_augmented_data(x_train,Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwiNzlVTslqU"
      },
      "source": [
        "##Read augmented data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KcTNARA_orKP"
      },
      "outputs": [],
      "source": [
        "def read_aug_data():                                      #read augmented data, from the one collected, right now there are 8k new data\n",
        "  ris = pd.DataFrame()\n",
        "  for i in range(500,8500,500):\n",
        "    df = pd.read_csv(dir_path+\"aug_\"+str(int(i))+\".csv\")\n",
        "    ris = ris.append(df)\n",
        "  return ris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CPAuE-aXpGfW"
      },
      "outputs": [],
      "source": [
        "ris_aug = read_aug_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PSEaFexRsy0W"
      },
      "outputs": [],
      "source": [
        "x_train_aug = ris_aug['argument']\n",
        "Y_train_aug = ris_aug['score']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGfxf4XmwyT5"
      },
      "source": [
        "#Data from UKP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyCbglAfI8Ba",
        "outputId": "7348e18b-b78e-4132-8300-3bf0d758a8e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(693, 3)\n",
            "(167, 3)\n",
            "(192, 3)\n"
          ]
        }
      ],
      "source": [
        "ukp_path = dir_path + \"UKPConvArg1-Ranking-CSV/\"            #read data from UKP dataset, these will be splitted into\n",
        "i=0                                                         #training, validation and test set\n",
        "ukp_dataset_train = pd.DataFrame()\n",
        "ukp_dataset_valid = pd.DataFrame()\n",
        "ukp_dataset_test = pd.DataFrame()\n",
        "for csv in os.listdir(ukp_path):\n",
        "  if i<=20:\n",
        "    df = pd.read_csv(ukp_path+csv,sep='\\t')\n",
        "    ukp_dataset_train = ukp_dataset_train.append(df)\n",
        "  elif 20<i<=25 :\n",
        "    df = pd.read_csv(ukp_path+csv,sep='\\t')\n",
        "    ukp_dataset_valid = ukp_dataset_valid.append(df)\n",
        "  else:\n",
        "    df = pd.read_csv(ukp_path+csv,sep='\\t')\n",
        "    ukp_dataset_test = ukp_dataset_test.append(df)\n",
        "  i+=1\n",
        "\n",
        "print(ukp_dataset_train.shape)\n",
        "print(ukp_dataset_valid.shape)\n",
        "print(ukp_dataset_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KWI35htq-wdX"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text_ukp(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('\\\"|-|\\\\\\\\|`|/|\\'', ' ', text)  # delete this chars from the string [\"-\\`]\n",
        "  text = re.sub('<br/>', ' ', text)\n",
        "  text = re.sub(':\\)', ' ', text)\n",
        "  text = re.sub('[\\.]+[\\.]+', ' ', text)         # delete ...\n",
        "  #text = re.sub(\"([?.!,])\", r\" \\1 \", text)\n",
        "  text = re.sub('&', ' and ', text)        # replace & with and\n",
        "  text = re.sub(' +', ' ', text)           # delete additional whitespace\n",
        "  text = text.rstrip()                  \n",
        "  text = text.lstrip()\n",
        "  text = \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n",
        "  return text\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Omw-W9EyHIpP"
      },
      "outputs": [],
      "source": [
        "ukp_dataset_train['argument'] = ukp_dataset_train.apply(lambda row : clean_text_ukp(row['argument']), axis = 1)\n",
        "ukp_dataset_valid['argument'] = ukp_dataset_valid.apply(lambda row : clean_text_ukp(row['argument']), axis = 1)\n",
        "ukp_dataset_test['argument'] = ukp_dataset_test.apply(lambda row : clean_text_ukp(row['argument']), axis = 1)\n",
        "\n",
        "x_train_ukp = ukp_dataset_train['argument']\n",
        "Y_train_ukp = ukp_dataset_train['rank']\n",
        "\n",
        "x_val_ukp = ukp_dataset_valid['argument']\n",
        "Y_val_ukp = ukp_dataset_valid['rank']\n",
        "\n",
        "x_test_ukp = ukp_dataset_test['argument']\n",
        "Y_test_ukp = ukp_dataset_test['rank']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZOP4Qz9NhBs"
      },
      "source": [
        "#Final Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_augmented_added = False        #flag indicating if you want to add the augmented dataset\n",
        "is_ukp_added = False              #flag indicating if you want to add the ukp dataset"
      ],
      "metadata": {
        "id": "17uNUJF1Hml6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_final_dataset(x_train,Y_train,x_val,Y_val,x_test,Y_test,\n",
        "                           x_train_aug,Y_train_aug,\n",
        "                           x_train_ukp,Y_train_ukp,x_val_ukp,Y_val_ukp,x_test_ukp,Y_test_ukp,\n",
        "                           is_augmented_added,is_ukp_added):\n",
        "  \n",
        "  if is_augmented_added==True:\n",
        "\n",
        "    x_train = (x_train.append(x_train_aug)).reset_index(drop=True)\n",
        "    Y_train = (Y_train.append(Y_train_aug)).reset_index(drop=True)\n",
        "  \n",
        "  if is_ukp_added==True:\n",
        "\n",
        "    x_train = (x_train.append(x_train_aug)).reset_index(drop=True)\n",
        "    Y_train = (Y_train.append(Y_train_aug)).reset_index(drop=True)\n",
        "\n",
        "    x_val = (x_val.append(x_val_ukp)).reset_index(drop=True)\n",
        "    Y_val = (Y_val.append(Y_val_ukp)).reset_index(drop=True)\n",
        "\n",
        "    x_test = (x_test.append(x_test_ukp)).reset_index(drop=True)\n",
        "    Y_test = (Y_test.append(Y_test_ukp)).reset_index(drop=True)\n",
        "\n",
        "  return x_train,Y_train,x_val,Y_val,x_test,Y_test"
      ],
      "metadata": {
        "id": "nhK5YOsuIDFq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gJtlb4EsNgrH"
      },
      "outputs": [],
      "source": [
        "x_train,Y_train,x_val,Y_val,x_test,Y_test= generate_final_dataset(x_train,Y_train,\n",
        "                                                                  x_val,Y_val,\n",
        "                                                                  x_test,Y_test,\n",
        "                                                                  x_train_aug,Y_train_aug,\n",
        "                                                                  x_train_ukp,Y_train_ukp,\n",
        "                                                                  x_val_ukp,Y_val_ukp,\n",
        "                                                                  x_test_ukp,Y_test_ukp,\n",
        "                                                                  is_augmented_added,is_ukp_added)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6PDqt2Kfz_T"
      },
      "source": [
        "#[Bert](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8_ctG55-uTX",
        "outputId": "7a52850f-0775-4535-e5fc-a9e016d901b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ],
      "source": [
        "# @title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'  # @param [\"bert_en_uncased_L-24_H-1024_A-16\",\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Gzql3TDrBtg-"
      },
      "outputs": [],
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) #preprocessing layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Pduw5iP9Dgz_"
      },
      "outputs": [],
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)               #bert model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "vWN6VYotHCw9"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model(dense_size=100):                     #model used to compute the score of the argument\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dense(dense_size, activation=keras.activations.relu, name='fc_1')(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=keras.activations.sigmoid, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "-_pX1FzsK2uj"
      },
      "outputs": [],
      "source": [
        "classifier_model = build_classifier_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAijDE-crAt4",
        "outputId": "b4aa6318-0aca-42b4-b914-e0d92342fe90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " preprocessing (KerasLayer)     {'input_type_ids':   0           ['text[0][0]']                   \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_mask': (Non                                               \n",
            "                                e, 128),                                                          \n",
            "                                 'input_word_ids':                                                \n",
            "                                (None, 128)}                                                      \n",
            "                                                                                                  \n",
            " BERT_encoder (KerasLayer)      {'pooled_output': (  109482241   ['preprocessing[0][0]',          \n",
            "                                None, 768),                       'preprocessing[0][1]',          \n",
            "                                 'sequence_output':               'preprocessing[0][2]']          \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 'encoder_outputs':                                               \n",
            "                                 [(None, 128, 768),                                               \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)],                                               \n",
            "                                 'default': (None,                                                \n",
            "                                768)}                                                             \n",
            "                                                                                                  \n",
            " fc_1 (Dense)                   (None, 100)          76900       ['BERT_encoder[0][13]']          \n",
            "                                                                                                  \n",
            " classifier (Dense)             (None, 1)            101         ['fc_1[0][0]']                   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,559,242\n",
            "Trainable params: 109,559,241\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "classifier_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1L4k5p-bIdyq"
      },
      "outputs": [],
      "source": [
        "def pearson_loss(y_true, y_pred):   #loss used to optimize the bert model, in the end MSE turned out to be better\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x, axis=0)\n",
        "    my = K.mean(y, axis=0)\n",
        "    xm, ym = x - mx, y - my\n",
        "    r_num = K.sum(xm * ym)\n",
        "    x_square_sum = K.sum(xm * xm)\n",
        "    y_square_sum = K.sum(ym * ym)\n",
        "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
        "    r = -r_num / r_den\n",
        "    return K.mean(r)\n",
        "\n",
        "def pearson_metric(y_true, y_pred): #metric used to do some analysis for the data\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x, axis=0)\n",
        "    my = K.mean(y, axis=0)\n",
        "    xm, ym = x - mx, y - my\n",
        "    r_num = K.sum(xm * ym)\n",
        "    x_square_sum = K.sum(xm * xm)\n",
        "    y_square_sum = K.sum(ym * ym)\n",
        "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
        "    r = r_num / r_den\n",
        "    return K.mean(r)\n",
        "\n",
        "loss = pearson_loss\n",
        "loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "metric_pearson = pearson_metric\n",
        "metric_mse = tf.keras.metrics.MeanSquaredError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a07yUREWKK8s"
      },
      "source": [
        "#Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ7fM7G1KNJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d406ac1-eb76-4fe7-aa68-8478e95cf790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  <keras.losses.MeanSquaredError object at 0x7f62cc1c2dd0>\n",
            " Epochs:  2\n",
            "  Start Learning Rate:  3e-05\n",
            "   Batch Size:  32\n",
            "    Dense size:  100\n",
            "Epoch 1/2\n",
            "656/656 [==============================] - 1456s 2s/step - loss: 0.1087 - pearson_metric: 0.4440 - mean_squared_error: 0.1087\n",
            "Epoch 2/2\n",
            "656/656 [==============================] - 1433s 2s/step - loss: 0.0808 - pearson_metric: 0.6378 - mean_squared_error: 0.0808\n",
            "101/101 [==============================] - 81s 792ms/step - loss: 0.0977 - pearson_metric: 0.4789 - mean_squared_error: 0.0977\n",
            "     Pearson:  0.47894126176834106\n",
            "     MSE:  0.09766768664121628\n",
            "Loss:  <function pearson_loss at 0x7f62c6746e60>\n",
            " Epochs:  2\n",
            "  Start Learning Rate:  3e-05\n",
            "   Batch Size:  32\n",
            "    Dense size:  100\n",
            "Epoch 1/2\n",
            "656/656 [==============================] - 1456s 2s/step - loss: -0.4633 - pearson_metric: 0.4632 - mean_squared_error: 0.1534\n",
            "Epoch 2/2\n",
            "656/656 [==============================] - 1432s 2s/step - loss: -0.6514 - pearson_metric: 0.6516 - mean_squared_error: 0.1244\n",
            "101/101 [==============================] - 81s 790ms/step - loss: -0.4863 - pearson_metric: 0.4843 - mean_squared_error: 0.1314\n",
            "     Pearson:  0.48425066471099854\n",
            "     MSE:  0.1314152330160141\n",
            "-1\n",
            "{'epochs': 2, 'batch_size': 32, 'start_lr': 3e-05, 'dense_size': 100, 'loss': -0.4863112270832062}\n"
          ]
        }
      ],
      "source": [
        "parameters = {'epochs': [1,2,3], \n",
        "              'batch_size':[32],\n",
        "              'init_lr': [3e-6,3e-5],\n",
        "              'dense_size' : [100,200,300],\n",
        "              'loss' : [tf.keras.losses.MeanSquaredError(), pearson_loss]\n",
        "              }\n",
        "\n",
        "best_scores = -1\n",
        "best_params = {1: dict()}\n",
        "\n",
        "for loss in parameters['loss']:\n",
        "  print(\"Loss: \", loss)\n",
        "  for epochs in parameters['epochs']:\n",
        "    print(\" Epochs: \", epochs)\n",
        "    for init_lr in parameters['init_lr']:\n",
        "      print(\"  Start Learning Rate: \", init_lr)\n",
        "      for batch_size in parameters['batch_size']:\n",
        "        print(\"   Batch Size: \", batch_size)\n",
        "        for dense_size in parameters['dense_size']:\n",
        "          print(\"    Dense size: \", dense_size)\n",
        "          steps_per_epoch = x_train.shape[0] / batch_size \n",
        "          num_train_steps = steps_per_epoch * epochs\n",
        "          num_warmup_steps = int(epochs * x_train.shape[0] * 0.1 / batch_size)\n",
        "          optimizer = optimization.create_optimizer(init_lr=init_lr, \n",
        "                                                    num_train_steps=num_train_steps, \n",
        "                                                    num_warmup_steps=num_warmup_steps, \n",
        "                                                    optimizer_type='adamw')\n",
        "          classifier_model = build_classifier_model(dense_size)\n",
        "          classifier_model.compile(optimizer=optimizer, loss=loss, \n",
        "                                   metrics=[metric_pearson, metric_mse])\n",
        "          history = classifier_model.fit(x=x_train, y=Y_train, epochs=epochs, \n",
        "                                         batch_size=batch_size)\n",
        "          loss_calculated, pearson ,mse = classifier_model.evaluate(x=x_val, \n",
        "                                                                    y=Y_val)\n",
        "          print(\"     Pearson: \", pearson)\n",
        "          print(\"     MSE: \", mse)\n",
        "          if pearson > best_scores:                 \n",
        "            best_score = pearson\n",
        "            best_params = {'epochs': epochs, \n",
        "                           'batch_size': batch_size, \n",
        "                           'start_lr': init_lr,  \n",
        "                           'dense_size': dense_size,\n",
        "                           'loss': loss_calculated}\n",
        "print(best_scores)\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGxguBd7tbb-"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8DfZjrPIvy7",
        "outputId": "b1f993a6-8e31-44b0-8a7d-8796fe6dd9f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "756/756 [==============================] - 1277s 2s/step - loss: 0.1078 - pearson_metric: 0.4523 - mean_squared_error: 0.1078\n",
            "Epoch 2/2\n",
            "756/756 [==============================] - 1253s 2s/step - loss: 0.0791 - pearson_metric: 0.6454 - mean_squared_error: 0.0791\n",
            "198/198 [==============================] - 125s 625ms/step - loss: 0.1035 - pearson_metric: 0.4533 - mean_squared_error: 0.1035\n",
            "Pearson:  0.4533013701438904\n",
            "MSE:  0.1034751832485199\n"
          ]
        }
      ],
      "source": [
        "# Best parameter found on grid search\n",
        "parameters = {'epochs': 2, \n",
        "              'batch_size': 32,\n",
        "              'init_lr': 3e-5,\n",
        "              'dense_size': 100,\n",
        "              'loss': tf.keras.losses.MeanSquaredError()\n",
        "              }\n",
        "\n",
        "epochs = parameters['epochs']\n",
        "batch_size = parameters['batch_size']\n",
        "init_lr = parameters['init_lr']\n",
        "dense_size = parameters['dense_size']\n",
        "loss = parameters['loss']\n",
        "\n",
        "steps_per_epoch = (x_train.append(x_val)).shape[0] / batch_size \n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(epochs * ((x_train.append(x_val)).shape[0]) * 0.1 / batch_size)\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr, \n",
        "                                          num_train_steps=num_train_steps, \n",
        "                                          num_warmup_steps=num_warmup_steps, \n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model = build_classifier_model(dense_size)\n",
        "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metric_pearson, \n",
        "                                                                  metric_mse])\n",
        "history = classifier_model.fit(x=(x_train.append(x_val)).reset_index(drop=True), y=(Y_train.append(Y_val)).reset_index(drop=True), \n",
        "                               epochs=epochs, batch_size=batch_size)\n",
        "loss_calculated, pearson, mse = classifier_model.evaluate(x_test, Y_test)\n",
        "print(\"Pearson: \", pearson)\n",
        "print(\"MSE: \", mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTZwGeyPtd4v"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5eU9VjBLV8T"
      },
      "outputs": [],
      "source": [
        "classifier_model.save(\"drive/MyDrive/Colab Notebooks/NLP/classifierIBM30k.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data analysis"
      ],
      "metadata": {
        "id": "XCZh2fWOLEQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ris = classifier_model.predict(x_test)"
      ],
      "metadata": {
        "id": "ST13PUTLLGhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = [0] * 10\n",
        "mse_size = [0] * 10\n",
        "for i in range(len(Y_test)):\n",
        "  mse[math.ceil(ris[i][0]*10)-1] += (ris[i][0]-Y_test[i])**2\n",
        "  mse_size[math.ceil(ris[i][0]*10)-1] +=1\n",
        "for i in range(len(mse)):\n",
        " mse[i]=mse[i]/mse_size[i]"
      ],
      "metadata": {
        "id": "db18mk3jNE7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.set_title('MSE by value range')\n",
        "ax.set_ylabel(\"MSE\")\n",
        "ax.bar(['0.1','0.2','0.3','0.4','0.5','0.6','0.7','0.8','0.8','0.9'],mse)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "cjfw-04ZRdU0",
        "outputId": "15806e01-d1b3-4b61-d40d-5180eaa88f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAFPCAYAAABpizZkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAajElEQVR4nO3dfbRddX3n8fdHIiDKg0IclQBBiaNBHWsj2JlWHfEBdEl8CJooA1g6jLbozFJXTZct0ljXSKeV2krXyPiEYRQo1Zm0RKkzwHLqEkqQJyMDDRgk2NbwqICAge/8cXamh8O9uTck+57fzX2/1jqL/fDb53zuXpd87t5nn31SVUiSpPY8adwBJEnSxCxpSZIaZUlLktQoS1qSpEZZ0pIkNcqSliSpUZa0NMskuSzJb4w5w+lJzh1nBmkusKSlHZBkY5KHkxwwsvzqJJVkYTe/IMlfJrkjyb1Jvp/kpG7dwm7sfSOPd874DySpKfPGHUDaBfwQWAH8GUCSFwN7jYxZDVwLHAI8BLwYeNbImP2qaku/UWeXJPPcJ5rLPJKWdtxq4ISh+ROBL4+MeTnwpaq6v6q2VNXVVfWNHXjN5yX5uyQ/TfI/kzwDIMlFSd4/PDDJdUneOvoESb6R5NSRZdcmeVs3/ekkt3WvcVWSX5soSJJXJ9k0smxjktd2009KsjLJzUnuTHLB1ryTPVeSjyT5R+CLSZ6e5K+TbE5ydze9YGiby5J8PMl3kvwsyd8Mn9lIckKSW7vX/r0nmk0aB0ta2nGXA/skeWGS3YDlwOj7tZcDZyVZnuTgnfCaJwC/Djwb2AL8abf8HOD4rYOS/CvgQOCiCZ7jqwzOAGwdu5jBkf7WsVcCLwWeAXwF+Iskez6BrO8H3gK8CngOcDdw1jbGP6t7zUOAUxj8O/XFbv5g4OfAZ0a2eRfwHuCZwO7Ah4d+pj8H3s1gX+3LYH880WzSjLKkpZ1j69H064AbgNtH1h8H/B/g94AfJrkmyctHxtyR5J6hxwu39XpV9f2qur97znd0fyCsAZ6fZFE37t8B51fVwxM8x9eBlyY5pJt/N/C1qnoIoKrOrao7uyP/Pwb2AP7l1Lvicd4LfLSqNnXPfTqwLMlkb7c9Cnysqh6qqp93Gf6yqh6oqp8Bn2BQqsO+WFU3VdXPgQsY/HEBsAz4q6r6224fnAYMf2HB9maTZpQlLe0cqxkczZ3E4091U1V3V9XKqjoc+BfANcD/SJKhYQdU1X5Djxu28Xq3DU3fCjy52/5B4Hzg+CRPYnCkvHqiJ+gK7yIGR/50Y//71vVJPpzkhu5Ct3sYHIUe8PhnmtIhwNe3/vHB4I+YRxjsh4ls7n6OrTn2SvLZ7pT1T4FvA/t1f5Rs9Y9D0w8AT+umn8PQvqqqB4A7dyCbNKMsaWknqKpbGVxA9kbga1OMvQP4IwYF8kTf/zxoaPpg4BfAHd38OQyOio8CHqiq727jeb4KrEjyK8CewKUA3fvPvw28A3h6Ve0H3Atkgue4n6EL5brynD+0/jbgmJE/QPasqtGzDVuNfjXfhxgcwR9ZVfsAr9z6Utv4ubb6B2D4/eunAPvvQDZpRlnS0s5zMvCa7hT0YyQ5I8mLksxLsjfwPmBDVd35uGeZnuOTLE6yF7AKuLCqHgHoSvlR4I+Z5Ch6yFoGR5OrGJwWf7RbvjeD97o3A/OSnAbsM8lz3ATsmeRNSZ4M/C6DU+Nb/VfgE1tPqyeZn2TpdvysezN4H/qe7qKuj23HthcCb07yr5PszuB09nC572g2qVeWtLSTVNXNVbVuktV7MXgP+B7gFgbFeOzImHtGPif9wW283GrgSwxO8+4JfGBk/ZcZfMxrmzcc6d6H/RrwWgYXh211MfBNBgV8K/Agjz3FPvwc9wK/CXyOwXvx9wPDV3t/msF75X+T5GcMLqI7clu5RvwJ8BQGZwou73JNS1WtZ3Bx2HkMjqrvA37C4GNwOyOb1KtUjZ5ZkjTbJTkBOKWqfnXcWVqS5GkM/lBaVFU/HHceaSoeSUu7mO4U+G8CZ487SwuSvLm7+OypDK4FuB7YON5U0vRY0tIuJMkbGLyP/E889vT1XLYU+HH3WAQsL08hapbwdLckSY3ySFqSpEZZ0pIkNWqXufXdAQccUAsXLhx3DEmStstVV111R1XNn2jdLlPSCxcuZN26yT6iKklSm5LcOtk6T3dLktQoS1qSpEZZ0pIkNcqSliSpUZa0JEmNsqQlSWqUJS1JUqMsaUmSGmVJS5LUKEtakqRGWdKSJDXKkpYkqVG7zBdsSNo1LFx50bgjPMbGT75p3BE0h1nSkrSD/MNCffF0tyRJjbKkJUlqlCUtSVKjLGlJkhplSUuS1ChLWpKkRvkRLGkX19LHg/xokLR9PJKWJKlRlrQkSY2ypCVJapQlLUlSoyxpSZIaZUlLktQoS1qSpEZZ0pIkNarXkk5ydJIbk2xIsnKC9a9M8r0kW5IsG1r+0iTfTbI+yXVJ3tlnTkmSWtRbSSfZDTgLOAZYDKxIsnhk2I+Ak4CvjCx/ADihqg4Hjgb+JMl+fWWVJKlFfd4W9AhgQ1XdApDkPGAp8IOtA6pqY7fu0eENq+qmoekfJ/kJMB+4p8e8kiQ1pc+SPhC4bWh+E3Dk9j5JkiOA3YGbd1Iu6Qlr6T7Y4L2wpV1d0xeOJXk2sBp4T1U9OsH6U5KsS7Ju8+bNMx9QkqQe9VnStwMHDc0v6JZNS5J9gIuAj1bV5RONqaqzq2pJVS2ZP3/+DoWVJKk1fZb0lcCiJIcm2R1YDqyZzobd+K8DX66qC3vMKElSs3or6araApwKXAzcAFxQVeuTrEpyLECSlyfZBBwHfDbJ+m7zdwCvBE5Kck33eGlfWSVJalGfF45RVWuBtSPLThuavpLBafDR7c4Fzu0zmyRJrWv6wjFJkuYyS1qSpEZZ0pIkNcqSliSpUZa0JEmNsqQlSWqUJS1JUqMsaUmSGmVJS5LUKEtakqRGWdKSJDXKkpYkqVGWtCRJjbKkJUlqlCUtSVKjLGlJkhplSUuS1ChLWpKkRlnSkiQ1ypKWJKlRlrQkSY2aN+4AmrsWrrxo3BEeY+Mn3zTuCJL0GB5JS5LUKEtakqRGWdKSJDXKkpYkqVGWtCRJjbKkJUlqlCUtSVKjLGlJkhplSUuS1CjvOLYLaekOXt69S2pXS/9WgP9ebItH0pIkNcqSliSpUb2WdJKjk9yYZEOSlROsf2WS7yXZkmTZyLoTk/x99zixz5ySJLWot5JOshtwFnAMsBhYkWTxyLAfAScBXxnZ9hnAx4AjgSOAjyV5el9ZJUlqUZ9H0kcAG6rqlqp6GDgPWDo8oKo2VtV1wKMj274B+FZV3VVVdwPfAo7uMaskSc3ps6QPBG4bmt/ULdtp2yY5Jcm6JOs2b978hINKktSiWX3hWFWdXVVLqmrJ/Pnzxx1HkqSdqs+Svh04aGh+Qbes720lSdol9FnSVwKLkhyaZHdgObBmmtteDLw+ydO7C8Ze3y2TJGnO6K2kq2oLcCqDcr0BuKCq1idZleRYgCQvT7IJOA74bJL13bZ3AR9nUPRXAqu6ZZIkzRm93ha0qtYCa0eWnTY0fSWDU9kTbfsF4At95pMkqWWz+sIxSZJ2ZZa0JEmNsqQlSWqUJS1JUqMsaUmSGmVJS5LUKEtakqRGWdKSJDXKkpYkqVGWtCRJjbKkJUlqlCUtSVKjLGlJkhplSUuS1ChLWpKkRlnSkiQ1ypKWJKlRlrQkSY2ypCVJapQlLUlSoyxpSZIaZUlLktQoS1qSpEZZ0pIkNcqSliSpUZa0JEmNsqQlSWqUJS1JUqMsaUmSGmVJS5LUKEtakqRGWdKSJDXKkpYkqVG9lnSSo5PcmGRDkpUTrN8jyfnd+iuSLOyWPznJOUmuT3JDkt/pM6ckSS3qraST7AacBRwDLAZWJFk8Muxk4O6qOgw4EzijW34csEdVvRj4ZeA/bC1wSZLmij6PpI8ANlTVLVX1MHAesHRkzFLgnG76QuCoJAEKeGqSecBTgIeBn/aYVZKk5vRZ0gcCtw3Nb+qWTTimqrYA9wL7Myjs+4F/AH4E/FFV3TX6AklOSbIuybrNmzfv/J9AkqQxavXCsSOAR4DnAIcCH0ry3NFBVXV2VS2pqiXz58+f6YySJPWqz5K+HThoaH5Bt2zCMd2p7X2BO4F3Ad+sql9U1U+A7wBLeswqSVJz+izpK4FFSQ5NsjuwHFgzMmYNcGI3vQy4pKqKwSnu1wAkeSrwCuD/9phVkqTm9FbS3XvMpwIXAzcAF1TV+iSrkhzbDfs8sH+SDcAHga0f0zoLeFqS9QzK/otVdV1fWSVJatG8Pp+8qtYCa0eWnTY0/SCDj1uNbnffRMslSZpLWr1wTJKkOc+SliSpUZa0JEmNsqQlSWqUJS1JUqMsaUmSGmVJS5LUKEtakqRGWdKSJDXKkpYkqVGWtCRJjdpmSSc5fmj634ysO7WvUJIkaeoj6Q8OTf/ZyLpf38lZJEnSkKlKOpNMTzQvSZJ2oqlKuiaZnmhekiTtRFN9n/QLklzH4Kj5ed003fxze00mSdIcN1VJv3BGUkiSpMfZZklX1a3D80n2B14J/KiqruozmCRJc91UH8H66yQv6qafDXyfwVXdq5P8pxnIJ0nSnDXVhWOHVtX3u+n3AN+qqjcDR+JHsCRJ6tVUJf2LoemjgLUAVfUz4NG+QkmSpKkvHLstyfuBTcDLgG8CJHkK8OSes0mSNKdNdSR9MnA4cBLwzqq6p1v+CuCLPeaSJGnOm+rq7p8A751g+aXApX2FkiRJU5R0kjXbWl9Vx+7cOJIkaaup3pP+FeA24KvAFXi/bkmSZsxUJf0s4HXACuBdwEXAV6tqfd/BJEma67Z54VhVPVJV36yqExlcLLYBuMzvkpYkqX9THUmTZA/gTQyOphcCfwp8vd9YkiRpqgvHvgy8iMFNTH5/6O5jkiSpZ1MdSR8P3A/8R+ADyf+/bixAVdU+PWaTJGlOm+pz0lPd7ESSJPXEEpYkqVG9lnSSo5PcmGRDkpUTrN8jyfnd+iuSLBxa95Ik302yPsn1SfbsM6skSa3praST7AacBRwDLAZWJFk8Muxk4O6qOgw4Ezij23YecC7w3qo6HHg1j/1GLkmSdnl9HkkfAWyoqluq6mHgPGDpyJilwDnd9IXAURlcnfZ64Lqquhagqu6sqkd6zCpJUnP6LOkDGdxSdKtN3bIJx1TVFuBeYH/g+UAluTjJ95L89kQvkOSUJOuSrNu8efNO/wEkSRqnVi8cmwf8KvDu7r9vTXLU6KCqOruqllTVkvnz5890RkmSetVnSd8OHDQ0v6BbNuGY7n3ofYE7GRx1f7uq7qiqBxjcTOVlPWaVJKk5fZb0lcCiJIcm2R1YDox+9eUa4MRuehlwSVUVcDHw4iR7deX9KuAHPWaVJKk5U967+4mqqi3dF3FcDOwGfKGq1idZBayrqjXA54HVSTYAdzEocqrq7iSfYlD0Baytqov6yipJUot6K2mAqlrL4FT18LLThqYfBI6bZNtzGXwMS5KkOanVC8ckSZrzLGlJkhplSUuS1ChLWpKkRlnSkiQ1ypKWJKlRlrQkSY2ypCVJapQlLUlSoyxpSZIaZUlLktQoS1qSpEZZ0pIkNcqSliSpUZa0JEmNsqQlSWqUJS1JUqMsaUmSGmVJS5LUKEtakqRGWdKSJDXKkpYkqVGWtCRJjbKkJUlqlCUtSVKjLGlJkhplSUuS1ChLWpKkRlnSkiQ1ypKWJKlRlrQkSY2ypCVJapQlLUlSo3ot6SRHJ7kxyYYkKydYv0eS87v1VyRZOLL+4CT3JflwnzklSWpRbyWdZDfgLOAYYDGwIsnikWEnA3dX1WHAmcAZI+s/BXyjr4ySJLWszyPpI4ANVXVLVT0MnAcsHRmzFDinm74QOCpJAJK8BfghsL7HjJIkNavPkj4QuG1oflO3bMIxVbUFuBfYP8nTgI8Av99jPkmSmtbqhWOnA2dW1X3bGpTklCTrkqzbvHnzzCSTJGmGzOvxuW8HDhqaX9Atm2jMpiTzgH2BO4EjgWVJ/hDYD3g0yYNV9ZnhjavqbOBsgCVLllQvP4UkSWPSZ0lfCSxKciiDMl4OvGtkzBrgROC7wDLgkqoq4Ne2DkhyOnDfaEFLkuaOhSsvGneEx9j4yTfNyOv0VtJVtSXJqcDFwG7AF6pqfZJVwLqqWgN8HlidZANwF4MilyRJ9HskTVWtBdaOLDttaPpB4LgpnuP0XsJJktS4Vi8ckyRpzrOkJUlqlCUtSVKjLGlJkhplSUuS1ChLWpKkRlnSkiQ1qtfPSc9mc/XuNpKkdngkLUlSoyxpSZIaZUlLktQoS1qSpEZZ0pIkNcqSliSpUZa0JEmNsqQlSWqUJS1JUqMsaUmSGmVJS5LUKEtakqRGWdKSJDXKkpYkqVGWtCRJjbKkJUlqlCUtSVKjLGlJkhplSUuS1ChLWpKkRlnSkiQ1ypKWJKlRlrQkSY2ypCVJapQlLUlSo3ot6SRHJ7kxyYYkKydYv0eS87v1VyRZ2C1/XZKrklzf/fc1feaUJKlFvZV0kt2As4BjgMXAiiSLR4adDNxdVYcBZwJndMvvAN5cVS8GTgRW95VTkqRW9XkkfQSwoapuqaqHgfOApSNjlgLndNMXAkclSVVdXVU/7pavB56SZI8es0qS1Jw+S/pA4Lah+U3dsgnHVNUW4F5g/5Exbwe+V1UP9ZRTkqQmzRt3gG1JcjiDU+Cvn2T9KcApAAcffPAMJpMkqX99HknfDhw0NL+gWzbhmCTzgH2BO7v5BcDXgROq6uaJXqCqzq6qJVW1ZP78+Ts5viRJ49VnSV8JLEpyaJLdgeXAmpExaxhcGAawDLikqirJfsBFwMqq+k6PGSVJalZvJd29x3wqcDFwA3BBVa1PsirJsd2wzwP7J9kAfBDY+jGtU4HDgNOSXNM9ntlXVkmSWtTre9JVtRZYO7LstKHpB4HjJtjuD4A/6DObJEmt845jkiQ1ypKWJKlRlrQkSY2ypCVJapQlLUlSoyxpSZIaZUlLktQoS1qSpEZZ0pIkNcqSliSpUZa0JEmNsqQlSWqUJS1JUqMsaUmSGmVJS5LUKEtakqRGWdKSJDXKkpYkqVGWtCRJjbKkJUlqlCUtSVKjLGlJkhplSUuS1ChLWpKkRlnSkiQ1ypKWJKlRlrQkSY2ypCVJapQlLUlSoyxpSZIaZUlLktQoS1qSpEZZ0pIkNarXkk5ydJIbk2xIsnKC9XskOb9bf0WShUPrfqdbfmOSN/SZU5KkFvVW0kl2A84CjgEWAyuSLB4ZdjJwd1UdBpwJnNFtuxhYDhwOHA38efd8kiTNGX0eSR8BbKiqW6rqYeA8YOnImKXAOd30hcBRSdItP6+qHqqqHwIbuueTJGnO6LOkDwRuG5rf1C2bcExVbQHuBfaf5raSJO3S5o07wI5IcgpwSjd7X5Ibx5lnEgcAd+zok+SMnZBk+nY482zLC2aewmzLC2aeCbMtL7SZ+ZDJVvRZ0rcDBw3NL+iWTTRmU5J5wL7AndPclqo6Gzh7J2be6ZKsq6ol486xPWZb5tmWF2Zf5tmWF8w8E2ZbXph9mfs83X0lsCjJoUl2Z3Ah2JqRMWuAE7vpZcAlVVXd8uXd1d+HAouAv+sxqyRJzentSLqqtiQ5FbgY2A34QlWtT7IKWFdVa4DPA6uTbADuYlDkdOMuAH4AbAF+q6oe6SurJEkt6vU96apaC6wdWXba0PSDwHGTbPsJ4BN95pshTZ+On8Rsyzzb8sLsyzzb8oKZZ8JsywuzLHMGZ5clSVJrvC2oJEmNsqR3kmncAvWVSb6XZEuSZePIOJJnqrwfTPKDJNcl+d9JJv2IwEyZRub3Jrk+yTVJ/naCO9zNuKkyD417e5JKMtarTqexj09Ksrnbx9ck+Y1x5BzJNOU+TvKO7vd5fZKvzHTGkSxT7eMzh/bvTUnuGUfOkUxTZT44yaVJru7+zXjjOHKOZJoq8yHdv23XJbksyYJx5JxSVfnYwQeDC+NuBp4L7A5cCyweGbMQeAnwZWDZLMj7b4G9uun3AefPgsz7DE0fC3yz9czduL2BbwOXA0tazgucBHxmnPv1CWReBFwNPL2bf2bLeUfGv5/BRbet7+Ozgfd104uBjbMg818AJ3bTrwFWjzPzZA+PpHeOKW+BWlUbq+o64NFxBBwxnbyXVtUD3ezlDD6rPk7TyfzTodmnAuO+4GI6t8YF+DiD+9Y/OJPhJjDdvC2ZTuZ/D5xVVXcDVNVPZjjjsO3dxyuAr85IsslNJ3MB+3TT+wI/nsF8E5lO5sXAJd30pROsb4IlvXPMttuYbm/ek4Fv9JpoatPKnOS3ktwM/CHwgRnKNpkpMyd5GXBQVV00k8EmMd3fi7d3pwgvTHLQBOtn0nQyPx94fpLvJLk8ydEzlu7xpv3/XvcW06H8c5GMy3Qynw4cn2QTg0/0vH9mok1qOpmvBd7WTb8V2DvJ/jOQbbtY0tqmJMcDS4D/Mu4s01FVZ1XV84CPAL877jzbkuRJwKeAD407y3b4K2BhVb0E+Bb//AU5LZvH4JT3qxkcmf63JPuNNdH0LAcurNlxj4gVwJeqagHwRgb3v2i9Xz4MvCrJ1cCrGNzVsrl93fpOnC2mdRvThkwrb5LXAh8Fjq2qh2Yo22S2dx+fB7yl10RTmyrz3sCLgMuSbAReAawZ48VjU+7jqrpz6Hfhc8Avz1C2yUzn92ITsKaqflGDb9W7iUFpj8P2/B4vZ/ynumF6mU8GLgCoqu8CezK4R/a4TOd3+cdV9baq+iUG/85RVWO/SO9xxv2m+K7wYPCX+i0MTk1tvUjh8EnGfonxXzg2ZV7glxhceLFo3Pt3OzIvGpp+M4M72zWdeWT8ZYz3wrHp7ONnD02/Fbi89X3M4Dvpz+mmD2BwGnT/VvN2414AbKS7l8Us2MffAE7qpl/I4D3psWWfZuYDgCd1058AVo17X0/4s4w7wK7yYHCK56au2D7aLVvF4CgU4OUM/qK/n8GXiKxvPO//Av4JuKZ7rJkF+/jTwPou76XbKsRWMo+MHWtJT3Mf/+duH1/b7eMXtL6PgTB4W+EHwPXA8pbzdvOnA58c977djn28GPhO93txDfD6WZB5GfD33ZjPAXuMO/NED+84JklSo3xPWpKkRlnSkiQ1ypKWJKlRlrQkSY2ypCVJapQlLUlSoyxpSZIaZUlLktSo/wdNBqbWFeG5QgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data  = df[is_test_data].reset_index(drop=True)\n",
        "mse_topic = {}\n",
        "for el in test_data['topic'].unique():\n",
        "  mse_topic[el] = [0,0,0]\n",
        "for i in range(test_data.shape[0]):\n",
        "  mse_topic[test_data['topic'][i]][0]+=(ris[i][0]-Y_test[i])**2\n",
        "  mse_topic[test_data['topic'][i]][1]+=1\n",
        "\n",
        "for i in range(test_data.shape[0]): \n",
        "  mse_topic[test_data['topic'][i]][2]= mse_topic[test_data['topic'][i]][0]/mse_topic[test_data['topic'][i]][1]\n",
        "\n"
      ],
      "metadata": {
        "id": "ACNkRI3kXiVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame.from_dict(mse_topic,orient='index',columns=['tot_mse', 'size', 'mse']).drop(['tot_mse','size'],axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "BFt5Z1Xlgp4_",
        "outputId": "b98336b6-b094-45ad-880f-0c99712e9a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2c16b872-51c0-4463-b61e-3c26e72aa3b8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Holocaust denial should be a criminal offence</th>\n",
              "      <td>0.120940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>We should prohibit women in combat</th>\n",
              "      <td>0.114560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The use of public defenders should be mandatory</th>\n",
              "      <td>0.117140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>We should ban factory farming</th>\n",
              "      <td>0.074360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>We should adopt libertarianism</th>\n",
              "      <td>0.110478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>We should abolish the three-strikes laws</th>\n",
              "      <td>0.095094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>We should prohibit school prayer</th>\n",
              "      <td>0.102039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>We should ban targeted killing</th>\n",
              "      <td>0.111434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>We should legalize cannabis</th>\n",
              "      <td>0.085343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>We should adopt atheism</th>\n",
              "      <td>0.124196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>We should ban private military companies</th>\n",
              "      <td>0.096590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Social media brings more harm than good</th>\n",
              "      <td>0.092167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>We should ban algorithmic trading</th>\n",
              "      <td>0.104693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>We should ban missionary work</th>\n",
              "      <td>0.099823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>We should abolish the Olympic Games</th>\n",
              "      <td>0.109103</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c16b872-51c0-4463-b61e-3c26e72aa3b8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2c16b872-51c0-4463-b61e-3c26e72aa3b8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2c16b872-51c0-4463-b61e-3c26e72aa3b8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                      mse\n",
              "Holocaust denial should be a criminal offence    0.120940\n",
              "We should prohibit women in combat               0.114560\n",
              "The use of public defenders should be mandatory  0.117140\n",
              "We should ban factory farming                    0.074360\n",
              "We should adopt libertarianism                   0.110478\n",
              "We should abolish the three-strikes laws         0.095094\n",
              "We should prohibit school prayer                 0.102039\n",
              "We should ban targeted killing                   0.111434\n",
              "We should legalize cannabis                      0.085343\n",
              "We should adopt atheism                          0.124196\n",
              "We should ban private military companies         0.096590\n",
              "Social media brings more harm than good          0.092167\n",
              "We should ban algorithmic trading                0.104693\n",
              "We should ban missionary work                    0.099823\n",
              "We should abolish the Olympic Games              0.109103"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Predictor_score.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
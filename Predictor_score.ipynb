{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElFosco/NLP_argument_creation/blob/main/Predictor_score.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWUoqJbhZUaH"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHiGJ5Ou8Mwo"
      },
      "outputs": [],
      "source": [
        "!pip install -q tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-VGaZQA8ANi"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vzt9o9nfaCDH"
      },
      "outputs": [],
      "source": [
        "pip install torch>=1.6.0 transformers>=4.11.3 sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twaYdrRVblzG"
      },
      "outputs": [],
      "source": [
        "pip install numpy requests nlpaug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrxfQlV-7lbr"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmPHpfi8AkgA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.layers import concatenate\n",
        "from keras import Sequential\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import nlpaug.augmenter.word as naw #data augmentation\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE_to76fAVf7"
      },
      "source": [
        "#Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn6dyvb0AzdK"
      },
      "outputs": [],
      "source": [
        "# Using google drive to upload the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dir_path = \"Data/\"  # Point to project folder\n",
        "dataset = \"arg_quality_rank_30k.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca1VdgDoAURT"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dir_path + dataset)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV1gnJwQNIZd"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBRUbfFqwIyu"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text(text,topic):\n",
        "  text = re.sub('\\\"|-|\\\\\\\\|`', ' ', text)  # delete this chars from the string [\"-\\`]\n",
        "  text = re.sub('\\n', ' ', text)\n",
        "  text = re.sub('^[.]+', '', text)         # delete dots at the beginning of the sentence\n",
        "  #text = re.sub(\"([?.!,])\", r\" \\1 \", text)\n",
        "  text = re.sub('\\. \\.', '.', text)        # delete . .\n",
        "  text = re.sub('&', ' and ', text)        # replace & with and\n",
        "  text = re.sub(' +', ' ', text)           # delete additional whitespace\n",
        "  text = text.rstrip()                  \n",
        "  text = text.lstrip()\n",
        "  text = \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n",
        "  if not (re.search('[\\.|?|!]$',text)): #append the topic \n",
        "    text = text+' [SEP]'\n",
        "  else:\n",
        "    text = re.sub('[\\.|?|!]$',' [SEP]',text)\n",
        "  text = text + \" \" + topic.lower()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6314Kqgy-bl"
      },
      "outputs": [],
      "source": [
        "df.loc[2, \"argument\"] = \"zero tolerance policy in schools should not be adopted as circumstances are often not black and white, being more nuanced. no one should be written off due to a mistake of judgement.\"\n",
        "df['argument'] = df.apply(lambda row : clean_text(row['argument'],row['topic']), axis = 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQfS-UzU6QXS"
      },
      "source": [
        "##Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A82m4_1K5pfv"
      },
      "outputs": [],
      "source": [
        "is_training_data =  df['set']=='train'\n",
        "is_validation_data =  df['set']=='dev'\n",
        "is_test_data =  df['set']=='test'\n",
        "\n",
        "training_data = df[is_training_data]\n",
        "validation_data = df[is_validation_data]\n",
        "test_data  = df[is_test_data ]\n",
        "\n",
        "x_train = training_data['argument'].reset_index(drop=True)\n",
        "Y_train = training_data['MACE-P'].reset_index(drop=True)\n",
        "\n",
        "x_val = validation_data['argument'].reset_index(drop=True)\n",
        "Y_val = validation_data['MACE-P'].reset_index(drop=True)\n",
        "\n",
        "x_test = test_data['argument'].reset_index(drop=True)\n",
        "Y_test = test_data['MACE-P'].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_aug=Y_train_aug=x_train_ukp=Y_train_ukp=x_val_ukp=Y_val_ukp=x_test_ukp=Y_test_ukp=None"
      ],
      "metadata": {
        "id": "_4uRCAECzxYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTdXuAQSUMHm"
      },
      "source": [
        "#Data Augmentation (Not used in the final project)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBGSpoUYcrLD"
      },
      "outputs": [],
      "source": [
        "def create_augmented_data(x,Y):                   # run this two cells if you want to create other augmented data\n",
        "                                                  # right now, we have created around 8k of new data\n",
        "  back_translation_aug = naw.BackTranslationAug(\n",
        "    from_model_name='facebook/wmt19-en-de', \n",
        "    to_model_name='facebook/wmt19-de-en'\n",
        "  )\n",
        "\n",
        "  tmp_df = pd.DataFrame()\n",
        "  for i in range(8000,int(x.shape[0])):\n",
        "    print('['+str(i+1)+'/'+str(int(x.shape[0])+1)+']')\n",
        "    new_argument = back_translation_aug.augment(x[i])\n",
        "    score = Y[i]\n",
        "    tmp = {'argument': new_argument, 'score': score}\n",
        "    tmp_df = tmp_df.append(tmp,ignore_index=True)\n",
        "    if ((i+1)%500)==0:\n",
        "      print(\"Print on file:\"+str(int(i+1)))\n",
        "      tmp_df.to_csv(dir_path+\"aug_\"+str(int(i+1))+\".csv\", index=False, encoding='utf-8-sig')\n",
        "      tmp_df = pd.DataFrame()\n",
        "  tmp_df.to_csv(dir_path+\"aug.csv\", index=False, encoding='utf-8-sig')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZm0HqoJRSeA"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  create_augmented_data(x_train,Y_train)        # run this two cells if you want to create other augmented data\n",
        "                                                # right now, we have created around 8k of new data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwiNzlVTslqU"
      },
      "source": [
        "##Read augmented data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcTNARA_orKP"
      },
      "outputs": [],
      "source": [
        "def read_aug_data():                                      #read augmented data, from the one collected, right now there are 8k new data\n",
        "  ris = pd.DataFrame()\n",
        "  for i in range(500,8500,500):                           #change 8500 if you have created additional data\n",
        "    df = pd.read_csv(dir_path+\"aug_\"+str(int(i))+\".csv\")\n",
        "    ris = ris.append(df)\n",
        "  return ris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPAuE-aXpGfW"
      },
      "outputs": [],
      "source": [
        "ris_aug = read_aug_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSEaFexRsy0W"
      },
      "outputs": [],
      "source": [
        "x_train_aug = ris_aug['argument']\n",
        "Y_train_aug = ris_aug['score']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGfxf4XmwyT5"
      },
      "source": [
        "#Data from UKP (Not used in final implementation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyCbglAfI8Ba"
      },
      "outputs": [],
      "source": [
        "ukp_path = dir_path + \"UKPConvArg1-Ranking-CSV/\"            #read data from UKP dataset, these will be splitted into\n",
        "i=0                                                         #training, validation and test set\n",
        "ukp_dataset_train = pd.DataFrame()\n",
        "ukp_dataset_valid = pd.DataFrame()\n",
        "ukp_dataset_test = pd.DataFrame()\n",
        "for csv in os.listdir(ukp_path):\n",
        "  if i<=20:\n",
        "    df = pd.read_csv(ukp_path+csv,sep='\\t')\n",
        "    ukp_dataset_train = ukp_dataset_train.append(df)\n",
        "  elif 20<i<=25 :\n",
        "    df = pd.read_csv(ukp_path+csv,sep='\\t')\n",
        "    ukp_dataset_valid = ukp_dataset_valid.append(df)\n",
        "  else:\n",
        "    df = pd.read_csv(ukp_path+csv,sep='\\t')\n",
        "    ukp_dataset_test = ukp_dataset_test.append(df)\n",
        "  i+=1\n",
        "\n",
        "print(ukp_dataset_train.shape)\n",
        "print(ukp_dataset_valid.shape)\n",
        "print(ukp_dataset_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWI35htq-wdX"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_text_ukp(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('\\\"|-|\\\\\\\\|`|/|\\'', ' ', text)  # delete this chars from the string [\"-\\`]\n",
        "  text = re.sub('<br/>', ' ', text)\n",
        "  text = re.sub(':\\)', ' ', text)\n",
        "  text = re.sub('[\\.]+[\\.]+', ' ', text)         # delete ...\n",
        "  #text = re.sub(\"([?.!,])\", r\" \\1 \", text)\n",
        "  text = re.sub('&', ' and ', text)        # replace & with and\n",
        "  text = re.sub(' +', ' ', text)           # delete additional whitespace\n",
        "  text = text.rstrip()                  \n",
        "  text = text.lstrip()\n",
        "  text = \" \".join([lemmatizer.lemmatize(x) for x in text.split()])\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Omw-W9EyHIpP"
      },
      "outputs": [],
      "source": [
        "ukp_dataset_train['argument'] = ukp_dataset_train.apply(lambda row : clean_text_ukp(row['argument']), axis = 1)\n",
        "ukp_dataset_valid['argument'] = ukp_dataset_valid.apply(lambda row : clean_text_ukp(row['argument']), axis = 1)\n",
        "ukp_dataset_test['argument'] = ukp_dataset_test.apply(lambda row : clean_text_ukp(row['argument']), axis = 1)\n",
        "\n",
        "x_train_ukp = ukp_dataset_train['argument']\n",
        "Y_train_ukp = ukp_dataset_train['rank']\n",
        "\n",
        "x_val_ukp = ukp_dataset_valid['argument']\n",
        "Y_val_ukp = ukp_dataset_valid['rank']\n",
        "\n",
        "x_test_ukp = ukp_dataset_test['argument']\n",
        "Y_test_ukp = ukp_dataset_test['rank']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZOP4Qz9NhBs"
      },
      "source": [
        "#Final Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_augmented_added = False        #flag indicating if you want to add the augmented dataset\n",
        "is_ukp_added = False              #flag indicating if you want to add the ukp dataset"
      ],
      "metadata": {
        "id": "17uNUJF1Hml6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_final_dataset(x_train,Y_train,x_val,Y_val,x_test,Y_test,\n",
        "                           x_train_aug,Y_train_aug,\n",
        "                           x_train_ukp,Y_train_ukp,x_val_ukp,Y_val_ukp,x_test_ukp,Y_test_ukp,\n",
        "                           is_augmented_added,is_ukp_added):\n",
        "  \n",
        "  if is_augmented_added==True:\n",
        "\n",
        "    x_train = (x_train.append(x_train_aug)).reset_index(drop=True)\n",
        "    Y_train = (Y_train.append(Y_train_aug)).reset_index(drop=True)\n",
        "  \n",
        "  if is_ukp_added==True:\n",
        "\n",
        "    x_train = (x_train.append(x_train_aug)).reset_index(drop=True)\n",
        "    Y_train = (Y_train.append(Y_train_aug)).reset_index(drop=True)\n",
        "\n",
        "    x_val = (x_val.append(x_val_ukp)).reset_index(drop=True)\n",
        "    Y_val = (Y_val.append(Y_val_ukp)).reset_index(drop=True)\n",
        "\n",
        "    x_test = (x_test.append(x_test_ukp)).reset_index(drop=True)\n",
        "    Y_test = (Y_test.append(Y_test_ukp)).reset_index(drop=True)\n",
        "\n",
        "  return x_train,Y_train,x_val,Y_val,x_test,Y_test"
      ],
      "metadata": {
        "id": "nhK5YOsuIDFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJtlb4EsNgrH"
      },
      "outputs": [],
      "source": [
        "x_train,Y_train,x_val,Y_val,x_test,Y_test= generate_final_dataset(x_train,Y_train,\n",
        "                                                                  x_val,Y_val,\n",
        "                                                                  x_test,Y_test,\n",
        "                                                                  x_train_aug,Y_train_aug,\n",
        "                                                                  x_train_ukp,Y_train_ukp,\n",
        "                                                                  x_val_ukp,Y_val_ukp,\n",
        "                                                                  x_test_ukp,Y_test_ukp,\n",
        "                                                                  is_augmented_added,is_ukp_added)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6PDqt2Kfz_T"
      },
      "source": [
        "#[Bert](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8_ctG55-uTX"
      },
      "outputs": [],
      "source": [
        "# @title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'  # @param [\"bert_en_uncased_L-24_H-1024_A-16\",\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gzql3TDrBtg-"
      },
      "outputs": [],
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) #preprocessing layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pduw5iP9Dgz_"
      },
      "outputs": [],
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)               #bert model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWN6VYotHCw9"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model(dense_size=100):                     #model used to compute the score of the argument\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dense(dense_size, activation=keras.activations.relu, name='fc_1')(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=keras.activations.sigmoid, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_pX1FzsK2uj"
      },
      "outputs": [],
      "source": [
        "classifier_model = build_classifier_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAijDE-crAt4"
      },
      "outputs": [],
      "source": [
        "classifier_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L4k5p-bIdyq"
      },
      "outputs": [],
      "source": [
        "def pearson_metric(y_true, y_pred): #metric used to do some analysis for the data\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x, axis=0)\n",
        "    my = K.mean(y, axis=0)\n",
        "    xm, ym = x - mx, y - my\n",
        "    r_num = K.sum(xm * ym)\n",
        "    x_square_sum = K.sum(xm * xm)\n",
        "    y_square_sum = K.sum(ym * ym)\n",
        "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
        "    r = r_num / r_den\n",
        "    return K.mean(r)\n",
        "\n",
        "loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "metric_pearson = pearson_metric\n",
        "metric_mse = tf.keras.metrics.MeanSquaredError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a07yUREWKK8s"
      },
      "source": [
        "#Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ7fM7G1KNJK"
      },
      "outputs": [],
      "source": [
        "parameters = {'epochs': [1,2,3], \n",
        "              'batch_size':[32],\n",
        "              'init_lr': [3e-6,3e-5],\n",
        "              'dense_size' : [100,200,300],\n",
        "              'loss' : [tf.keras.losses.MeanSquaredError()]\n",
        "              }\n",
        "\n",
        "best_scores = -1\n",
        "best_params = {1: dict()}\n",
        "\n",
        "for loss in parameters['loss']:\n",
        "  print(\"Loss: \", loss)\n",
        "  for epochs in parameters['epochs']:\n",
        "    print(\" Epochs: \", epochs)\n",
        "    for init_lr in parameters['init_lr']:\n",
        "      print(\"  Start Learning Rate: \", init_lr)\n",
        "      for batch_size in parameters['batch_size']:\n",
        "        print(\"   Batch Size: \", batch_size)\n",
        "        for dense_size in parameters['dense_size']:\n",
        "          print(\"    Dense size: \", dense_size)\n",
        "          steps_per_epoch = x_train.shape[0] / batch_size \n",
        "          num_train_steps = steps_per_epoch * epochs\n",
        "          num_warmup_steps = int(epochs * x_train.shape[0] * 0.1 / batch_size)\n",
        "          optimizer = optimization.create_optimizer(init_lr=init_lr, \n",
        "                                                    num_train_steps=num_train_steps, \n",
        "                                                    num_warmup_steps=num_warmup_steps, \n",
        "                                                    optimizer_type='adamw')\n",
        "          classifier_model = build_classifier_model(dense_size)\n",
        "          classifier_model.compile(optimizer=optimizer, loss=loss, \n",
        "                                   metrics=[metric_pearson, metric_mse])\n",
        "          history = classifier_model.fit(x=x_train, y=Y_train, epochs=epochs, \n",
        "                                         batch_size=batch_size)\n",
        "          loss_calculated, pearson ,mse = classifier_model.evaluate(x=x_val, \n",
        "                                                                    y=Y_val)\n",
        "          print(\"     Pearson: \", pearson)\n",
        "          print(\"     MSE: \", mse)\n",
        "          if pearson > best_scores:                 \n",
        "            best_score = pearson\n",
        "            best_params = {'epochs': epochs, \n",
        "                           'batch_size': batch_size, \n",
        "                           'start_lr': init_lr,  \n",
        "                           'dense_size': dense_size,\n",
        "                           'loss': loss_calculated}\n",
        "print(best_scores)\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGxguBd7tbb-"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8DfZjrPIvy7"
      },
      "outputs": [],
      "source": [
        "# Best parameter found on grid search\n",
        "parameters = {'epochs': 2, \n",
        "              'batch_size': 32,\n",
        "              'init_lr': 3e-5,\n",
        "              'dense_size': 100,\n",
        "              'loss': tf.keras.losses.MeanSquaredError()\n",
        "              }\n",
        "\n",
        "epochs = parameters['epochs']\n",
        "batch_size = parameters['batch_size']\n",
        "init_lr = parameters['init_lr']\n",
        "dense_size = parameters['dense_size']\n",
        "loss = parameters['loss']\n",
        "\n",
        "steps_per_epoch = (x_train.append(x_val)).shape[0] / batch_size \n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(epochs * ((x_train.append(x_val)).shape[0]) * 0.1 / batch_size)\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr, \n",
        "                                          num_train_steps=num_train_steps, \n",
        "                                          num_warmup_steps=num_warmup_steps, \n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model = build_classifier_model(dense_size)\n",
        "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metric_pearson, \n",
        "                                                                  metric_mse])\n",
        "history = classifier_model.fit(x=(x_train.append(x_val)).reset_index(drop=True), y=(Y_train.append(Y_val)).reset_index(drop=True), \n",
        "                               epochs=epochs, batch_size=batch_size)\n",
        "loss_calculated, pearson, mse = classifier_model.evaluate(x_test, Y_test)\n",
        "print(\"Pearson: \", pearson)\n",
        "print(\"MSE: \", mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTZwGeyPtd4v"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5eU9VjBLV8T"
      },
      "outputs": [],
      "source": [
        "classifier_model.save(\"Models/<model_name>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data analysis"
      ],
      "metadata": {
        "id": "XCZh2fWOLEQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ris = classifier_model.predict(x_test)"
      ],
      "metadata": {
        "id": "ST13PUTLLGhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = [0] * 10\n",
        "mse_size = [0] * 10\n",
        "for i in range(len(Y_test)):\n",
        "  mse[math.ceil(ris[i][0]*10)-1] += (ris[i][0]-Y_test[i])**2\n",
        "  mse_size[math.ceil(ris[i][0]*10)-1] +=1\n",
        "for i in range(len(mse)):\n",
        " mse[i]=mse[i]/mse_size[i]"
      ],
      "metadata": {
        "id": "db18mk3jNE7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.set_title('MSE by value range')\n",
        "ax.set_ylabel(\"MSE\")\n",
        "ax.bar(['0.1','0.2','0.3','0.4','0.5','0.6','0.7','0.8','0.8','0.9'],mse)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cjfw-04ZRdU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data  = df[is_test_data].reset_index(drop=True)\n",
        "mse_topic = {}\n",
        "for el in test_data['topic'].unique():\n",
        "  mse_topic[el] = [0,0,0]\n",
        "for i in range(test_data.shape[0]):\n",
        "  mse_topic[test_data['topic'][i]][0]+=(ris[i][0]-Y_test[i])**2\n",
        "  mse_topic[test_data['topic'][i]][1]+=1\n",
        "\n",
        "for i in range(test_data.shape[0]): \n",
        "  mse_topic[test_data['topic'][i]][2]= mse_topic[test_data['topic'][i]][0]/mse_topic[test_data['topic'][i]][1]"
      ],
      "metadata": {
        "id": "ACNkRI3kXiVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame.from_dict(mse_topic,orient='index',columns=['tot_mse', 'size', 'mse']).drop(['tot_mse','size'],axis=1)"
      ],
      "metadata": {
        "id": "BFt5Z1Xlgp4_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Predictor_score.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElFosco/NLP_argument_creation/blob/main/XLNet_STS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mBzLdrdzodb"
      },
      "source": [
        "## Setup\n",
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRHRPImGUth7",
        "outputId": "fad96932-7470-42b8-f565-c7418d60d2aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "! pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy8gUsPuJNyw"
      },
      "source": [
        "Download the pretrained XLNet model and unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfPDGsUtHKG0",
        "outputId": "1aa75aee-6ac1-42db-ead2-0176d76a2a76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-30 17:14:37--  https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.194.128, 142.250.152.128, 173.194.197.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.194.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1338042341 (1.2G) [application/zip]\n",
            "Saving to: ‘cased_L-24_H-1024_A-16.zip’\n",
            "\n",
            "cased_L-24_H-1024_A 100%[===================>]   1.25G   133MB/s    in 11s     \n",
            "\n",
            "2022-01-30 17:14:47 (119 MB/s) - ‘cased_L-24_H-1024_A-16.zip’ saved [1338042341/1338042341]\n",
            "\n",
            "Archive:  cased_L-24_H-1024_A-16.zip\n",
            "   creating: xlnet_cased_L-24_H-1024_A-16/\n",
            "  inflating: xlnet_cased_L-24_H-1024_A-16/xlnet_model.ckpt.index  \n",
            "  inflating: xlnet_cased_L-24_H-1024_A-16/xlnet_model.ckpt.data-00000-of-00001  \n",
            "  inflating: xlnet_cased_L-24_H-1024_A-16/spiece.model  \n",
            "  inflating: xlnet_cased_L-24_H-1024_A-16/xlnet_model.ckpt.meta  \n",
            "  inflating: xlnet_cased_L-24_H-1024_A-16/xlnet_config.json  \n"
          ]
        }
      ],
      "source": [
        "# only needs to be done once\n",
        "! wget https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip\n",
        "! unzip cased_L-24_H-1024_A-16.zip "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uUwjq3BJRbu"
      },
      "source": [
        "Download extract the sts-b dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOGRICbOIsU8",
        "outputId": "cd92c18e-c459-4e55-864d-8e31efee941c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-30 17:15:11--  https://data.deepai.org/Stsbenchmark.zip\n",
            "Resolving data.deepai.org (data.deepai.org)... 138.201.36.183\n",
            "Connecting to data.deepai.org (data.deepai.org)|138.201.36.183|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 409703 (400K) [application/zip]\n",
            "Saving to: ‘Stsbenchmark.zip’\n",
            "\n",
            "Stsbenchmark.zip    100%[===================>] 400.10K   908KB/s    in 0.4s    \n",
            "\n",
            "2022-01-30 17:15:14 (908 KB/s) - ‘Stsbenchmark.zip’ saved [409703/409703]\n",
            "\n",
            "Archive:  Stsbenchmark.zip\n",
            "   creating: stsbenchmark/\n",
            "  inflating: stsbenchmark/readme.txt  \n",
            "  inflating: stsbenchmark/sts-test.csv  \n",
            "  inflating: stsbenchmark/correlation.pl  \n",
            "  inflating: stsbenchmark/LICENSE.txt  \n",
            "  inflating: stsbenchmark/sts-dev.csv  \n",
            "  inflating: stsbenchmark/sts-train.csv  \n"
          ]
        }
      ],
      "source": [
        "! wget https://data.deepai.org/Stsbenchmark.zip\n",
        "! unzip Stsbenchmark.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGY_ggUUMrwU"
      },
      "source": [
        "Git clone XLNet repo for access to run_classifier and the rest of the xlnet module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r190eYVMpiG",
        "outputId": "2db17191-5153-45f9-cd71-072bbad67dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'xlnet'...\n",
            "remote: Enumerating objects: 122, done.\u001b[K\n",
            "remote: Total 122 (delta 0), reused 0 (delta 0), pack-reused 122\u001b[K\n",
            "Receiving objects: 100% (122/122), 2.92 MiB | 15.66 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/zihangdai/xlnet.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDP-IaVuPC-z"
      },
      "source": [
        "## Define Variables\n",
        "Define all the dirs: data, xlnet scripts & pretrained model. \n",
        "If you would like to save models then you can authenticate a GCP account and use that for the OUTPUT_DIR & CHECKPOINT_DIR - you will need a large amount storage to fix these models. \n",
        "\n",
        "Alternatively it is easy to integrate a google drive account, checkout this guide for [I/O in colab](https://colab.research.google.com/notebooks/io.ipynb) but rememeber these will take up a large amount of storage. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7N_xVwavQlV"
      },
      "outputs": [],
      "source": [
        "SCRIPTS_DIR = 'xlnet' #@param {type:\"string\"}\n",
        "DATA_DIR = 'stsbenchmark' #@param {type:\"string\"}\n",
        "OUTPUT_DIR = 'proc_data/sts-b' #@param {type:\"string\"}\n",
        "PRETRAINED_MODEL_DIR = 'xlnet_cased_L-24_H-1024_A-16' #@param {type:\"string\"}\n",
        "CHECKPOINT_DIR = 'exp/sts-b' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR6euqwL1KBV"
      },
      "source": [
        "## Run Model\n",
        "This will set off the fine tuning of XLNet. There are a few things to note here:\n",
        "\n",
        "\n",
        "1.   This script will train and evaluate the model\n",
        "2.   This will store the results locally on colab and will be lost when you are disconnected from the runtime\n",
        "3.   This uses the large version of the model (base not released presently)\n",
        "4.   We are using a max seq length of 128 with a batch size of 8 please refer to the [README](https://github.com/zihangdai/xlnet#memory-issue-during-finetuning) for why this is.\n",
        "5. This will take approx 4hrs to run on GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to fix the dependecies issues\n",
        "!pip install tensorflow-gpu==1.15.0"
      ],
      "metadata": {
        "id": "KWMEzYDQ_sdm",
        "outputId": "da3fd18f-943a-43f3-e2e9-3998ff9f2f26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu==1.15.0\n",
            "  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5 MB 8.4 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.13.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.0.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.37.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.43.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 41.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=5b0e8daf91384c2eab6d7994190830e1bc79b44893687e0b1cddd84eb3374240\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 1.15.1 which is incompatible.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEMuT6LU0avg",
        "outputId": "b499ad69-4fd0-4965-aed6-7f21d41d51ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/xlnet/model_utils.py:295: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From xlnet/run_classifier.py:855: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From xlnet/run_classifier.py:637: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0130 17:19:43.970555 140257370036096 module_wrapper.py:139] From xlnet/run_classifier.py:637: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From xlnet/run_classifier.py:637: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0130 17:19:43.970804 140257370036096 module_wrapper.py:139] From xlnet/run_classifier.py:637: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From xlnet/run_classifier.py:661: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n",
            "W0130 17:19:43.971073 140257370036096 module_wrapper.py:139] From xlnet/run_classifier.py:661: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n",
            "WARNING:tensorflow:From xlnet/run_classifier.py:662: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0130 17:19:43.971373 140257370036096 module_wrapper.py:139] From xlnet/run_classifier.py:662: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/xlnet/model_utils.py:27: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0130 17:19:44.019254 140257370036096 module_wrapper.py:139] From /content/xlnet/model_utils.py:27: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/xlnet/model_utils.py:36: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W0130 17:19:44.019536 140257370036096 module_wrapper.py:139] From /content/xlnet/model_utils.py:36: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:Single device mode.\n",
            "I0130 17:19:44.019695 140257370036096 model_utils.py:36] Single device mode.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0130 17:19:44.019906 140257370036096 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "I0130 17:19:44.816865 140257370036096 utils.py:159] NumExpr defaulting to 2 threads.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'exp/sts-b', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            ", '_keep_checkpoint_max': 0, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8fcb2d2150>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=1, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
            "I0130 17:19:45.175376 140257370036096 estimator.py:212] Using config: {'_model_dir': 'exp/sts-b', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            ", '_keep_checkpoint_max': 0, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8fcb2d2150>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=1, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function get_model_fn.<locals>.model_fn at 0x7f8fcb2cf8c0>) includes params argument, but params are not passed to Estimator.\n",
            "W0130 17:19:45.175733 140257370036096 model_fn.py:630] Estimator's model_fn (<function get_model_fn.<locals>.model_fn at 0x7f8fcb2cf8c0>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Use tfrecord file proc_data/sts-b/spiece.model.len-128.train.tf_record\n",
            "I0130 17:19:45.176091 140257370036096 run_classifier.py:703] Use tfrecord file proc_data/sts-b/spiece.model.len-128.train.tf_record\n",
            "WARNING:tensorflow:From xlnet/run_classifier.py:186: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0130 17:19:45.176337 140257370036096 module_wrapper.py:139] From xlnet/run_classifier.py:186: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"xlnet/run_classifier.py\", line 855, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 312, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 258, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"xlnet/run_classifier.py\", line 705, in main\n",
            "    train_examples = processor.get_train_examples(FLAGS.data_dir)\n",
            "  File \"xlnet/run_classifier.py\", line 211, in get_train_examples\n",
            "    self._read_tsv(os.path.join(data_dir, self.train_file)), \"train\")\n",
            "  File \"xlnet/run_classifier.py\", line 189, in _read_tsv\n",
            "    for line in reader:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/lib/io/file_io.py\", line 220, in __next__\n",
            "    return self.next()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/lib/io/file_io.py\", line 214, in next\n",
            "    retval = self.readline()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/lib/io/file_io.py\", line 178, in readline\n",
            "    self._preread_check()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/lib/io/file_io.py\", line 84, in _preread_check\n",
            "    compat.as_bytes(self.__name), 1024 * 512)\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: stsbenchmark/train.tsv; No such file or directory\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_command = \"python xlnet/run_classifier.py \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=False \\\n",
        "  --eval_all_ckpt=False \\\n",
        "  --task_name=sts-b \\\n",
        "  --data_dir=\"+DATA_DIR+\" \\\n",
        "  --output_dir=\"+OUTPUT_DIR+\" \\\n",
        "  --model_dir=\"+CHECKPOINT_DIR+\" \\\n",
        "  --uncased=False \\\n",
        "  --spiece_model_file=\"+PRETRAINED_MODEL_DIR+\"/spiece.model \\\n",
        "  --model_config_path=\"+PRETRAINED_MODEL_DIR+\"/xlnet_config.json \\\n",
        "  --init_checkpoint=\"+PRETRAINED_MODEL_DIR+\"/xlnet_model.ckpt \\\n",
        "  --max_seq_length=128 \\\n",
        "  --train_batch_size=8 \\\n",
        "  --eval_batch_size=8 \\\n",
        "  --num_hosts=1 \\\n",
        "  --num_core_per_host=1 \\\n",
        "  --learning_rate=2e-5 \\\n",
        "  --train_steps=4000 \\\n",
        "  --warmup_steps=500 \\\n",
        "  --save_steps=500 \\\n",
        "  --iterations=500\"\n",
        "\n",
        "! {train_command}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvhqD-sO0Kyh"
      },
      "source": [
        "## Running & Results\n",
        "These are the results that I got from running this experiment\n",
        "### Params\n",
        "*    --max_seq_length=128 \\\n",
        "*    --train_batch_size= 8 \n",
        "\n",
        "### Times\n",
        "*   Training: 1hr 11mins\n",
        "*   Evaluation: 2.5hr\n",
        "\n",
        "### Results\n",
        "*  Most accurate model on final step\n",
        "*  Accuracy: 0.92416, eval_loss: 0.31708\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUW2avFM_fi_"
      },
      "source": [
        "### Model\n",
        "\n",
        "*   The trained model checkpoints can be found in 'exp/imdb'\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copia di XLNet-imdb-GPU.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNO+jWJoG6Qrj/FZBJ43+7G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElFosco/NLP_argument_creation/blob/main/Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installation"
      ],
      "metadata": {
        "id": "yF85RhAe_rG9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2qRNRty-YBk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7255d6c-7d56-47ba-c18a-ffc251c4f7cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.2 MB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 31.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 709 kB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 29.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 234 kB 8.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47.7 MB 93 kB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 30.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 36.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 37.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 32.8 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-UIt-40_ct-",
        "outputId": "55eaa181-5d58-42be-fa7e-ec35573dd4be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (2.8.1)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.21.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.24.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (13.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.10.0.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.13.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.43.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import"
      ],
      "metadata": {
        "id": "T9riNQ0y_wZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.layers import concatenate\n",
        "from keras import Sequential\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder  # one-hot encoding\n",
        "\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "from urllib import request\n",
        "import zipfile\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZPzn4Yp_m4i",
        "outputId": "423c09d4-ca5c-4238-9ecb-c501d3ed9fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Extraction"
      ],
      "metadata": {
        "id": "a7_c-TgJ5c6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using google drive to upload the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VF_LvtxAf6kC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64201703-bdfe-46d3-b8b7-ff1d3e45dd06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\")\n",
        "url = 'https://nlp.stanford.edu/projects/snli/snli_1.0.zip'\n",
        "dataset_path = os.path.join(dataset_folder, \"snli_1.0.zip\")\n",
        "dataset_unzip = os.path.join(dataset_folder,\"snli_1.0\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "  os.makedirs(dataset_folder)\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "  print(\"Downloading dataset...\")\n",
        "  request.urlretrieve(url, dataset_path)\n",
        "  print(\"Download complete!\")\n",
        "\n",
        "if not os.path.exists(dataset_unzip):\n",
        "  print(\"Extracting dataset... (it may take a while...)\")\n",
        "  with zipfile.ZipFile(dataset_path) as loaded_tar:\n",
        "    loaded_tar.extractall(dataset_folder)\n",
        "  print(\"Extraction completed!\")\n",
        "\n",
        "dataset_train =  os.path.join(dataset_unzip,\"snli_1.0_train.jsonl\")\n",
        "dataset_dev = os.path.join(dataset_unzip,\"snli_1.0_dev.jsonl\")\n",
        "dataset_test = os.path.join(dataset_unzip,\"snli_1.0_test.jsonl\")\n"
      ],
      "metadata": {
        "id": "XkSkvESG_qfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7ac185-18d6-4d99-962f-e6658a7febea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Download complete!\n",
            "Extracting dataset... (it may take a while...)\n",
            "Extraction completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_json(dataset_train,lines = True)[['sentence1','sentence2','gold_label']]\n",
        "df_dev = pd.read_json(dataset_dev,lines = True)[['sentence1','sentence2','gold_label']]\n",
        "df_test = pd.read_json(dataset_test,lines = True)[['sentence1','sentence2','gold_label']]"
      ],
      "metadata": {
        "id": "gBvvUcYM-ibD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def append_sentences(sentence_1,sentence_2):\n",
        "  if not (re.search('[\\.|?|!]$',sentence_1)): #append the topic \n",
        "    sentence_1 = sentence_1+'. '\n",
        "  else:\n",
        "    sentence_1 = re.sub('[\\.|?|!]$','. ',sentence_1)\n",
        "  sentence_1 = (sentence_1 + sentence_2).lower()\n",
        "  return sentence_1"
      ],
      "metadata": {
        "id": "BGYXT9s-DMY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['sentences'] = df_train.apply(lambda row: append_sentences(row['sentence1'],row['sentence2']), axis = 1)\n",
        "df_dev['sentences'] =   df_dev.apply(lambda row: append_sentences(row['sentence1'],row['sentence2']), axis = 1)\n",
        "df_test['sentences'] =  df_test.apply(lambda row: append_sentences(row['sentence1'],row['sentence2']), axis = 1)\n",
        "\n",
        "\n",
        "df_train = df_train.drop(['sentence1','sentence2'],axis=1)\n",
        "df_dev = df_dev.drop(['sentence1','sentence2'],axis=1)\n",
        "df_test = df_test.drop(['sentence1','sentence2'],axis=1)"
      ],
      "metadata": {
        "id": "aRIJLUm0DFpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[df_train.gold_label != '-']\n",
        "df_dev = df_dev[df_dev.gold_label != '-']\n",
        "df_test = df_test[df_test.gold_label != '-']\n",
        "\n",
        "df_train_grid = df_train.sample(frac=0.5)\n",
        "df_dev_grid = df_dev.sample(frac=0.5)\n",
        "\n",
        "x_train = df_train['sentences']\n",
        "Y_train = df_train['gold_label']\n",
        "\n",
        "x_train_grid = df_train_grid['sentences']\n",
        "Y_train_grid = df_train_grid['gold_label']\n",
        "\n",
        "x_dev = df_dev['sentences']\n",
        "Y_dev = df_dev['gold_label']\n",
        "\n",
        "x_dev_grid = df_dev_grid['sentences']\n",
        "Y_dev_grid = df_dev_grid['gold_label']\n",
        "\n",
        "x_test = df_test['sentences']\n",
        "Y_test = df_test['gold_label']"
      ],
      "metadata": {
        "id": "eJQ6o_ZoPXd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##One hot encoding"
      ],
      "metadata": {
        "id": "-l21QzLNMvZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHotEncoderWrapper(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.label_encoder = LabelEncoder()\n",
        "\n",
        "  def get_one_hot_encoding(self, list_pos):\n",
        "    # creates a dictionary containing pos and its one hot encoding\n",
        "    ris = {}\n",
        "    integer_encoded = self.label_encoder.fit_transform(list_pos)\n",
        "    # binary encode\n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\n",
        "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "    for i in range(len(list_pos)):\n",
        "      ris[list_pos[i]] = onehot_encoded[i]\n",
        "    return ris\n",
        "  \n",
        "  def get_inverse(self, encoded):\n",
        "    # from one hot encoding to original pos\n",
        "    return self.label_encoder.inverse_transform([np.argmax(encoded)])\n",
        "\n",
        "  def get_pos_from_label(self, label):\n",
        "    # from label to original pos\n",
        "    return self.label_encoder.inverse_transform([label])"
      ],
      "metadata": {
        "id": "Wbodk6vQMxHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoder = OneHotEncoderWrapper()\n",
        "set_label = df_train['gold_label'].unique()\n",
        "ris = one_hot_encoder.get_one_hot_encoding(set_label)\n",
        "\n",
        "Y_train_encoded = [ris[label] for label in Y_train]\n",
        "Y_train_encoded_grid = [ris[label] for label in Y_train_grid]\n",
        "\n",
        "Y_dev_encoded = [ris[label] for label in Y_dev]\n",
        "Y_dev_encoded_grid = [ris[label] for label in Y_dev_grid]\n",
        "\n",
        "Y_test_encoded = [ris[label] for label in Y_test]\n",
        "\n",
        "Y_train_encoded = np.asarray(Y_train_encoded)\n",
        "Y_train_encoded_grid = np.asarray(Y_train_encoded_grid)\n",
        "\n",
        "Y_dev_encoded = np.asarray(Y_dev_encoded)\n",
        "Y_dev_encoded_grid = np.asarray(Y_dev_encoded_grid)\n",
        "\n",
        "Y_test_encoded = np.asarray(Y_test_encoded)\n"
      ],
      "metadata": {
        "id": "pv9jz2YeNF7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bert"
      ],
      "metadata": {
        "id": "L2tUALOfJN63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'albert_en_base'  # @param [\"bert_en_uncased_L-24_H-1024_A-16\",\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "-IgMLE24JJ66",
        "outputId": "ec1133dc-4323-4b47-aca0-788e40b08993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/albert_en_base/3\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/albert_en_preprocess/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) #preprocessing layer"
      ],
      "metadata": {
        "id": "ABHw1xYzKjEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)               #bert model"
      ],
      "metadata": {
        "id": "rdHHOttyKjY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model(dense_size=100):                     #model used to compute the score of the argument\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dense(dense_size, activation=keras.activations.relu, name='fc_1')(net)\n",
        "  net = tf.keras.layers.Dense(3, activation=keras.activations.softmax, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "j4e_ySupLAZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Grid Search"
      ],
      "metadata": {
        "id": "3zTsyy4zU1yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_grid.shape "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvo87LZSif-9",
        "outputId": "ee92f603-2f00-4f99-8336-8d654d055615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54937,)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {'epochs': [1,2], \n",
        "              'batch_size':[16,32,64],\n",
        "              'init_lr': [3e-5],\n",
        "              'dense_size' : [100,200,300]\n",
        "              }\n",
        "best_scores = -1\n",
        "best_params = {1: dict()}\n",
        "\n",
        "\n",
        "\n",
        "for epochs in parameters['epochs']:\n",
        "  print(\" Epochs: \", epochs)\n",
        "  for init_lr in parameters['init_lr']:\n",
        "    print(\"  Start Learning Rate: \", init_lr)\n",
        "    for batch_size in parameters['batch_size']:\n",
        "      print(\"   Batch Size: \", batch_size)\n",
        "      for dense_size in parameters['dense_size']:\n",
        "        print(\"    Dense size: \", dense_size)\n",
        "        steps_per_epoch = x_train_grid.shape[0] / batch_size \n",
        "        num_train_steps = steps_per_epoch * epochs\n",
        "        num_warmup_steps = int(epochs * x_train_grid.shape[0] * 0.1 / batch_size)\n",
        "        optimizer = optimization.create_optimizer(init_lr=init_lr, \n",
        "                                                    num_train_steps=num_train_steps, \n",
        "                                                    num_warmup_steps=num_warmup_steps, \n",
        "                                                    optimizer_type='adamw')\n",
        "        classifier_model = build_classifier_model(dense_size)\n",
        "        classifier_model.compile(optimizer=optimizer, loss='categorical_crossentropy', \n",
        "                                   metrics=['accuracy'])\n",
        "        history = classifier_model.fit(x=x_train_grid, y=Y_train_encoded_grid, epochs=epochs, \n",
        "                                         batch_size=batch_size)\n",
        "        loss_calculated, accuracy = classifier_model.evaluate(x=x_dev_grid, y=Y_dev_encoded_grid)\n",
        "        print(\"     Loss: \", loss_calculated)\n",
        "        print(\"     Accuracy: \", accuracy)\n",
        "        if accuracy > best_scores:                 \n",
        "          best_score = accuracy\n",
        "          best_params = {'epochs': epochs, \n",
        "                          'batch_size': batch_size, \n",
        "                          'start_lr': init_lr,  \n",
        "                          'dense_size': dense_size,\n",
        "                          'loss': loss_calculated}\n",
        "print(best_scores)\n",
        "print(best_params)\n"
      ],
      "metadata": {
        "id": "TI2mefjeU3al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "lgFmY532aNBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best parameter found on grid search\n",
        "parameters = {'epochs': 1, \n",
        "              'batch_size': 32,\n",
        "              'init_lr': 3e-5,\n",
        "              'dense_size': 100\n",
        "              }\n",
        "\n",
        "epochs = parameters['epochs']\n",
        "batch_size = parameters['batch_size']\n",
        "init_lr = parameters['init_lr']\n",
        "dense_size = parameters['dense_size']\n",
        "\n",
        "\n",
        "x_final = x_train_grid.append(x_dev_grid)\n",
        "Y_final_encoded = np.array(Y_train_encoded_grid.tolist() + Y_dev_encoded_grid.tolist())\n",
        "\n",
        "steps_per_epoch = (x_final).shape[0] / batch_size \n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(epochs * x_final.shape[0] * 0.1 / batch_size)\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr, \n",
        "                                          num_train_steps=num_train_steps, \n",
        "                                          num_warmup_steps=num_warmup_steps, \n",
        "                                          optimizer_type='adamw')\n",
        "classifier_model = build_classifier_model(dense_size)\n",
        "classifier_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = classifier_model.fit(x=(x_final), y=(Y_final_encoded), \n",
        "                               epochs=epochs, batch_size=batch_size)\n",
        "ris = classifier_model.evaluate(x=x_test, y=Y_test_encoded)\n",
        "print(\"Accuracy: \", ris)"
      ],
      "metadata": {
        "id": "2YTJbMK2aMzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b6428fb-46af-4975-f13b-2e1550339936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8738/8738 [==============================] - 16788s 2s/step - loss: 0.4351 - accuracy: 0.8308\n",
            "307/307 [==============================] - 209s 679ms/step - loss: 0.2997 - accuracy: 0.8910\n",
            "Accuracy:  [0.2997262179851532, 0.8909812569618225]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.save(\"drive/MyDrive/Colab Notebooks/NLP/classifierNLI.h5\")"
      ],
      "metadata": {
        "id": "9Ij2EL1lfvqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}